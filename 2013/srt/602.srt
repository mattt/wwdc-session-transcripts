X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

1
00:00:00,506 --> 00:00:10,706 A:middle
[ Silence ]

2
00:00:11,206 --> 00:00:11,886 A:middle
>> Good morning everyone.

3
00:00:12,496 --> 00:00:14,316 A:middle
My name is Tony Guetta
and I'm the Manager

4
00:00:14,316 --> 00:00:15,576 A:middle
in the Core Audio
group at Apple.

5
00:00:15,716 --> 00:00:17,186 A:middle
And today, I'm going
to talk to you about,

6
00:00:17,186 --> 00:00:21,356 A:middle
What's New in Core
Audio for iOS.

7
00:00:21,476 --> 00:00:23,706 A:middle
We're going to begin with a
very high level overview of some

8
00:00:23,756 --> 00:00:25,466 A:middle
of the new audio
features in iOS 7.

9
00:00:25,626 --> 00:00:28,106 A:middle
And for the majority of this
session, we're going to talk--

10
00:00:28,106 --> 00:00:30,556 A:middle
spend our time focused on one
new technology in particular

11
00:00:30,766 --> 00:00:32,305 A:middle
that we think you're going
to be very excited about.

12
00:00:32,625 --> 00:00:34,896 A:middle
So, let's dive in to the
list of new features.

13
00:00:35,906 --> 00:00:39,756 A:middle
First is Audio Input Selection
and with input selection,

14
00:00:39,886 --> 00:00:41,986 A:middle
your application now has
the ability to specify

15
00:00:41,986 --> 00:00:44,696 A:middle
which audio input it would like
to use in certain situations.

16
00:00:44,736 --> 00:00:48,506 A:middle
So for example, if the user had
a wired headset plugged into his

17
00:00:48,506 --> 00:00:50,706 A:middle
or her device, but your
app wanted to continue

18
00:00:50,706 --> 00:00:52,396 A:middle
to use the built-in
microphone for input,

19
00:00:52,636 --> 00:00:55,506 A:middle
you now have the
capability to control that.

20
00:00:56,166 --> 00:00:58,436 A:middle
With input selection, you can
also choose which microphone

21
00:00:58,436 --> 00:01:00,456 A:middle
that you'd like to use on
our multi-mic platforms

22
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

23
00:01:00,996 --> 00:01:03,426 A:middle
and on devices that support
it such as the iPhone 5.

24
00:01:03,846 --> 00:01:04,616 A:middle
You can take advantage

25
00:01:04,616 --> 00:01:06,136 A:middle
of microphone beam
forming processing

26
00:01:06,636 --> 00:01:08,846 A:middle
to set an effective
microphone directivity

27
00:01:08,976 --> 00:01:12,096 A:middle
by specifying a polar pattern
such as cardioid or subcardioid.

28
00:01:14,716 --> 00:01:17,176 A:middle
We've made some enhancements
to multichannel audio on iOS 7.

29
00:01:17,556 --> 00:01:19,536 A:middle
And through the use of
the AVAudioSession API,

30
00:01:20,236 --> 00:01:22,266 A:middle
you can now discover the
maximum number of input

31
00:01:22,266 --> 00:01:23,446 A:middle
and output channels
that are supported

32
00:01:23,446 --> 00:01:25,656 A:middle
by the current audio route
as well as being able

33
00:01:25,656 --> 00:01:29,096 A:middle
to specify your preferred number
of input and output channels.

34
00:01:29,096 --> 00:01:31,006 A:middle
For audio outputs
supported such as HDMI,

35
00:01:31,906 --> 00:01:33,786 A:middle
you can obtain audio
channel labels

36
00:01:33,786 --> 00:01:35,806 A:middle
which associate a
particular audio channel

37
00:01:36,126 --> 00:01:38,196 A:middle
with the description of a
physical speaker location

38
00:01:38,196 --> 00:01:41,486 A:middle
such as front left, front
right, center and so on.

39
00:01:43,236 --> 00:01:44,916 A:middle
We've added some
extensions to Open AL

40
00:01:44,916 --> 00:01:47,886 A:middle
to enhance the gaming audio
experience in iOS 7 starting

41
00:01:47,886 --> 00:01:51,036 A:middle
with the ability to specify a
spatialization rendering quality

42
00:01:51,086 --> 00:01:52,376 A:middle
on a per-sound source basis.

43
00:01:53,126 --> 00:01:54,046 A:middle
Now, you might use this

44
00:01:54,046 --> 00:01:56,196 A:middle
to specify a very high
quality rendering algorithm

45
00:01:56,196 --> 00:01:58,146 A:middle
for the important sound
sources in your game.

46
00:01:58,396 --> 00:02:00,066 A:middle
But a less CPU intensive
algorithm

47
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

48
00:02:00,066 --> 00:02:02,036 A:middle
for those less importance
sound sources in your game.

49
00:02:03,576 --> 00:02:04,816 A:middle
We've also made some
improvements

50
00:02:04,816 --> 00:02:06,996 A:middle
to our high quality
spatialization

51
00:02:06,996 --> 00:02:07,676 A:middle
rendering algorithm.

52
00:02:08,326 --> 00:02:10,366 A:middle
And also added the ability
to support rendering

53
00:02:10,366 --> 00:02:12,356 A:middle
to multichannel output
hardware when it's available.

54
00:02:13,326 --> 00:02:15,016 A:middle
Finally, we've added
an extension

55
00:02:15,016 --> 00:02:16,116 A:middle
to allow capturing the output

56
00:02:16,116 --> 00:02:18,166 A:middle
of the current Open AL
3D rendering context.

57
00:02:18,666 --> 00:02:22,666 A:middle
We've added time-pitch
capabilities to Audio Queue.

58
00:02:22,916 --> 00:02:25,496 A:middle
So your application can now
control the speed up and slow

59
00:02:25,496 --> 00:02:27,266 A:middle
down of Audio Queue
playback both in terms

60
00:02:27,266 --> 00:02:28,576 A:middle
of time and in frequency.

61
00:02:31,276 --> 00:02:34,176 A:middle
We've enhanced the security
around audio recording in iOS 7

62
00:02:34,546 --> 00:02:37,456 A:middle
and we now require explicit user
approval before your application

63
00:02:37,456 --> 00:02:38,336 A:middle
can do audio input.

64
00:02:38,966 --> 00:02:40,166 A:middle
Now, the reason for
doing this is

65
00:02:40,166 --> 00:02:42,146 A:middle
to prevent a malicious
application from being able

66
00:02:42,146 --> 00:02:43,876 A:middle
to record a user without
him or her knowing it.

67
00:02:45,046 --> 00:02:46,976 A:middle
The way that this works
is very similar to the way

68
00:02:46,976 --> 00:02:49,876 A:middle
that the location service's
permission mechanism works.

69
00:02:49,876 --> 00:02:50,976 A:middle
In that the user is presented

70
00:02:50,976 --> 00:02:53,206 A:middle
with a model dialog
requesting his or her permission

71
00:02:53,206 --> 00:02:54,096 A:middle
to use the audio input.

72
00:02:54,486 --> 00:02:58,006 A:middle
The decision is made on
per-application basis

73
00:02:58,076 --> 00:02:59,196 A:middle
and it is a one-time decision.

74
00:02:59,726 --> 00:03:01,976 A:middle
However, if you'd like to
go in and change your mind

75
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

76
00:03:01,976 --> 00:03:03,206 A:middle
at a later time,
you can always go

77
00:03:03,206 --> 00:03:04,816 A:middle
into the Settings
application to do that.

78
00:03:06,666 --> 00:03:08,776 A:middle
Until the user has given
your application permission

79
00:03:08,776 --> 00:03:11,186 A:middle
to use audio input, you
will get silence so you need

80
00:03:11,186 --> 00:03:12,276 A:middle
to be prepared to handle that.

81
00:03:12,276 --> 00:03:15,876 A:middle
Now, what actually triggers
the dialog from being presented

82
00:03:15,876 --> 00:03:17,976 A:middle
to the user is an attempt
by your application

83
00:03:17,976 --> 00:03:20,626 A:middle
to use an audio session
category that would enable input

84
00:03:20,626 --> 00:03:22,546 A:middle
such as the record
category or play and record.

85
00:03:23,596 --> 00:03:26,076 A:middle
However, if you'd
like to have control

86
00:03:26,076 --> 00:03:27,856 A:middle
over when the user is
presented with his dialog

87
00:03:28,016 --> 00:03:29,966 A:middle
so that it can happen
at a more opportune time

88
00:03:29,966 --> 00:03:32,256 A:middle
for your application,
we've added some API

89
00:03:32,446 --> 00:03:35,426 A:middle
and AVAudioSession for
you to be able to do that.

90
00:03:38,226 --> 00:03:39,976 A:middle
Finally, just a note on
the AudioSession API.

91
00:03:40,426 --> 00:03:42,766 A:middle
As we mentioned at last year's
conference, the C version

92
00:03:42,766 --> 00:03:45,316 A:middle
of the AudioSession API is
officially being deprecated

93
00:03:45,316 --> 00:03:46,086 A:middle
in iOS 7.

94
00:03:46,326 --> 00:03:48,386 A:middle
So, we hope that over the
course of the past year,

95
00:03:48,576 --> 00:03:50,416 A:middle
you've all had the opportunity
to move your applications

96
00:03:50,416 --> 00:03:52,286 A:middle
over to using the
AVAudioSession API.

97
00:03:55,696 --> 00:03:57,976 A:middle
So, here is a summary of the
features that we just discussed.

98
00:03:57,976 --> 00:04:00,116 A:middle
We're not going to spend anymore
time today going over any

99
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

100
00:04:00,116 --> 00:04:01,446 A:middle
of these topics in
any more detail.

101
00:04:01,616 --> 00:04:03,326 A:middle
So, if you have any
questions about these

102
00:04:03,416 --> 00:04:05,636 A:middle
or like a more detailed
overview of any of these items,

103
00:04:06,026 --> 00:04:08,036 A:middle
we encourage you to come by
our labs either later today

104
00:04:08,316 --> 00:04:10,106 A:middle
or tomorrow morning and
we'd be happy to discuss

105
00:04:10,106 --> 00:04:10,936 A:middle
with you in more detail.

106
00:04:11,666 --> 00:04:14,046 A:middle
I'd also encourage you to have
a look at the documentation

107
00:04:14,046 --> 00:04:16,046 A:middle
in the various header files
that I outlined in the course

108
00:04:16,046 --> 00:04:17,276 A:middle
of going through
each of these topics.

109
00:04:18,276 --> 00:04:21,526 A:middle
So for the remainder of this
session, we're going to focus

110
00:04:21,526 --> 00:04:23,546 A:middle
on one new technology in
particular that again,

111
00:04:23,856 --> 00:04:25,406 A:middle
we think you're going
to be very excited about

112
00:04:25,406 --> 00:04:28,046 A:middle
and that's Inter-App Audio.

113
00:04:28,516 --> 00:04:29,926 A:middle
So what is Inter-App Audio?

114
00:04:30,286 --> 00:04:32,856 A:middle
Well, as the name implies,

115
00:04:32,856 --> 00:04:34,996 A:middle
Inter-App Audio provides
the ability to stream audio

116
00:04:34,996 --> 00:04:36,406 A:middle
between applications
in real-time.

117
00:04:36,756 --> 00:04:39,326 A:middle
So, if you have a really cool
effects application and you want

118
00:04:39,326 --> 00:04:40,806 A:middle
to integrate that into
your DAW application,

119
00:04:40,906 --> 00:04:42,726 A:middle
you now have the
ability to do that.

120
00:04:43,266 --> 00:04:46,146 A:middle
We've built Inter-App Audio on
top of existing Core Audio APIs

121
00:04:46,146 --> 00:04:47,986 A:middle
so it should be very
easy for you to integrate

122
00:04:47,986 --> 00:04:49,776 A:middle
into your existing applications

123
00:04:49,776 --> 00:04:52,616 A:middle
and deploy quickly
to the app store.

124
00:04:52,616 --> 00:04:54,326 A:middle
Because it's built into
the operating system,

125
00:04:54,526 --> 00:04:56,776 A:middle
the solution is very efficient
with zero additional latency

126
00:04:57,296 --> 00:05:00,026 A:middle
and should provide for a stable
platform for the evolution

127
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

128
00:05:00,026 --> 00:05:00,796 A:middle
of the feature over time.

129
00:05:01,706 --> 00:05:03,476 A:middle
Now, before we get into any
of the technical details

130
00:05:03,476 --> 00:05:06,406 A:middle
of how Inter-App Audio works,
I'd like to invite up Alec

131
00:05:06,636 --> 00:05:09,106 A:middle
from the GarageBand
team to give you a demo.

132
00:05:09,181 --> 00:05:11,181 A:middle
[ Applause ]

133
00:05:11,256 --> 00:05:16,566 A:middle
>> Thanks Tony.

134
00:05:17,056 --> 00:05:17,476 A:middle
Am I up?

135
00:05:17,766 --> 00:05:17,986 A:middle
>> Yeah.

136
00:05:18,416 --> 00:05:22,996 A:middle
>> My name is Alec, I am a
product designer for GarageBand

137
00:05:22,996 --> 00:05:26,296 A:middle
and Logic and I'm going to
switch over here to my iPad.

138
00:05:27,106 --> 00:05:30,496 A:middle
So, what I want to do today
is give a quick demonstration

139
00:05:30,546 --> 00:05:33,876 A:middle
about how we have been working
with the development version,

140
00:05:33,876 --> 00:05:36,016 A:middle
kind of a sneak peek into
a development version

141
00:05:36,016 --> 00:05:38,976 A:middle
of GarageBand and how we're
doing some experiments

142
00:05:39,096 --> 00:05:40,096 A:middle
with Inter-App Audio.

143
00:05:40,936 --> 00:05:43,946 A:middle
So, what I have up here is
just a simple FourTrack song

144
00:05:43,946 --> 00:05:45,736 A:middle
in GarageBand, I'm going
to play a little bit

145
00:05:45,736 --> 00:05:47,396 A:middle
so you can get an idea
of what it sounds like.

146
00:05:47,396 --> 00:05:47,936 A:middle
[ Music ]

147
00:05:47,936 --> 00:05:56,386 A:middle
OK. So the first thing
I want to do is I want

148
00:05:56,386 --> 00:05:58,556 A:middle
to add a little keyboard
part to this.

149
00:05:59,056 --> 00:06:01,736 A:middle
But instead of using one
of the built-in instruments

150
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

151
00:06:01,736 --> 00:06:05,946 A:middle
in GarageBand, I want to use
an instrument, on the system

152
00:06:05,946 --> 00:06:07,216 A:middle
that is not part of GarageBand.

153
00:06:07,216 --> 00:06:08,806 A:middle
So to do that, I'm going to go

154
00:06:08,806 --> 00:06:10,666 A:middle
out to the GarageBand
instrument browser.

155
00:06:10,776 --> 00:06:13,356 A:middle
Now, what we see here are
the instruments that ship

156
00:06:13,356 --> 00:06:15,096 A:middle
with GarageBand,
part of GarageBand.

157
00:06:15,576 --> 00:06:17,976 A:middle
And then we have a new
icon here, Music Apps.

158
00:06:18,266 --> 00:06:22,536 A:middle
I'm going to tap on that and
we see the icons of other apps

159
00:06:22,536 --> 00:06:26,466 A:middle
on the system which
are audio apps.

160
00:06:26,516 --> 00:06:30,116 A:middle
So, I'm going to click on
sampler one here and we'll see

161
00:06:30,116 --> 00:06:32,556 A:middle
that the sampler launches
in the background.

162
00:06:33,126 --> 00:06:36,976 A:middle
Now here it with the UI in the
foreground and we can hear it.

163
00:06:37,936 --> 00:06:39,906 A:middle
Now, you see there's a
transport here and that's,

164
00:06:39,906 --> 00:06:42,626 A:middle
this transport is remotely
controlling the transport

165
00:06:42,626 --> 00:06:43,386 A:middle
of GarageBand.

166
00:06:43,556 --> 00:06:45,456 A:middle
So, when I press the record
button, what we're going

167
00:06:45,456 --> 00:06:48,716 A:middle
to hear is a count off from
GarageBand and then the track

168
00:06:48,716 --> 00:06:53,856 A:middle
that I just played and I'll
record over the top of it.

169
00:06:54,356 --> 00:07:01,376 A:middle
[ Music ]

170
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

171
00:07:01,876 --> 00:07:03,236 A:middle
Brilliant musical passage.

172
00:07:04,816 --> 00:07:07,996 A:middle
So now, if I-- if you look up at
this transport again you'll see

173
00:07:07,996 --> 00:07:09,736 A:middle
that there's a GarageBand icon.

174
00:07:10,366 --> 00:07:12,916 A:middle
When I tap on that
icon, I switch back

175
00:07:12,916 --> 00:07:16,756 A:middle
to the GarageBand application
and now, in the tracks view,

176
00:07:17,126 --> 00:07:20,456 A:middle
a new track has been added
with this little keyboard part

177
00:07:20,456 --> 00:07:26,846 A:middle
that I played we
can listen to it.

178
00:07:26,936 --> 00:07:30,346 A:middle
[Background Music] And
add some keyboard to it.

179
00:07:30,346 --> 00:07:30,896 A:middle
[ Music ]

180
00:07:30,896 --> 00:07:33,746 A:middle
So, that was bringing audio
from another application,

181
00:07:33,746 --> 00:07:35,986 A:middle
controlling that
application in its interface,

182
00:07:35,986 --> 00:07:37,486 A:middle
and recording that
in GarageBand.

183
00:07:38,096 --> 00:07:39,626 A:middle
The next thing I
want to do is I want

184
00:07:39,626 --> 00:07:43,066 A:middle
to process an input
from GarageBand.

185
00:07:43,066 --> 00:07:47,736 A:middle
So, I'm going to put on my
little guitar here and we'll go

186
00:07:47,736 --> 00:07:50,896 A:middle
to the guitar amp in GarageBand.

187
00:07:51,426 --> 00:07:53,516 A:middle
Now, this guitar
amp is part of--

188
00:07:53,516 --> 00:07:57,466 A:middle
one of the instruments built
in the GarageBand and I'm going

189
00:07:57,466 --> 00:07:59,856 A:middle
to turn on input monitoring
so I can hear myself.

190
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

191
00:08:00,616 --> 00:08:04,886 A:middle
[Background Music] You
guys got it out there?

192
00:08:07,456 --> 00:08:10,076 A:middle
It's a little phase switch.

193
00:08:13,456 --> 00:08:15,586 A:middle
OK. Or we're going to--

194
00:08:15,586 --> 00:08:18,176 A:middle
there you go, that's
more rock and roll.

195
00:08:18,686 --> 00:08:21,186 A:middle
OK. So, that's a
good sound right?

196
00:08:21,186 --> 00:08:23,316 A:middle
That's using the guitar
app from GarageBand.

197
00:08:23,316 --> 00:08:25,456 A:middle
What I want to do though
is I want to process it

198
00:08:25,626 --> 00:08:27,476 A:middle
with another effect
on my system.

199
00:08:27,926 --> 00:08:31,426 A:middle
So again, I'm going to go into
the input settings in GarageBand

200
00:08:31,426 --> 00:08:32,416 A:middle
and if you see about halfway

201
00:08:32,416 --> 00:08:34,466 A:middle
down this list, it
says Effect App.

202
00:08:34,466 --> 00:08:38,076 A:middle
I'm going to tap on that and
we can see a list of apps

203
00:08:38,076 --> 00:08:40,525 A:middle
on my system that are
effects so I'm going

204
00:08:40,525 --> 00:08:42,046 A:middle
to click on this Audio Delay.

205
00:08:42,905 --> 00:08:45,606 A:middle
[Music] So, there is the delay

206
00:08:45,656 --> 00:08:47,366 A:middle
but it's not really
the settings I want.

207
00:08:47,496 --> 00:08:51,276 A:middle
So, I'm going to tap on
the Effect icon and switch

208
00:08:51,276 --> 00:08:52,546 A:middle
to the Effects Interface.

209
00:08:53,086 --> 00:08:54,636 A:middle
I'm going to take the feedback

210
00:08:54,636 --> 00:08:58,356 A:middle
down here a little
bit and the mix.

211
00:08:59,016 --> 00:08:59,196 A:middle
[Music] OK.

212
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

213
00:09:00,266 --> 00:09:04,836 A:middle
So that's a little bit better.

214
00:09:04,836 --> 00:09:06,726 A:middle
So now, what I'm doing
is I'm taking the input

215
00:09:07,296 --> 00:09:09,316 A:middle
through GarageBand, sending
it out to this effect

216
00:09:09,686 --> 00:09:11,476 A:middle
and bringing it back
on to GarageBand.

217
00:09:12,226 --> 00:09:13,676 A:middle
Then I can hit record.

218
00:09:13,676 --> 00:09:14,256 A:middle
[ Music ]

219
00:09:14,256 --> 00:09:54,026 A:middle
Now, if we switch back
to the tracks view,

220
00:09:55,626 --> 00:09:57,406 A:middle
we can see that new
region has been recorded

221
00:09:57,406 --> 00:09:59,606 A:middle
in GarageBand and if I play.

222
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

223
00:10:00,106 --> 00:10:03,576 A:middle
[ Music ]

224
00:10:04,076 --> 00:10:08,566 A:middle
There is the source
with the delay added

225
00:10:08,676 --> 00:10:10,676 A:middle
to it recorded in
the GarageBand.

226
00:10:14,816 --> 00:10:17,626 A:middle
So that's just the
quick overview

227
00:10:17,626 --> 00:10:19,536 A:middle
of how we're doing some
experiments inside this

228
00:10:19,536 --> 00:10:21,096 A:middle
development version
of GarageBand

229
00:10:21,186 --> 00:10:24,056 A:middle
with the new Inter-App
Audio APIs.

230
00:10:24,056 --> 00:10:25,976 A:middle
And next, we're going
to bring up Doug

231
00:10:25,976 --> 00:10:27,996 A:middle
to give you a little more
detail about how some

232
00:10:28,136 --> 00:10:36,726 A:middle
of the stuff works
under the hood.

233
00:10:36,726 --> 00:10:36,793 A:middle
[ Applause ]

234
00:10:36,793 --> 00:10:39,286 A:middle
>> Thank you Alec.

235
00:10:39,286 --> 00:10:42,096 A:middle
Hi, my name is Doug Wyatt, I'm a
plumber in the Core Audio group.

236
00:10:42,686 --> 00:10:44,896 A:middle
I'd like to present to
you some of the details

237
00:10:44,896 --> 00:10:46,776 A:middle
of the Inter-App Audio APIs.

238
00:10:48,376 --> 00:10:52,956 A:middle
So, conceptually here, we
have two kinds of applications

239
00:10:52,956 --> 00:10:54,746 A:middle
which we call the
host application

240
00:10:55,146 --> 00:10:56,476 A:middle
and the node application.

241
00:10:57,296 --> 00:10:58,536 A:middle
The fundamental distinction

242
00:10:58,536 --> 00:11:01,556 A:middle
between these two
applications is that the host is

243
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

244
00:11:01,556 --> 00:11:04,066 A:middle
where we ultimately
want the audio coming

245
00:11:04,066 --> 00:11:06,296 A:middle
from the node application
to end up.

246
00:11:06,296 --> 00:11:09,766 A:middle
So, GarageBand in this
example was a host application.

247
00:11:09,766 --> 00:11:12,626 A:middle
It was receiving audio from
the sampler application

248
00:11:13,066 --> 00:11:15,256 A:middle
and from the delay
effect application.

249
00:11:15,816 --> 00:11:18,396 A:middle
So, given these two
kinds of applications,

250
00:11:18,916 --> 00:11:20,256 A:middle
we're going to look at APIs

251
00:11:20,256 --> 00:11:22,776 A:middle
for how node applications
can register themselves

252
00:11:22,776 --> 00:11:26,306 A:middle
with the system and how host
applications can discover those

253
00:11:26,306 --> 00:11:27,916 A:middle
registered node applications.

254
00:11:28,816 --> 00:11:32,206 A:middle
We'll look at how
host applications can

255
00:11:32,206 --> 00:11:35,766 A:middle
of initiate connections through
the system to node applications.

256
00:11:35,766 --> 00:11:38,476 A:middle
And once those connections
are established,

257
00:11:38,846 --> 00:11:41,476 A:middle
the two applications can stream
audio between each other.

258
00:11:41,846 --> 00:11:45,986 A:middle
But, again, primarily the
destination has to be the host

259
00:11:46,036 --> 00:11:47,826 A:middle
that could optionally
send the audio to the node

260
00:11:47,826 --> 00:11:49,886 A:middle
if the node is providing
an effect.

261
00:11:50,506 --> 00:11:54,266 A:middle
We'll look at how furthermore
host applications can send MIDI

262
00:11:54,266 --> 00:11:57,576 A:middle
events to node applications to
control their audio rendering.

263
00:11:58,106 --> 00:12:01,656 A:middle
So for example, with
that sampler application,

264
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

265
00:12:01,656 --> 00:12:04,106 A:middle
the host could have been
actually sending the MIDI nodes

266
00:12:04,556 --> 00:12:08,946 A:middle
to the sampler and receiving
the rendered audio back.

267
00:12:09,556 --> 00:12:11,766 A:middle
We'll look at some interfaces

268
00:12:11,766 --> 00:12:14,226 A:middle
where the host can
express information

269
00:12:14,226 --> 00:12:17,016 A:middle
about its transport
controls and transport state

270
00:12:17,016 --> 00:12:19,826 A:middle
and timeline position
to node applications.

271
00:12:20,036 --> 00:12:21,306 A:middle
And finally, we'll look

272
00:12:21,306 --> 00:12:23,866 A:middle
at how node applications
can remotely control

273
00:12:23,866 --> 00:12:24,976 A:middle
host applications.

274
00:12:24,976 --> 00:12:28,416 A:middle
So, let's look inside
host applications.

275
00:12:28,956 --> 00:12:31,636 A:middle
So, this is your
basic standalone music

276
00:12:31,636 --> 00:12:33,516 A:middle
or audio application on iOS.

277
00:12:34,106 --> 00:12:38,366 A:middle
We have AURemoteIO audio unit
and its function is to connect

278
00:12:38,466 --> 00:12:41,606 A:middle
to the audio input and
output system with zero--

279
00:12:41,606 --> 00:12:45,876 A:middle
well, very low latency using
pretty much the same mechanisms

280
00:12:45,876 --> 00:12:49,426 A:middle
as on the desktop but always
through the RemoteIO audio unit.

281
00:12:49,786 --> 00:12:53,506 A:middle
So, feeding the audio unit, we
have the host's audio engine

282
00:12:53,506 --> 00:12:55,696 A:middle
and that can be constructed
in a number of ways,

283
00:12:55,696 --> 00:12:57,316 A:middle
we'll show some examples later.

284
00:12:58,016 --> 00:13:00,196 A:middle
But for the-- the purposes
of Inter-App Audio,

285
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

286
00:13:01,176 --> 00:13:04,206 A:middle
the host engine connects
to a node application

287
00:13:04,786 --> 00:13:06,806 A:middle
by instantiating
a node audio unit.

288
00:13:07,216 --> 00:13:10,976 A:middle
This is another Apple supplied
audio unit that, in effect,

289
00:13:11,426 --> 00:13:14,546 A:middle
creates the bridge to the
remote node application

290
00:13:14,606 --> 00:13:16,546 A:middle
to communicate audio with it.

291
00:13:17,916 --> 00:13:21,256 A:middle
Now, on the node side, a
node application is also

292
00:13:21,686 --> 00:13:25,796 A:middle
by default a normal application
with an AURemoteIO that can play

293
00:13:25,796 --> 00:13:29,366 A:middle
and record as always and it's
got its own audio engine.

294
00:13:30,386 --> 00:13:33,086 A:middle
What's a little different here
is in the Inter-App scenario,

295
00:13:33,086 --> 00:13:36,976 A:middle
the node application has its
input and output redirected

296
00:13:36,976 --> 00:13:40,236 A:middle
from the mic and speaker
to the host application.

297
00:13:40,866 --> 00:13:44,646 A:middle
So, that's the node application.

298
00:13:45,636 --> 00:13:48,996 A:middle
So, you see then we're
implementing this API

299
00:13:48,996 --> 00:13:50,966 A:middle
as a series of extensions

300
00:13:50,966 --> 00:13:54,276 A:middle
to the existing
AudioUnit.framework APIs.

301
00:13:54,276 --> 00:13:57,496 A:middle
The host sees the node
application as an audio unit

302
00:13:57,496 --> 00:13:58,546 A:middle
that it communicates with

303
00:13:59,206 --> 00:14:04,536 A:middle
and the nodes AURemoteIO unit
gets redirected to the host

304
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

305
00:14:04,596 --> 00:14:06,226 A:middle
so the node's communication

306
00:14:06,226 --> 00:14:09,226 A:middle
to the host application
is through that IO unit.

307
00:14:11,696 --> 00:14:15,486 A:middle
So, to express the capabilities
of these node applications

308
00:14:15,486 --> 00:14:17,346 A:middle
and to distinguish them a bit

309
00:14:17,406 --> 00:14:20,246 A:middle
from the existing
audio unit types,

310
00:14:20,336 --> 00:14:22,126 A:middle
we have these four new types.

311
00:14:23,306 --> 00:14:26,196 A:middle
They all are the same in that
they produce audio output

312
00:14:27,046 --> 00:14:30,136 A:middle
but they differ in what input
they receive from the host.

313
00:14:30,546 --> 00:14:33,116 A:middle
We have remote generators
which require no input.

314
00:14:33,796 --> 00:14:36,456 A:middle
We have remote instruments
which take MIDI input

315
00:14:36,456 --> 00:14:38,436 A:middle
to produce output, audio output.

316
00:14:39,156 --> 00:14:41,306 A:middle
We have effects which
are audio in and out.

317
00:14:41,776 --> 00:14:45,086 A:middle
And finally, we have music
effects which take both audio

318
00:14:45,086 --> 00:14:47,546 A:middle
and MIDI input and
produce audio.

319
00:14:48,876 --> 00:14:52,546 A:middle
So, node applications
use these component types

320
00:14:52,546 --> 00:14:54,286 A:middle
to describe their capabilities.

321
00:14:54,726 --> 00:14:57,776 A:middle
And furthermore one node
application may actually have

322
00:14:58,316 --> 00:15:01,966 A:middle
multiple sets of capabilities
and may wish to present itself

323
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

324
00:15:02,096 --> 00:15:03,996 A:middle
in multiple ways to hosts.

325
00:15:04,156 --> 00:15:05,486 A:middle
As a simple example,

326
00:15:05,486 --> 00:15:09,106 A:middle
a node application may produce
audio just fine on its own,

327
00:15:09,576 --> 00:15:10,876 A:middle
in which case, it's a generator.

328
00:15:11,316 --> 00:15:15,006 A:middle
It may optionally be
able to respond to MIDI

329
00:15:15,006 --> 00:15:16,936 A:middle
in producing that audio.

330
00:15:17,616 --> 00:15:19,566 A:middle
So, it can also be a generator.

331
00:15:19,566 --> 00:15:22,436 A:middle
So, such a node application
could publish itself

332
00:15:22,436 --> 00:15:24,326 A:middle
as two different
audio components

333
00:15:24,566 --> 00:15:25,996 A:middle
with separate capabilities.

334
00:15:26,856 --> 00:15:29,036 A:middle
Another example of
that is an application

335
00:15:29,036 --> 00:15:33,336 A:middle
like a guitar amp simulator
where the application appears

336
00:15:33,336 --> 00:15:36,816 A:middle
to the user as an effect
because audio is going in,

337
00:15:37,136 --> 00:15:39,636 A:middle
it's being processed in some
way and then it comes out.

338
00:15:40,206 --> 00:15:41,746 A:middle
But from the host's
point of view,

339
00:15:42,156 --> 00:15:45,616 A:middle
this application can appear
either as a generator an effect

340
00:15:45,776 --> 00:15:48,056 A:middle
and the node can publish
itself either way.

341
00:15:48,566 --> 00:15:52,676 A:middle
For example, if a node says I'm
a generator, it can continue

342
00:15:52,676 --> 00:15:57,906 A:middle
to receive microphone or a line
input from a guitar directly

343
00:15:57,976 --> 00:16:02,736 A:middle
from the underlying AURemoteIO
while only sending the audio

344
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

345
00:16:02,736 --> 00:16:03,736 A:middle
output to the host.

346
00:16:04,066 --> 00:16:05,446 A:middle
So again, that's generator mode.

347
00:16:05,986 --> 00:16:07,906 A:middle
Or if a host application

348
00:16:07,906 --> 00:16:10,686 A:middle
like GarageBand might have
a prerecorded guitar track

349
00:16:11,176 --> 00:16:13,846 A:middle
and want to process that through
the guitar amp simulator.

350
00:16:14,306 --> 00:16:17,226 A:middle
The guitar amp simulator can
function fully as an effect,

351
00:16:17,786 --> 00:16:20,036 A:middle
not communicate with the
audio hardware at all,

352
00:16:20,556 --> 00:16:22,896 A:middle
and just communicate
the two audio streams

353
00:16:23,276 --> 00:16:27,366 A:middle
between itself and the host.

354
00:16:27,966 --> 00:16:30,416 A:middle
Let's move on and look at
some of the requirements

355
00:16:30,416 --> 00:16:33,276 A:middle
for the Inter-App Audio
feature, it's available

356
00:16:33,276 --> 00:16:35,826 A:middle
on most iOS 7 compatible
devices,

357
00:16:35,866 --> 00:16:37,886 A:middle
the exception being
the iPhone 4.

358
00:16:38,196 --> 00:16:41,476 A:middle
And on the iPhone 4, you
don't really have to deal

359
00:16:41,476 --> 00:16:43,786 A:middle
with this specially because
what will happen is node

360
00:16:43,786 --> 00:16:46,916 A:middle
applications, if they
attempt to register themselves

361
00:16:46,916 --> 00:16:50,276 A:middle
with the system, those calls
will just fail silently,

362
00:16:50,276 --> 00:16:51,396 A:middle
the system will ignore them.

363
00:16:51,396 --> 00:16:55,776 A:middle
And on the host side, the
host will simply see no node

364
00:16:55,776 --> 00:16:58,626 A:middle
applications on the system.

365
00:16:58,626 --> 00:17:00,676 A:middle
Both host and node
applications need

366
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

367
00:17:00,676 --> 00:17:04,185 A:middle
to have a new entitlement called
"inter-app-audio" and this--

368
00:17:04,656 --> 00:17:06,566 A:middle
you can set this
for your application

369
00:17:06,566 --> 00:17:08,396 A:middle
in the Xcode Capabilities tab.

370
00:17:10,156 --> 00:17:15,296 A:middle
Furthermore, most applications
will want to have audio

371
00:17:15,296 --> 00:17:17,076 A:middle
in their UIBackgroundModes.

372
00:17:17,806 --> 00:17:21,036 A:middle
Most especially hosts
for obvious reasons

373
00:17:21,086 --> 00:17:23,925 A:middle
because hosts will keep
running their engines

374
00:17:23,925 --> 00:17:25,226 A:middle
when nodes are on
the foreground.

375
00:17:25,736 --> 00:17:29,436 A:middle
Also, nodes like the guitar amp
simulator I just mentioned may

376
00:17:29,436 --> 00:17:32,206 A:middle
want to continue accessing the
mic and to be able to do that,

377
00:17:32,256 --> 00:17:34,786 A:middle
they too need to have the
audio background mode.

378
00:17:35,766 --> 00:17:37,686 A:middle
One final requirement for nodes

379
00:17:37,686 --> 00:17:39,846 A:middle
in particular is
the MixWithOthers

380
00:17:39,846 --> 00:17:41,616 A:middle
AudioSessionCategoryOption.

381
00:17:42,276 --> 00:17:45,556 A:middle
Hosts can go either
way on this one.

382
00:17:45,666 --> 00:17:47,576 A:middle
We'll get into that
in more detail later.

383
00:17:48,176 --> 00:17:52,086 A:middle
OK. Getting in to the nuts
and bolts of the APIs here,

384
00:17:52,086 --> 00:17:54,276 A:middle
let's look at how node
applications can register

385
00:17:54,276 --> 00:17:57,876 A:middle
themselves with the system.

386
00:17:57,876 --> 00:18:01,656 A:middle
So, there's two pieces
of registering one's self

387
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

388
00:18:01,656 --> 00:18:02,766 A:middle
for a node application.

389
00:18:03,246 --> 00:18:07,186 A:middle
The first is an Info.plist
entry called AudioComponents.

390
00:18:07,626 --> 00:18:11,306 A:middle
So, the presence of this
Info.plist entry makes the app

391
00:18:11,586 --> 00:18:15,096 A:middle
discoverable and
launchable to the system.

392
00:18:15,396 --> 00:18:16,896 A:middle
The system knows,
oh I've got one

393
00:18:16,896 --> 00:18:18,706 A:middle
of this node applications
installed.

394
00:18:19,226 --> 00:18:22,866 A:middle
The second part of registration
is for the node application

395
00:18:22,866 --> 00:18:26,286 A:middle
to call AudioOutputUnitPublish
which checks

396
00:18:26,286 --> 00:18:30,106 A:middle
in that registration that it
advertised in its Info.plist.

397
00:18:30,286 --> 00:18:33,146 A:middle
It says, I've been launched and
here I am ready to communicate.

398
00:18:33,366 --> 00:18:36,316 A:middle
So let's look at those two
pieces in a little detail.

399
00:18:37,096 --> 00:18:41,286 A:middle
So here is the AudioComponents
entry in the Info.plist.

400
00:18:41,286 --> 00:18:43,756 A:middle
Its value is an array
and in that array,

401
00:18:43,756 --> 00:18:46,436 A:middle
there is a dictionary
for every AudioComponent

402
00:18:46,436 --> 00:18:47,836 A:middle
that the node wants to register.

403
00:18:48,576 --> 00:18:50,986 A:middle
And in that dictionary,
if you're familiar

404
00:18:50,986 --> 00:18:53,056 A:middle
with AudioComponentDescriptions
already,

405
00:18:53,056 --> 00:18:55,276 A:middle
you'll see some familiar
fields there.

406
00:18:55,276 --> 00:18:59,946 A:middle
There is the type, subtype and
manufacturer along with the name

407
00:18:59,946 --> 00:19:01,056 A:middle
and the version number.

408
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

409
00:19:01,656 --> 00:19:04,186 A:middle
So, that completely
describes the AudioComponent

410
00:19:04,566 --> 00:19:07,376 A:middle
that the node application
is advertising.

411
00:19:07,976 --> 00:19:13,026 A:middle
So, moving on to the second
part of the registration here,

412
00:19:13,636 --> 00:19:16,306 A:middle
this is when the node
application launches,

413
00:19:16,806 --> 00:19:21,676 A:middle
the first piece of code here
is the node's normal process

414
00:19:21,676 --> 00:19:24,416 A:middle
for creating its
AURemoteIO when it launches.

415
00:19:24,846 --> 00:19:26,856 A:middle
It creates an
AudioComponentDescription

416
00:19:26,856 --> 00:19:30,176 A:middle
describing the Apple
AURemoteIO instance,

417
00:19:30,176 --> 00:19:32,146 A:middle
it uses AudioComponentFindNext

418
00:19:32,146 --> 00:19:35,476 A:middle
to go find the AudioComponent
for the AURemoteIO.

419
00:19:35,996 --> 00:19:38,896 A:middle
And finally, it creates an
instance of the AURemoteIO

420
00:19:38,896 --> 00:19:43,326 A:middle
and this is something just about
every audio and music app on--

421
00:19:43,556 --> 00:19:47,586 A:middle
today will do for creating
a low latency IO channel.

422
00:19:48,106 --> 00:19:52,166 A:middle
What's new is that the node
application, to participate

423
00:19:52,166 --> 00:19:56,696 A:middle
in Inter-App Audio is now
going to connect that IO Unit

424
00:19:56,736 --> 00:20:00,086 A:middle
that it just created with
the component description

425
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

426
00:20:00,086 --> 00:20:02,356 A:middle
that was published in
the Info.plist entry.

427
00:20:02,976 --> 00:20:06,126 A:middle
So, to do that, we're
seeing this code here

428
00:20:06,126 --> 00:20:08,506 A:middle
that that node creates an
AudioComponentDescription

429
00:20:08,506 --> 00:20:11,736 A:middle
which matches the one in the
Info.plist we saw a moment ago.

430
00:20:12,356 --> 00:20:15,736 A:middle
It supplies the name and version
number and passes all that along

431
00:20:15,736 --> 00:20:17,456 A:middle
with the AURemoteIO instance

432
00:20:17,456 --> 00:20:20,806 A:middle
to a new API called
AudioOutputUnitPublish.

433
00:20:20,976 --> 00:20:24,476 A:middle
So again, that connects what
was advertised in the Info.plist

434
00:20:25,126 --> 00:20:29,476 A:middle
with the actual RemoteIO
instance in the application

435
00:20:29,806 --> 00:20:32,106 A:middle
to which the host
application will connect

436
00:20:32,106 --> 00:20:34,216 A:middle
as we'll see in a little bit.

437
00:20:34,216 --> 00:20:36,556 A:middle
So, to make this all
work, a requirement

438
00:20:36,556 --> 00:20:40,726 A:middle
of the node application is
to publish that RemoteIO unit

439
00:20:40,836 --> 00:20:44,426 A:middle
when it launches because the
node application is going

440
00:20:44,426 --> 00:20:46,986 A:middle
to get launched by host
applications when--

441
00:20:46,986 --> 00:20:48,896 A:middle
at times when the user
what's to use them.

442
00:20:49,936 --> 00:20:53,366 A:middle
And so, the node application
basically has to acknowledge,

443
00:20:53,366 --> 00:20:54,526 A:middle
I'm here, I've been launched.

444
00:20:55,666 --> 00:21:00,306 A:middle
And so, you can see then why the
Info.plist entry and the call

445
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

446
00:21:00,306 --> 00:21:04,636 A:middle
to AudioOutputUnitPublish
must have the same component

447
00:21:04,636 --> 00:21:06,426 A:middle
descriptions, names,
and versions.

448
00:21:07,426 --> 00:21:09,416 A:middle
One note here is
that by convention,

449
00:21:09,826 --> 00:21:13,106 A:middle
the component name should
contain your manufacture name

450
00:21:13,106 --> 00:21:17,236 A:middle
and application name and that
lets host applications sort the

451
00:21:17,236 --> 00:21:20,416 A:middle
available node applications by
manufacture name if they like.

452
00:21:20,416 --> 00:21:26,516 A:middle
So, that's the registration
process for node applications,

453
00:21:26,516 --> 00:21:29,416 A:middle
let's look at how host
applications can discover

454
00:21:29,416 --> 00:21:30,556 A:middle
those registrations.

455
00:21:33,836 --> 00:21:37,426 A:middle
So again, if you've used the
AudioComponent calls before,

456
00:21:37,626 --> 00:21:39,346 A:middle
this should look
fairly familiar.

457
00:21:39,726 --> 00:21:43,456 A:middle
What we here-- have here is a
loop where we want to iterate

458
00:21:43,526 --> 00:21:46,126 A:middle
through all of the components on
the system because we're looking

459
00:21:46,126 --> 00:21:48,656 A:middle
for nodes and there
are multiple types.

460
00:21:48,786 --> 00:21:50,286 A:middle
So, the simplest
way to do that is

461
00:21:50,286 --> 00:21:52,506 A:middle
to create a wild card
component description

462
00:21:52,506 --> 00:21:54,916 A:middle
and that's the searchDesc,
it's full of zeros.

463
00:21:55,586 --> 00:21:58,906 A:middle
And so then, this loop will
call AudioComponentFindNext

464
00:21:58,906 --> 00:22:02,936 A:middle
repeatedly and that
will yield in turn each

465
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

466
00:22:02,936 --> 00:22:05,166 A:middle
of the AudioComponents
on the system which are

467
00:22:05,526 --> 00:22:06,886 A:middle
in the local variable comp.

468
00:22:07,516 --> 00:22:10,486 A:middle
When we find null, then we've
gotten to the end of the list

469
00:22:10,486 --> 00:22:14,006 A:middle
of all the components in
the system and we're done

470
00:22:14,006 --> 00:22:15,286 A:middle
with our loop, we'll
have found them all.

471
00:22:16,126 --> 00:22:18,256 A:middle
Now, for each component on
the system, what we want

472
00:22:18,256 --> 00:22:20,716 A:middle
to do is call
AudioComponentGetDescription

473
00:22:21,596 --> 00:22:24,486 A:middle
and this will supply to us
the AudioComponent description

474
00:22:24,576 --> 00:22:27,446 A:middle
of the actual unit as
opposed to that wild card

475
00:22:27,646 --> 00:22:28,786 A:middle
that we used for searching.

476
00:22:29,436 --> 00:22:32,816 A:middle
So now, in foundDesc, we can
look at its component type

477
00:22:32,816 --> 00:22:37,706 A:middle
and see if it's one of the
four inter-app audio unit types

478
00:22:37,706 --> 00:22:40,466 A:middle
that we're interested in, the
RemoteEffects, RemoteGenerator,

479
00:22:40,466 --> 00:22:42,196 A:middle
RemoteInstrument, and
RemoteMusicEffect.

480
00:22:42,676 --> 00:22:46,596 A:middle
If we see one of those, then
we know we found the node.

481
00:22:47,136 --> 00:22:49,336 A:middle
OK. So the host has
found a node.

482
00:22:49,926 --> 00:22:52,496 A:middle
So now, I'm going to
walk through a little bit

483
00:22:52,496 --> 00:22:54,746 A:middle
of code here from one
of our sample apps.

484
00:22:55,296 --> 00:22:59,126 A:middle
It creates an objective C
object of its own just as a way

485
00:22:59,126 --> 00:23:01,486 A:middle
of storing information about
the nodes that it's found.

486
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

487
00:23:02,436 --> 00:23:05,686 A:middle
And it calls this class
RemoteAU and it stores away

488
00:23:05,686 --> 00:23:08,266 A:middle
into it the component
description that was found

489
00:23:08,806 --> 00:23:10,866 A:middle
and the AudioComponent
that was found.

490
00:23:11,886 --> 00:23:14,516 A:middle
It also fetches the
component's name and stores

491
00:23:14,516 --> 00:23:17,806 A:middle
that in the field of
the RemoteAU object.

492
00:23:18,696 --> 00:23:22,806 A:middle
It sets the image from
AudioComponentGetIcon

493
00:23:22,806 --> 00:23:25,536 A:middle
which is a new API call which
works with inter-app audio.

494
00:23:26,186 --> 00:23:28,046 A:middle
This gives you the
application of--

495
00:23:28,376 --> 00:23:30,966 A:middle
I'm sorry, the icon of
the node application.

496
00:23:32,836 --> 00:23:36,996 A:middle
We can also discover the time at
which the user last interacted

497
00:23:37,086 --> 00:23:41,656 A:middle
with the node app and this
can be useful if we want

498
00:23:41,696 --> 00:23:46,286 A:middle
to sort a list of available
node applications by time

499
00:23:46,286 --> 00:23:47,926 A:middle
of when they were
most recently used,

500
00:23:48,336 --> 00:23:50,016 A:middle
the way the home screen does.

501
00:23:50,686 --> 00:23:52,936 A:middle
So we've gathered up
all this information

502
00:23:52,936 --> 00:23:56,146 A:middle
about the node application,
and now we've built an array

503
00:23:56,146 --> 00:24:00,216 A:middle
from which we can drive a
table view and present the user

504
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

505
00:24:00,216 --> 00:24:05,306 A:middle
with a choice of node
applications to deal with.

506
00:24:06,046 --> 00:24:09,606 A:middle
One wrinkle though is having
cached all that information

507
00:24:09,606 --> 00:24:13,986 A:middle
in an array, it can become stale
and out of sync with the system.

508
00:24:14,436 --> 00:24:17,606 A:middle
Most notably, when apps
are installed and deleted

509
00:24:17,606 --> 00:24:21,656 A:middle
so if you find yourself caching
list of components like this,

510
00:24:21,656 --> 00:24:25,456 A:middle
you should probably also
listen to this new notification

511
00:24:25,456 --> 00:24:28,966 A:middle
that we supply, its name is
AudioComponentRegistrations

512
00:24:28,966 --> 00:24:30,196 A:middle
ChangedNotification.

513
00:24:30,586 --> 00:24:35,916 A:middle
So, you can pass that
to NSNotification center

514
00:24:35,916 --> 00:24:37,846 A:middle
to register for a notification.

515
00:24:38,136 --> 00:24:42,216 A:middle
In this example, we're supplying
a block to be called and then

516
00:24:42,216 --> 00:24:45,426 A:middle
that block which is called when
the notification or rather,

517
00:24:45,426 --> 00:24:48,166 A:middle
when the registration has
changed, we can refresh

518
00:24:48,236 --> 00:24:50,366 A:middle
that cached list of
audio units we built.

519
00:24:51,016 --> 00:24:55,346 A:middle
So, that's the process of
discovering node apps for host.

520
00:24:55,696 --> 00:24:57,656 A:middle
So now, we've built up a table

521
00:24:58,256 --> 00:25:01,656 A:middle
and maybe the user has
selected one of them

522
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

523
00:25:01,656 --> 00:25:03,246 A:middle
and in the host application now,

524
00:25:03,246 --> 00:25:05,116 A:middle
we want to actually
establish a connection

525
00:25:05,516 --> 00:25:08,836 A:middle
to the node application so
let's look at how that works.

526
00:25:09,886 --> 00:25:13,716 A:middle
The first step is very
simple because we held

527
00:25:13,716 --> 00:25:16,646 A:middle
on to the AudioComponent
of that node.

528
00:25:17,476 --> 00:25:20,356 A:middle
Now, all we have to do is create
an instance of that component

529
00:25:20,436 --> 00:25:22,266 A:middle
and now, we have an audio unit

530
00:25:22,386 --> 00:25:24,296 A:middle
through which we can
communicate with the node.

531
00:25:24,756 --> 00:25:27,746 A:middle
It's worth mentioning
that this is the moment

532
00:25:27,746 --> 00:25:30,556 A:middle
at which the node
application will get launched

533
00:25:30,556 --> 00:25:32,506 A:middle
into the background if it
it's not already running.

534
00:25:33,796 --> 00:25:36,006 A:middle
And we'll look it all the
mechanics of what happens

535
00:25:36,006 --> 00:25:37,486 A:middle
on the node side of that later.

536
00:25:37,826 --> 00:25:39,736 A:middle
Right now, we're just going
to focus on the host side.

537
00:25:40,446 --> 00:25:44,826 A:middle
So, the host has to do a fair--
a few steps here to get ready

538
00:25:44,906 --> 00:25:48,046 A:middle
to steam audio between
itself and the node.

539
00:25:49,396 --> 00:25:53,406 A:middle
Most importantly, the
host must be communicating

540
00:25:53,406 --> 00:25:57,376 A:middle
with the node using the same
hardware sample rate as--

541
00:25:57,466 --> 00:25:59,146 A:middle
or the same sample
rate as the hardware.

542
00:25:59,806 --> 00:26:04,036 A:middle
So, to be absolutely sure the
hardware sample rate is what

543
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

544
00:26:04,036 --> 00:26:07,156 A:middle
it's supposed to be, we should
be making our audio session

545
00:26:07,156 --> 00:26:08,666 A:middle
active if we haven't already.

546
00:26:09,996 --> 00:26:11,876 A:middle
So, once having done that,

547
00:26:11,876 --> 00:26:14,786 A:middle
then we can specify the audio
stream basic description

548
00:26:15,236 --> 00:26:19,516 A:middle
which is a detailed
description of the audio format

549
00:26:20,466 --> 00:26:23,116 A:middle
that the host wishes to use
to communicate with the node.

550
00:26:24,286 --> 00:26:26,936 A:middle
So, we can choose
mono or stereo.

551
00:26:26,936 --> 00:26:28,756 A:middle
In this example,
I've chosen stereo.

552
00:26:28,756 --> 00:26:32,136 A:middle
Here is where we're using
the hardware sample rate

553
00:26:32,756 --> 00:26:38,166 A:middle
And these lines of code here
are basically specifying 32 bit

554
00:26:38,166 --> 00:26:40,206 A:middle
floating-point, non-interleaved.

555
00:26:40,596 --> 00:26:43,866 A:middle
Now, the host application can
choose any format it likes here

556
00:26:44,006 --> 00:26:47,356 A:middle
and the system will perform
whatever conversions are

557
00:26:47,356 --> 00:26:50,926 A:middle
necessary as long as there's
not a sample rate conversion

558
00:26:51,086 --> 00:26:51,786 A:middle
being requested.

559
00:26:51,786 --> 00:26:56,196 A:middle
Again, you must use the sample
rate that matches the hardware.

560
00:26:57,426 --> 00:26:58,136 A:middle
All right.

561
00:26:58,136 --> 00:27:00,896 A:middle
Now, we have built up an
audio stream basic description

562
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

563
00:27:00,896 --> 00:27:02,906 A:middle
and we can use
AudioUnitSetProperty

564
00:27:02,906 --> 00:27:06,496 A:middle
on the node AudioUnit for
the stream format property

565
00:27:07,366 --> 00:27:10,476 A:middle
and this is specifying-- since
it's in the output scope,

566
00:27:10,476 --> 00:27:14,596 A:middle
this is specifying the output
format of the audio we need

567
00:27:14,596 --> 00:27:15,746 A:middle
to receive from the node.

568
00:27:15,866 --> 00:27:18,856 A:middle
If we're working with a
generator or instrument

569
00:27:18,856 --> 00:27:21,276 A:middle
which don't take audio
input, that's it, we've--

570
00:27:21,426 --> 00:27:25,786 A:middle
we're done, we've just
specified the output format.

571
00:27:26,146 --> 00:27:27,566 A:middle
But if we're dealing
with an effect,

572
00:27:27,686 --> 00:27:29,966 A:middle
then we should also
specify the input format.

573
00:27:30,426 --> 00:27:32,856 A:middle
And in many cases, it's
going to be identical

574
00:27:32,956 --> 00:27:35,396 A:middle
to the output format and so,

575
00:27:35,396 --> 00:27:39,486 A:middle
we can make that same call
using the input scope just

576
00:27:39,516 --> 00:27:42,936 A:middle
that the input format that we're
going to supply to the node.

577
00:27:44,076 --> 00:27:49,806 A:middle
So, having specified formats,
we can look how we're going

578
00:27:49,806 --> 00:27:52,426 A:middle
to get audio from the
host into the node

579
00:27:52,926 --> 00:27:56,496 A:middle
and this is starting it get into
the details of multiple ways

580
00:27:56,496 --> 00:27:58,776 A:middle
that your host may
be interacting

581
00:27:58,776 --> 00:27:59,866 A:middle
with the node AudioUnit.

582
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

583
00:28:00,216 --> 00:28:03,256 A:middle
Now, since we're connecting
input, this is only for effects

584
00:28:04,616 --> 00:28:08,426 A:middle
and the host at this point
can supply input to a node

585
00:28:08,426 --> 00:28:12,926 A:middle
from another audio unit using
AUGraphConnectNodeInput.

586
00:28:13,056 --> 00:28:15,836 A:middle
AUGraph is a higher level API
which I'm just going to touch

587
00:28:15,836 --> 00:28:20,586 A:middle
on a few times today but you can
use AUGraph to build up graphs

588
00:28:20,586 --> 00:28:23,236 A:middle
or a series of connections
between audio units.

589
00:28:24,316 --> 00:28:27,626 A:middle
The other way to make a
connection to the node's input

590
00:28:27,916 --> 00:28:30,096 A:middle
from some other audio unit is

591
00:28:30,096 --> 00:28:32,336 A:middle
with the AudioUnitProperty
MakeConnection.

592
00:28:33,606 --> 00:28:36,956 A:middle
Alternatively, a host can simply
supply a callback function

593
00:28:37,396 --> 00:28:39,236 A:middle
with the SetRenderCallback
property.

594
00:28:39,646 --> 00:28:42,066 A:middle
This callback function
gets called at render time

595
00:28:42,066 --> 00:28:46,016 A:middle
and the host supplies the audio
samples to be given to the node.

596
00:28:46,426 --> 00:28:49,036 A:middle
Now, as far as connecting
the output of the node,

597
00:28:49,116 --> 00:28:52,646 A:middle
this too depends on the way
you built your host engine.

598
00:28:53,196 --> 00:28:54,996 A:middle
If you're using audio
units, you want to connect

599
00:28:54,996 --> 00:28:57,366 A:middle
to the node output to
some other audio unit.

600
00:28:57,796 --> 00:29:00,416 A:middle
You can use the MakeConnection
property again.

601
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

602
00:29:01,146 --> 00:29:04,846 A:middle
If however you're pulling
audio into a custom engine,

603
00:29:04,846 --> 00:29:06,426 A:middle
then you would call
AudioUnitRender

604
00:29:06,996 --> 00:29:09,946 A:middle
but there's no setup
at this time for that.

605
00:29:11,276 --> 00:29:12,956 A:middle
We'll look at the
rendering process

606
00:29:12,956 --> 00:29:14,356 A:middle
in more detail a little later.

607
00:29:14,956 --> 00:29:22,006 A:middle
OK. One last a bit of mechanics
here that a host needs to do

608
00:29:22,006 --> 00:29:25,486 A:middle
to establish a reliable
connection to a node or actually

609
00:29:25,486 --> 00:29:29,706 A:middle
to reliably handle bad things
happening with the node is

610
00:29:30,166 --> 00:29:32,866 A:middle
to look out for what happens
when nodes become disconnected.

611
00:29:33,196 --> 00:29:35,996 A:middle
This could happen automatically
if the node app crashes,

612
00:29:36,326 --> 00:29:39,556 A:middle
if the system ejects it from
this memory before being

613
00:29:39,936 --> 00:29:41,726 A:middle
under memory pressure.

614
00:29:42,456 --> 00:29:44,196 A:middle
Also, if the host fails

615
00:29:44,196 --> 00:29:47,836 A:middle
to render the node
application regularly enough,

616
00:29:47,836 --> 00:29:49,526 A:middle
the system will evict it from--

617
00:29:49,866 --> 00:29:51,746 A:middle
or I'm sorry, will
break the connection.

618
00:29:52,206 --> 00:29:56,196 A:middle
When these things happen, then
the node AudioUnit becomes,

619
00:29:56,256 --> 00:29:59,466 A:middle
in effect of zombie,
meaning that it's--

620
00:29:59,596 --> 00:30:01,326 A:middle
there's still an
audio unit there.

621
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

622
00:30:01,426 --> 00:30:05,356 A:middle
You can make API calls on
it but they won't crash

623
00:30:06,256 --> 00:30:08,606 A:middle
but you will get errors
back and that's the error

624
00:30:08,606 --> 00:30:10,746 A:middle
that you'll get back, the
InstanceInvalidated error.

625
00:30:11,996 --> 00:30:16,136 A:middle
The mechanics of establishing
that disconnection callback -

626
00:30:16,456 --> 00:30:20,836 A:middle
we call
AudioUnitAddPropertyListener

627
00:30:20,956 --> 00:30:23,556 A:middle
for this new property
IsInterAppConnected.

628
00:30:24,696 --> 00:30:26,876 A:middle
Here is what you would do
in the connection listener,

629
00:30:26,876 --> 00:30:30,996 A:middle
you can fetch current value of
the property and see if it is 0

630
00:30:31,146 --> 00:30:35,446 A:middle
and if the local variable here
connected has become zero,

631
00:30:35,946 --> 00:30:38,136 A:middle
then you know the node
application has become

632
00:30:38,576 --> 00:30:40,646 A:middle
disconnected and you
should react accordingly.

633
00:30:41,716 --> 00:30:46,806 A:middle
So, all of that prep work
has led us up to the point

634
00:30:46,806 --> 00:30:49,066 A:middle
or we're ready to actually
initialize the node.

635
00:30:49,296 --> 00:30:53,736 A:middle
Now, the AudioUnit
initialize call basically says

636
00:30:53,736 --> 00:30:55,986 A:middle
to the system and
the other AudioUnit.

637
00:30:56,286 --> 00:30:57,436 A:middle
Here, allocate all

638
00:30:57,436 --> 00:30:59,406 A:middle
of the resources you
need for rendering.

639
00:30:59,986 --> 00:31:01,496 A:middle
In the case of inter-app audio,

640
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

641
00:31:01,886 --> 00:31:05,146 A:middle
the system at this point is
also allocating some resources

642
00:31:05,146 --> 00:31:07,626 A:middle
on behalf of that
connection such as the buffers

643
00:31:07,626 --> 00:31:10,706 A:middle
between the applications and
the real-time rendering thread

644
00:31:10,706 --> 00:31:11,966 A:middle
in the node application.

645
00:31:12,956 --> 00:31:14,656 A:middle
So, it's important to realize.

646
00:31:14,656 --> 00:31:18,756 A:middle
This is a point at which you are
beginning to consume resources

647
00:31:19,046 --> 00:31:21,526 A:middle
and as such, you have
the responsibility now

648
00:31:22,296 --> 00:31:24,706 A:middle
of calling AudioUnitRender
regularly

649
00:31:25,746 --> 00:31:27,846 A:middle
on this node audio unit.

650
00:31:29,296 --> 00:31:32,506 A:middle
So, that's the process
of setting up a host

651
00:31:32,506 --> 00:31:33,666 A:middle
to communicate with a node.

652
00:31:33,806 --> 00:31:36,896 A:middle
You activate your audio session,
you set your stream formats,

653
00:31:37,026 --> 00:31:39,966 A:middle
you connect your audio input,
add a disconnection listener,

654
00:31:40,026 --> 00:31:42,256 A:middle
and finally call
AudioUnitInitialize.

655
00:31:42,256 --> 00:31:46,666 A:middle
So having done that, you're at
the point now where you're ready

656
00:31:46,666 --> 00:31:50,116 A:middle
to begin streaming audio
between the two applications.

657
00:31:51,246 --> 00:31:54,466 A:middle
Let's look inside a host
application's engine

658
00:31:54,466 --> 00:31:55,206 A:middle
in more detail.

659
00:31:55,656 --> 00:31:59,036 A:middle
This is kind of a wonderfully
simple way to do things

660
00:31:59,036 --> 00:32:03,166 A:middle
if you can get your work
done using Apple AudioUnits.

661
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

662
00:32:03,166 --> 00:32:06,506 A:middle
So, the green box, the
green dotted lines--

663
00:32:07,256 --> 00:32:09,116 A:middle
box represents a host engine

664
00:32:09,116 --> 00:32:12,606 A:middle
but those red boxes inside are
all Apple supplied audio units.

665
00:32:12,876 --> 00:32:15,136 A:middle
So, there is the AURemoteIO,

666
00:32:15,136 --> 00:32:17,176 A:middle
we have a mixer AudioUnite
feeding that.

667
00:32:17,586 --> 00:32:20,356 A:middle
In feeding the mixer, we
have a file player AudioUnit

668
00:32:20,856 --> 00:32:22,836 A:middle
and the node AudioUnit.

669
00:32:23,736 --> 00:32:27,116 A:middle
But of course, there are many
things you would want to do

670
00:32:27,116 --> 00:32:29,586 A:middle
with audio that Apple doesn't
give you AudioUnits for.

671
00:32:29,586 --> 00:32:31,876 A:middle
If you want to that, then
you're going to write some code

672
00:32:31,876 --> 00:32:34,956 A:middle
of your own represented
by the green box

673
00:32:34,956 --> 00:32:36,556 A:middle
with the squiggly brackets.

674
00:32:36,946 --> 00:32:41,336 A:middle
So here, your engine is
feeding the AURemoteIO

675
00:32:41,536 --> 00:32:44,136 A:middle
and if you've written
an app like this before,

676
00:32:44,136 --> 00:32:47,386 A:middle
you know the way to provide
input to an AURemoteIO

677
00:32:47,386 --> 00:32:50,526 A:middle
from your own engine is with
the SetRenderCallback property.

678
00:32:51,266 --> 00:32:53,666 A:middle
And now in this case,
to fetch the audio

679
00:32:53,666 --> 00:32:57,316 A:middle
from the node AudioUnit, you
would call AudioUnitRender.

680
00:32:57,916 --> 00:33:03,126 A:middle
OK. So, that's a bunch
of stuff about how we--

681
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

682
00:33:03,286 --> 00:33:06,116 A:middle
a host application
interact with a node.

683
00:33:06,116 --> 00:33:09,676 A:middle
One final nice thing to
do for the user here is

684
00:33:09,676 --> 00:33:11,896 A:middle
to provide a way for the user

685
00:33:12,666 --> 00:33:15,806 A:middle
to bring the node
application to the foreground.

686
00:33:16,176 --> 00:33:22,296 A:middle
So, we can do this by asking
the audio unit for a PeerURL

687
00:33:22,296 --> 00:33:25,436 A:middle
and this URL is only
valid during the life

688
00:33:25,436 --> 00:33:26,276 A:middle
of the connection.

689
00:33:26,276 --> 00:33:28,166 A:middle
You don't want to hold on to it

690
00:33:28,166 --> 00:33:30,266 A:middle
because it's not going
to be useful later.

691
00:33:30,716 --> 00:33:33,576 A:middle
But right before the user
wants to switch in response

692
00:33:33,606 --> 00:33:35,016 A:middle
to that Icon tap or whatever,

693
00:33:35,556 --> 00:33:38,966 A:middle
you can fetch the PeerURL then
pass that to UIApplication

694
00:33:39,026 --> 00:33:43,196 A:middle
and ask it to open that URL and
that will accomplish the switch

695
00:33:43,446 --> 00:33:45,936 A:middle
of bringing the node
application to the foreground.

696
00:33:50,936 --> 00:33:54,446 A:middle
So, let's go back just
a little bit and look

697
00:33:54,516 --> 00:33:57,156 A:middle
at how node applications
see the process

698
00:33:57,156 --> 00:33:58,866 A:middle
of becoming connected to hosts.

699
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

700
00:34:01,556 --> 00:34:04,956 A:middle
So, the most important thing to
think about here as the author

701
00:34:04,956 --> 00:34:06,336 A:middle
of a node application is

702
00:34:06,416 --> 00:34:10,936 A:middle
that when the user opens
your application explicitly

703
00:34:10,936 --> 00:34:12,496 A:middle
from the home screen,
you're launched

704
00:34:12,496 --> 00:34:15,366 A:middle
into the foreground state,
you're ready start making music.

705
00:34:16,576 --> 00:34:19,306 A:middle
But if you're being
launched from the context

706
00:34:19,306 --> 00:34:22,545 A:middle
of a host application, you're
actually going to get launched

707
00:34:22,545 --> 00:34:25,045 A:middle
into the background state and
there are some limitations

708
00:34:25,045 --> 00:34:26,326 A:middle
about what you can do at this--

709
00:34:26,446 --> 00:34:28,436 A:middle
in this state and there's
also a requirement here.

710
00:34:29,096 --> 00:34:33,366 A:middle
You can't start running from the
background but you must create

711
00:34:33,366 --> 00:34:35,646 A:middle
and publish your I/O
unit as I showed earlier.

712
00:34:36,116 --> 00:34:40,126 A:middle
So, it's probably going
to be necessary and useful

713
00:34:40,456 --> 00:34:41,696 A:middle
in your node application

714
00:34:42,016 --> 00:34:44,906 A:middle
to ask UIApplication
what's the state here,

715
00:34:45,056 --> 00:34:47,196 A:middle
Am I in the background or
am I in the foreground?

716
00:34:47,496 --> 00:34:48,556 A:middle
and proceed accordingly.

717
00:34:50,976 --> 00:34:53,315 A:middle
So, node applications to find

718
00:34:53,315 --> 00:34:55,235 A:middle
out when they're
becoming connected

719
00:34:55,235 --> 00:34:57,966 A:middle
and disconnected can also listen

720
00:34:58,036 --> 00:35:01,166 A:middle
for the IsInterAppConnected
property just as I described

721
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

722
00:35:01,526 --> 00:35:03,086 A:middle
for host applications earlier.

723
00:35:04,596 --> 00:35:07,106 A:middle
For a node application,
you listen to this property

724
00:35:07,526 --> 00:35:10,036 A:middle
on your AURemoteIO instance.

725
00:35:11,456 --> 00:35:15,576 A:middle
So, in your property listener,
you can notice the transitions

726
00:35:15,576 --> 00:35:17,916 A:middle
of this property value
from zero to one.

727
00:35:18,236 --> 00:35:20,406 A:middle
When you see it becoming
true, then you know

728
00:35:20,406 --> 00:35:22,936 A:middle
that you're output unit has
been initialized underneath you

729
00:35:23,166 --> 00:35:26,076 A:middle
and that you should set
your audio session active

730
00:35:26,126 --> 00:35:27,816 A:middle
if you're going to
access the microphone.

731
00:35:29,076 --> 00:35:31,886 A:middle
You should at this time start
running because that's kind

732
00:35:31,886 --> 00:35:35,516 A:middle
of your final step of consent
saying, My engine is all hooked

733
00:35:35,516 --> 00:35:39,426 A:middle
up and ready to render,
start pulling on me.

734
00:35:39,986 --> 00:35:43,206 A:middle
You can, at this time,
start running even

735
00:35:43,206 --> 00:35:44,346 A:middle
if you are in the background.

736
00:35:44,346 --> 00:35:45,676 A:middle
This is the exception
to the rule

737
00:35:45,676 --> 00:35:46,826 A:middle
about running in the background.

738
00:35:47,466 --> 00:35:48,966 A:middle
When you are connected
to the host,

739
00:35:48,966 --> 00:35:52,066 A:middle
you can start running
in the background.

740
00:35:52,066 --> 00:35:53,306 A:middle
One further note, if you want

741
00:35:53,306 --> 00:35:55,476 A:middle
to draw an icon representing
the host

742
00:35:55,476 --> 00:35:56,856 A:middle
that you've become connected to,

743
00:35:57,186 --> 00:36:00,286 A:middle
there's a new API called
AudioOutputUnitGetHostIcon.

744
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

745
00:36:03,546 --> 00:36:07,036 A:middle
Pertaining further to the
IsInterAppConnected property,

746
00:36:07,036 --> 00:36:10,016 A:middle
you also want to watch
for the transition to zero

747
00:36:10,016 --> 00:36:13,166 A:middle
or false meaning that the host
has disconnected from you.

748
00:36:13,656 --> 00:36:17,466 A:middle
What you want to do at this
point is understand your output

749
00:36:17,466 --> 00:36:20,186 A:middle
unit has been uninitialized
and stopped for--

750
00:36:20,226 --> 00:36:21,296 A:middle
out from underneath you.

751
00:36:22,636 --> 00:36:25,646 A:middle
Now, if you were
accessing the microphone,

752
00:36:25,926 --> 00:36:28,206 A:middle
you should set your session
inactive at this time.

753
00:36:28,906 --> 00:36:31,476 A:middle
However, you might,
in some situations,

754
00:36:31,476 --> 00:36:35,336 A:middle
find yourself disconnected
while in the foreground.

755
00:36:35,336 --> 00:36:37,076 A:middle
Maybe the host application
crashed

756
00:36:37,076 --> 00:36:39,496 A:middle
or the system didn't have enough
memory to keep it running.

757
00:36:39,886 --> 00:36:42,666 A:middle
So, if that happens,
you probably do want

758
00:36:42,716 --> 00:36:45,196 A:middle
to start running and keep
your audio session active

759
00:36:45,196 --> 00:36:47,606 A:middle
or make it active
if it isn't already.

760
00:36:48,926 --> 00:36:50,746 A:middle
But again, you can only start--

761
00:36:50,836 --> 00:36:53,906 A:middle
you can only make your session
active and start running

762
00:36:53,906 --> 00:36:54,906 A:middle
when you're in the foreground.

763
00:36:55,506 --> 00:36:59,576 A:middle
So, just to reemphasize that.

764
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

765
00:37:00,586 --> 00:37:04,246 A:middle
Your node application can
start if you've been connected

766
00:37:04,246 --> 00:37:05,956 A:middle
to the host or you're
in the foreground

767
00:37:06,386 --> 00:37:08,326 A:middle
but you can keep running
in to the background

768
00:37:08,946 --> 00:37:11,366 A:middle
if you're connected, of
course, or if you are

769
00:37:11,366 --> 00:37:15,156 A:middle
in some other standalone
non inter-app scenario

770
00:37:15,316 --> 00:37:17,556 A:middle
where your app wants to keep
running in to the background.

771
00:37:21,506 --> 00:37:25,916 A:middle
Let's look again now at a few
different scenarios involving

772
00:37:25,946 --> 00:37:27,476 A:middle
how nodes render audio.

773
00:37:28,276 --> 00:37:30,486 A:middle
This is your normal
standalone mode

774
00:37:30,486 --> 00:37:31,686 A:middle
when the user has launched you.

775
00:37:32,346 --> 00:37:34,486 A:middle
You've got your engine
connected to the RemoteIO,

776
00:37:34,486 --> 00:37:37,036 A:middle
connected to the
audio I/O system.

777
00:37:37,446 --> 00:37:40,066 A:middle
If you're a generator
or instrument,

778
00:37:41,406 --> 00:37:44,406 A:middle
you may have your output
completely redirected

779
00:37:44,406 --> 00:37:45,706 A:middle
to the host.

780
00:37:47,156 --> 00:37:49,626 A:middle
But if you leave your
input bus enabled

781
00:37:49,766 --> 00:37:52,986 A:middle
but you advertise yourself
as a generator or instrument,

782
00:37:53,676 --> 00:37:56,566 A:middle
then you've continued
to receive input

783
00:37:56,566 --> 00:38:00,126 A:middle
from the microphone even while
your output has been redirected

784
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

785
00:38:00,126 --> 00:38:01,346 A:middle
to the host application.

786
00:38:02,186 --> 00:38:04,386 A:middle
Now, this doesn't
add any extra latency

787
00:38:05,406 --> 00:38:07,436 A:middle
because the system
is smart enough

788
00:38:07,436 --> 00:38:11,866 A:middle
to deliver your application, the
microphone input first and then

789
00:38:11,866 --> 00:38:15,936 A:middle
in that same I/O cycle, the
host application will pull

790
00:38:15,936 --> 00:38:16,646 A:middle
your output.

791
00:38:17,256 --> 00:38:21,466 A:middle
In the final node
rendering scenario -

792
00:38:21,866 --> 00:38:24,456 A:middle
is you have in effect
both your input

793
00:38:24,456 --> 00:38:26,796 A:middle
and output streams are
connected to the host rather

794
00:38:26,796 --> 00:38:29,386 A:middle
than the audio I/O system.

795
00:38:29,956 --> 00:38:34,026 A:middle
Node applications can also use

796
00:38:34,026 --> 00:38:36,506 A:middle
that PeerURL property
I described earlier

797
00:38:36,666 --> 00:38:40,496 A:middle
to show an icon as
Alec did in his demo.

798
00:38:41,286 --> 00:38:44,606 A:middle
He showed-- the Garageband
icon in the sampler app.

799
00:38:45,216 --> 00:38:48,446 A:middle
So, you can fetch that
icon from your remote--

800
00:38:48,446 --> 00:38:51,046 A:middle
your AURemoteIO instance
in this case.

801
00:38:51,046 --> 00:38:52,666 A:middle
You can-- I'm sorry.

802
00:38:52,666 --> 00:38:55,176 A:middle
You can fetch that URL
to accomplish the switch.

803
00:38:58,936 --> 00:39:00,926 A:middle
OK. Back on the host
side of things,

804
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

805
00:39:01,706 --> 00:39:04,496 A:middle
there are a few considerations
about stopping audio rendering.

806
00:39:05,476 --> 00:39:08,886 A:middle
The normal API calls for this
doing are AudioOutputUnitStop

807
00:39:08,886 --> 00:39:12,566 A:middle
or AUGraphStop and
what you want to do

808
00:39:12,566 --> 00:39:16,236 A:middle
at this point is promptly
uninitialize your AudioUnit

809
00:39:16,236 --> 00:39:17,276 A:middle
representing the node.

810
00:39:17,976 --> 00:39:20,696 A:middle
That releases the
resources that were allocated

811
00:39:20,756 --> 00:39:24,446 A:middle
when you initialized it and it
releases you from the promise

812
00:39:24,496 --> 00:39:25,836 A:middle
to keep rendering frequently.

813
00:39:27,076 --> 00:39:29,806 A:middle
You can turn around and
reinitialize when the user wants

814
00:39:29,806 --> 00:39:32,766 A:middle
to start communicating again
or if you're completely done

815
00:39:32,766 --> 00:39:33,996 A:middle
with that node AudioUnit,

816
00:39:33,996 --> 00:39:36,346 A:middle
you can call
AudioComponentInstanceDispose

817
00:39:37,066 --> 00:39:39,746 A:middle
and that's what you would do
the if the user, for example,

818
00:39:40,006 --> 00:39:43,176 A:middle
explicitly breaks the
connection or if you discover

819
00:39:43,176 --> 00:39:45,816 A:middle
that the node application
has become invalidated.

820
00:39:46,426 --> 00:39:50,326 A:middle
So, that's the process
of audio rendering.

821
00:39:51,056 --> 00:39:54,516 A:middle
Next, I'd like to look at how
we can communicate MIDI events

822
00:39:54,516 --> 00:39:57,256 A:middle
from host applications
to node applications.

823
00:39:57,726 --> 00:40:01,746 A:middle
Now, this of course, is
for remote instrument

824
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

825
00:40:01,816 --> 00:40:03,806 A:middle
and remote music effect nodes.

826
00:40:05,606 --> 00:40:07,986 A:middle
You would want to use this
if you have MIDI events

827
00:40:07,986 --> 00:40:11,486 A:middle
that are tightly coupled to your
audio that's being rendered.

828
00:40:12,126 --> 00:40:16,186 A:middle
It lets you sample-accurately
schedule MIDI note-ons,

829
00:40:16,186 --> 00:40:19,506 A:middle
control events, pitch-bends,
et cetera.

830
00:40:19,506 --> 00:40:23,436 A:middle
But this is not recommended as
a way of communicating clock

831
00:40:23,436 --> 00:40:24,876 A:middle
and time code information.

832
00:40:25,256 --> 00:40:28,516 A:middle
That's sort of a funny
way to communicate

833
00:40:28,516 --> 00:40:31,936 A:middle
that you're using seven
bit numbers to break

834
00:40:31,936 --> 00:40:33,596 A:middle
up timing information.

835
00:40:33,596 --> 00:40:35,456 A:middle
We actually have a
better way to do that.

836
00:40:35,916 --> 00:40:39,746 A:middle
I should also mention that this
does not replace the coreMIDI

837
00:40:39,746 --> 00:40:41,526 A:middle
framework which still has a role

838
00:40:41,926 --> 00:40:46,786 A:middle
when you're dealing USB MIDI
input and output devices or,

839
00:40:46,786 --> 00:40:48,326 A:middle
for example, the
MIDI network driver.

840
00:40:49,056 --> 00:40:51,046 A:middle
You might also be
dealing with applications

841
00:40:51,046 --> 00:40:53,126 A:middle
that don't support inter-app
audio and you still want

842
00:40:53,126 --> 00:40:57,236 A:middle
to communicate with them.

843
00:40:57,416 --> 00:41:00,276 A:middle
So, let's look at how a
host application can send

844
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

845
00:41:00,276 --> 00:41:01,246 A:middle
MIDI events.

846
00:41:01,526 --> 00:41:03,796 A:middle
You might do something
like this in this--

847
00:41:03,886 --> 00:41:06,526 A:middle
like the sampler demo
app, Alex showed.

848
00:41:06,686 --> 00:41:08,536 A:middle
It had an on-screen keyboard.

849
00:41:09,026 --> 00:41:11,966 A:middle
So, whenever the user touches
the key, you send a note-on.

850
00:41:11,966 --> 00:41:13,946 A:middle
When the key is released,
you send a note-off.

851
00:41:14,476 --> 00:41:17,816 A:middle
So, the APIs for
sending MIDI events are

852
00:41:17,816 --> 00:41:22,826 A:middle
in the header file MusicDevice.h
and there is a function

853
00:41:22,826 --> 00:41:24,966 A:middle
in there called
MusicDeviceMIDIEvent.

854
00:41:25,846 --> 00:41:28,466 A:middle
Here, you pass the
node AudioUnit,

855
00:41:28,956 --> 00:41:31,106 A:middle
the three byte MIDI--
MIDI message.

856
00:41:31,696 --> 00:41:34,826 A:middle
And here, offsetSampleFrames,
the final parameter,

857
00:41:35,386 --> 00:41:37,716 A:middle
that would be used for
sample-accurate scheduling

858
00:41:37,716 --> 00:41:42,226 A:middle
but since we're doing this
in kind of a UI context,

859
00:41:42,446 --> 00:41:45,956 A:middle
we don't really know how to have
that kind of sample accuracy.

860
00:41:46,256 --> 00:41:49,066 A:middle
I'll get into how
we do in a moment.

861
00:41:49,066 --> 00:41:51,966 A:middle
So, we just passed this sample
offset frames of zero that--

862
00:41:52,036 --> 00:41:54,176 A:middle
at that note-on will
appear at the beginning

863
00:41:54,176 --> 00:41:55,526 A:middle
of the next rendered buffer.

864
00:41:55,526 --> 00:41:59,446 A:middle
Now, if we do want to
do sample-accurate,

865
00:41:59,446 --> 00:42:03,006 A:middle
scheduling then we have to
schedule our MIDI events

866
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

867
00:42:03,006 --> 00:42:05,536 A:middle
on the same thread that
were rendering the audio,

868
00:42:05,976 --> 00:42:08,036 A:middle
because in that thread context,

869
00:42:08,136 --> 00:42:11,376 A:middle
we can say where the MIDI
events need to land relative

870
00:42:11,376 --> 00:42:12,806 A:middle
to the beginning of
that audio buffer.

871
00:42:13,346 --> 00:42:14,996 A:middle
For instance if that MIDI buffer

872
00:42:14,996 --> 00:42:19,416 A:middle
or audio buffer rather is 1,024
frames, we might do some math

873
00:42:19,416 --> 00:42:23,396 A:middle
and figure out, oh, that note-on
needs to land at 412 samples

874
00:42:23,396 --> 00:42:27,576 A:middle
in to that sample buffer and
we can specify that in our call

875
00:42:27,636 --> 00:42:29,966 A:middle
to MusicDeviceMIDIEvent.

876
00:42:30,416 --> 00:42:33,306 A:middle
Now, of course, we can call
MusicDeviceMIDIEvent any number

877
00:42:33,306 --> 00:42:36,716 A:middle
of times to schedule any number
of events for one render cycle.

878
00:42:36,716 --> 00:42:40,036 A:middle
I just put these next to each
other to emphasize that you have

879
00:42:40,036 --> 00:42:43,076 A:middle
to be in the rendering
thread context to be able

880
00:42:43,116 --> 00:42:45,466 A:middle
to schedule sample-accurately.

881
00:42:46,076 --> 00:42:48,616 A:middle
Now, for you're using
AUGraph and you want

882
00:42:48,616 --> 00:42:51,356 A:middle
to schedule sample-accurately,
it's similar

883
00:42:51,356 --> 00:42:54,566 A:middle
but a little different because
you're not calling AUGgraph--

884
00:42:54,566 --> 00:42:56,586 A:middle
I'm sorry, you're not
calling AudioUnitRender,

885
00:42:56,946 --> 00:42:58,776 A:middle
the graph is doing
that on your behalf.

886
00:42:59,846 --> 00:43:03,306 A:middle
So, the way to do this,
there's an AUGraph API

887
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

888
00:43:03,306 --> 00:43:06,296 A:middle
that lets you get called
back in the render context

889
00:43:06,296 --> 00:43:08,716 A:middle
and that's
AUGraphAddRenderNotify.

890
00:43:09,886 --> 00:43:12,926 A:middle
That gives you a callback
function that the graph calls

891
00:43:13,206 --> 00:43:15,876 A:middle
at the beginning of the render
cycle before actually pulling

892
00:43:15,876 --> 00:43:16,866 A:middle
audio from the node.

893
00:43:17,316 --> 00:43:20,456 A:middle
And that turns out to be
the precisely corrects time

894
00:43:20,866 --> 00:43:23,956 A:middle
to call MusicDeviceMIDIEvent
to schedule events

895
00:43:24,376 --> 00:43:25,666 A:middle
for that render cycle.

896
00:43:26,266 --> 00:43:29,926 A:middle
So, that's the process
of sending MIDIEvents,

897
00:43:30,586 --> 00:43:33,826 A:middle
let's look at how nodes
receive MIDI Events.

898
00:43:33,946 --> 00:43:35,756 A:middle
So, we have two basic
functions for sending

899
00:43:35,756 --> 00:43:37,416 A:middle
and then there's
MusicDeviceMIDIEvent

900
00:43:37,416 --> 00:43:38,836 A:middle
and MusicDeviceSysEx.

901
00:43:39,316 --> 00:43:42,846 A:middle
And we have two corresponding
callback functions for the use

902
00:43:42,846 --> 00:43:45,136 A:middle
of the node application,
the MIDIEventProc

903
00:43:45,136 --> 00:43:46,496 A:middle
and the MIDISysExProc.

904
00:43:48,826 --> 00:43:50,636 A:middle
So, in the node application,

905
00:43:50,636 --> 00:43:52,866 A:middle
here we have an example
of MIDIEventProc.

906
00:43:52,866 --> 00:43:55,456 A:middle
Well, it doesn't
do much but here is

907
00:43:55,456 --> 00:43:58,366 A:middle
where you receive each event
that's coming from the host

908
00:43:58,526 --> 00:44:02,376 A:middle
and typically, you would just
save it up in a local structure

909
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

910
00:44:02,976 --> 00:44:06,616 A:middle
and use it the next
time you render a buffer

911
00:44:08,276 --> 00:44:11,236 A:middle
because this function will
get called at the beginning

912
00:44:11,236 --> 00:44:14,046 A:middle
of each render cycle
with new events

913
00:44:14,046 --> 00:44:15,576 A:middle
that apply to that render cycle.

914
00:44:16,186 --> 00:44:18,586 A:middle
So, having created
that callback function,

915
00:44:18,586 --> 00:44:21,276 A:middle
we can populate a
structure of callbacks.

916
00:44:21,666 --> 00:44:23,906 A:middle
You can notice I left
the SysExProc null,

917
00:44:23,906 --> 00:44:25,876 A:middle
that just means I'm
not going to get called

918
00:44:25,936 --> 00:44:27,956 A:middle
if there is any SysEx.

919
00:44:28,036 --> 00:44:32,376 A:middle
We use AudioUnitSetProperty to
install those callbacks and now,

920
00:44:32,956 --> 00:44:34,486 A:middle
on the node application side,

921
00:44:34,486 --> 00:44:37,686 A:middle
I'm going to receive each
MIDIEvent as it arrives.

922
00:44:39,256 --> 00:44:43,256 A:middle
So, that's how hosts
can sent MIDI to nodes.

923
00:44:43,756 --> 00:44:47,016 A:middle
Let's look now at how host can
communicate their transport

924
00:44:47,016 --> 00:44:48,656 A:middle
and timeline information
to nodes.

925
00:44:49,226 --> 00:44:53,476 A:middle
So, the important thing
about this model is

926
00:44:53,476 --> 00:44:55,546 A:middle
that the host is
always the master here.

927
00:44:56,256 --> 00:44:58,846 A:middle
The nodes can just find
out where the host is

928
00:44:58,846 --> 00:45:00,016 A:middle
and synchronize to that.

929
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

930
00:45:00,266 --> 00:45:04,976 A:middle
We'll look at how the host can
communicate its musical position

931
00:45:05,866 --> 00:45:08,126 A:middle
as well as the state
of its transport.

932
00:45:08,126 --> 00:45:12,366 A:middle
And all of this is highly
precise and it's called

933
00:45:12,366 --> 00:45:15,246 A:middle
and pertains to the
render context.

934
00:45:15,596 --> 00:45:21,116 A:middle
So here too, we have a structure
full of callback functions,

935
00:45:21,226 --> 00:45:22,496 A:middle
we'll look at each of these.

936
00:45:23,466 --> 00:45:26,496 A:middle
So, this is probably
the most common one

937
00:45:26,496 --> 00:45:27,816 A:middle
that a host will implement,

938
00:45:28,206 --> 00:45:30,346 A:middle
this is called the
BeatAndTempo callback.

939
00:45:31,166 --> 00:45:33,666 A:middle
Here, the host can
say for the beginning

940
00:45:33,666 --> 00:45:37,136 A:middle
of the current audio buffer,
Where am I in the track

941
00:45:37,136 --> 00:45:40,826 A:middle
and that could be
in between beats.

942
00:45:40,826 --> 00:45:44,756 A:middle
The host can also communicate
what the current tempo is.

943
00:45:44,866 --> 00:45:48,116 A:middle
And so with these two pieces
of information, even--

944
00:45:48,116 --> 00:45:50,126 A:middle
even only these two
pieces information,

945
00:45:50,126 --> 00:45:52,776 A:middle
the node can do beat
synchronized effects

946
00:45:52,776 --> 00:45:54,546 A:middle
from the host for instance.

947
00:45:55,116 --> 00:46:01,016 A:middle
There's also some more detailed
musical location information

948
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

949
00:46:01,016 --> 00:46:03,816 A:middle
supplied by the host such as
the current time signature.

950
00:46:03,896 --> 00:46:08,556 A:middle
And finally, the host
can communicate some bits

951
00:46:08,556 --> 00:46:11,126 A:middle
of transport state, most
notably whether it's playing

952
00:46:11,126 --> 00:46:11,806 A:middle
or recording.

953
00:46:11,806 --> 00:46:14,546 A:middle
There's also a facility
for the host

954
00:46:14,546 --> 00:46:17,576 A:middle
to express whether it's
cycling or looping.

955
00:46:20,636 --> 00:46:23,546 A:middle
So here too, we're installing
a set of callback from--

956
00:46:23,606 --> 00:46:26,876 A:middle
callback functions
on an audio unit.

957
00:46:26,876 --> 00:46:29,576 A:middle
The host populates the host
callback info structure,

958
00:46:29,926 --> 00:46:32,766 A:middle
installs the callback
functions that it implements

959
00:46:32,956 --> 00:46:34,886 A:middle
and calls AudioUnitSetProperty.

960
00:46:35,416 --> 00:46:39,576 A:middle
So, once the host does this, the
system will call those callbacks

961
00:46:39,576 --> 00:46:42,466 A:middle
at the beginning of each
render cycle and communicate

962
00:46:42,466 --> 00:46:45,436 A:middle
that over-- that information
over to the node process

963
00:46:46,456 --> 00:46:50,256 A:middle
where the node application
will have access to them.

964
00:46:50,586 --> 00:46:54,906 A:middle
And the way the node
application gets that access is

965
00:46:54,906 --> 00:46:56,936 A:middle
by fetching the host
callback property.

966
00:46:57,926 --> 00:47:01,076 A:middle
It will receive that structure
full of function pointers.

967
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

968
00:47:01,076 --> 00:47:02,656 A:middle
They won't actually
point to functions

969
00:47:02,656 --> 00:47:05,326 A:middle
in the host process, of course.

970
00:47:05,526 --> 00:47:08,496 A:middle
We can't make a cross process
call there, but the information,

971
00:47:08,496 --> 00:47:11,506 A:middle
as I just said, has been
communicated over to the node.

972
00:47:11,506 --> 00:47:16,006 A:middle
And it can access them there
within its own process.

973
00:47:17,016 --> 00:47:21,016 A:middle
There are some considerations
of thread safety here.

974
00:47:21,256 --> 00:47:25,856 A:middle
Most people importantly, since
this information is accurate

975
00:47:25,856 --> 00:47:28,736 A:middle
as of the beginning of the
render cycle, if you call it

976
00:47:28,736 --> 00:47:33,446 A:middle
in some other context, you
might get inconsistent results.

977
00:47:33,826 --> 00:47:38,856 A:middle
It's easiest if you fetch this
information on the render thread

978
00:47:39,236 --> 00:47:41,466 A:middle
but of course, there are
some cases where you want

979
00:47:41,466 --> 00:47:43,936 A:middle
to observe a transport
state for instance.

980
00:47:44,716 --> 00:47:49,986 A:middle
So, we give you a better
way to receive notifications

981
00:47:49,986 --> 00:47:54,146 A:middle
of transport state changes on
a non-render thread context.

982
00:47:54,576 --> 00:47:56,236 A:middle
You can install this
property listener

983
00:47:56,236 --> 00:47:59,176 A:middle
for the HostTransportState
and get a callback

984
00:47:59,526 --> 00:48:00,796 A:middle
on a non-render thread.

985
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

986
00:48:01,456 --> 00:48:05,176 A:middle
Okay, so that's the
process of transport

987
00:48:05,176 --> 00:48:06,386 A:middle
and timeline information.

988
00:48:06,386 --> 00:48:09,136 A:middle
Finally, I'd like to look
at the whole mechanism

989
00:48:09,136 --> 00:48:12,366 A:middle
by which node applications
can send remote control events

990
00:48:12,606 --> 00:48:13,856 A:middle
to host applications.

991
00:48:14,016 --> 00:48:16,966 A:middle
To accomplish that, we
have something called

992
00:48:17,206 --> 00:48:19,476 A:middle
AudioUnitRemoteControlEvents.

993
00:48:19,646 --> 00:48:21,916 A:middle
Now, there's something
called RemoteControlEvents

994
00:48:21,916 --> 00:48:23,116 A:middle
in UIKit as well.

995
00:48:23,116 --> 00:48:26,386 A:middle
Those are kind of in
a different world.

996
00:48:27,036 --> 00:48:30,286 A:middle
These are more specific to the
needs of audio applications.

997
00:48:30,776 --> 00:48:33,896 A:middle
So with these events, the
node can control the host

998
00:48:33,896 --> 00:48:35,366 A:middle
application's transport.

999
00:48:35,896 --> 00:48:38,566 A:middle
And for now, we have these
three events to find.

1000
00:48:38,716 --> 00:48:42,186 A:middle
You can toggle-- you being a
node application-can toggle the

1001
00:48:42,806 --> 00:48:49,116 A:middle
host's play or pause state, its
recording state and the node,

1002
00:48:49,246 --> 00:48:51,066 A:middle
through an event, can
send the host back

1003
00:48:51,066 --> 00:48:54,096 A:middle
to the beginning of
the song or track.

1004
00:48:54,636 --> 00:48:56,666 A:middle
We do have some sample
applications

1005
00:48:56,666 --> 00:49:00,116 A:middle
where our node applications
have some standard looking

1006
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

1007
00:49:00,116 --> 00:49:01,166 A:middle
transport controls.

1008
00:49:01,166 --> 00:49:03,486 A:middle
And we'd like to encourage you
to check those out and use them

1009
00:49:03,486 --> 00:49:06,836 A:middle
in your application so that
we can have a consistent look

1010
00:49:06,836 --> 00:49:08,146 A:middle
and feel for these controls.

1011
00:49:09,796 --> 00:49:14,436 A:middle
So, looking at how node
applications can send

1012
00:49:15,046 --> 00:49:17,356 A:middle
RemoteControlEvents,
first, we want to find

1013
00:49:17,356 --> 00:49:19,736 A:middle
out whether the host actually
is listening and is going

1014
00:49:19,736 --> 00:49:21,416 A:middle
to support them because
if it doesn't,

1015
00:49:21,416 --> 00:49:22,326 A:middle
maybe we don't even want

1016
00:49:22,326 --> 00:49:24,486 A:middle
to bother drawing the
transport controls at all.

1017
00:49:24,996 --> 00:49:26,836 A:middle
So to do that, we can
fetch this property

1018
00:49:26,836 --> 00:49:28,946 A:middle
HostReceivesRemoteControlEvents.

1019
00:49:29,826 --> 00:49:32,186 A:middle
And to actually send
the RemoteControlEvent,

1020
00:49:32,756 --> 00:49:35,916 A:middle
the node calls
AudioUnitSetProperty using the

1021
00:49:35,916 --> 00:49:37,996 A:middle
remote control to
event or I'm sorry,

1022
00:49:37,996 --> 00:49:39,476 A:middle
remote control to host event.

1023
00:49:39,716 --> 00:49:43,066 A:middle
And the value of that
property is the actual control

1024
00:49:43,066 --> 00:49:45,766 A:middle
to be sent, toggle, or
record on this example.

1025
00:49:46,806 --> 00:49:49,386 A:middle
So, there's a node sending
a RemoteControlEvent.

1026
00:49:50,286 --> 00:49:55,146 A:middle
Here is a host receiving
one or rather preparing

1027
00:49:55,146 --> 00:49:56,266 A:middle
to receive them, I should say.

1028
00:49:57,006 --> 00:50:00,046 A:middle
So, to do that, the host creates
a block called the listenerBlock

1029
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

1030
00:50:01,766 --> 00:50:06,016 A:middle
and in that block, the host
simply takes the incoming

1031
00:50:06,016 --> 00:50:09,246 A:middle
AudioUnitRemoteControlEvent
and passes it to one

1032
00:50:09,246 --> 00:50:11,886 A:middle
of its own methods called
handleRemoteControlEvent.

1033
00:50:12,626 --> 00:50:16,426 A:middle
Now, that block is in
turn a property value

1034
00:50:16,426 --> 00:50:18,576 A:middle
for the
RemoteControlEventListener

1035
00:50:18,576 --> 00:50:22,266 A:middle
property so the host only
has to set that property

1036
00:50:22,596 --> 00:50:27,836 A:middle
on the node AudioUnit and that
accomplishes the installation

1037
00:50:27,866 --> 00:50:29,856 A:middle
of the listener for
RemoteControlEvents.

1038
00:50:30,306 --> 00:50:33,656 A:middle
Next, I'd like to bring up
my colleague Harry Tormey

1039
00:50:33,656 --> 00:50:35,996 A:middle
to show you about some
of these other aspects

1040
00:50:35,996 --> 00:50:38,096 A:middle
of the inter-app
audio API in action.

1041
00:50:38,886 --> 00:50:42,136 A:middle
>> Thanks Doug.

1042
00:50:42,486 --> 00:50:46,586 A:middle
Hey everybody, my name is Harry
Tormey and I work with Doug

1043
00:50:46,586 --> 00:50:47,946 A:middle
in the Core Audio
Group at Apple.

1044
00:50:48,626 --> 00:50:51,626 A:middle
And today, I'm going to be
giving you a demonstration

1045
00:50:51,626 --> 00:50:53,526 A:middle
of some of the sample
applications we're going

1046
00:50:53,526 --> 00:50:55,136 A:middle
to be releasing on
the developer portal

1047
00:50:55,206 --> 00:50:57,616 A:middle
to illustrate how
inter-app audio works.

1048
00:50:59,066 --> 00:51:00,936 A:middle
The first demo I'm going
to be giving you is

1049
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

1050
00:51:00,936 --> 00:51:05,616 A:middle
of a host application connecting
to a sampler node application

1051
00:51:05,896 --> 00:51:07,686 A:middle
and sending it some MIDI events.

1052
00:51:08,506 --> 00:51:11,616 A:middle
So what you see in the screen
up there is a host application

1053
00:51:11,616 --> 00:51:13,276 A:middle
and I'm going to bring up a list

1054
00:51:13,276 --> 00:51:16,996 A:middle
of all the remote instrument
node applications installed

1055
00:51:16,996 --> 00:51:18,186 A:middle
on this device and
I'm going to do

1056
00:51:18,186 --> 00:51:19,776 A:middle
that by touching the
add instrument button.

1057
00:51:21,026 --> 00:51:24,016 A:middle
So, none of these applications
are currently running.

1058
00:51:24,476 --> 00:51:25,886 A:middle
They have just published
themselves

1059
00:51:25,886 --> 00:51:27,506 A:middle
with their audio
component descriptions.

1060
00:51:27,806 --> 00:51:30,516 A:middle
When I select one of these
applications from the list,

1061
00:51:30,746 --> 00:51:33,596 A:middle
it will launch into the
background and connect

1062
00:51:33,596 --> 00:51:35,076 A:middle
to the host application.

1063
00:51:35,456 --> 00:51:37,636 A:middle
So I'm going to do that, I'm
going to select the sampler.

1064
00:51:38,756 --> 00:51:41,786 A:middle
OK. So, you can see
the sampler's icon

1065
00:51:41,786 --> 00:51:44,056 A:middle
up there underneath
the instrument label.

1066
00:51:44,276 --> 00:51:46,246 A:middle
That means it's connected
to the host application.

1067
00:51:46,576 --> 00:51:49,106 A:middle
So, I'm going to bring
up a keyboard in the host

1068
00:51:49,106 --> 00:51:52,166 A:middle
by touching the show
keyboard button and I'm going

1069
00:51:52,166 --> 00:51:55,466 A:middle
to send some MIDI
events from the host

1070
00:51:55,466 --> 00:52:00,556 A:middle
to the sampler by
playing the keys.

1071
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

1072
00:52:00,686 --> 00:52:01,436 A:middle
[Music] Totally awesome.

1073
00:52:02,336 --> 00:52:06,866 A:middle
OK. So, what if I want
to change the sample bank

1074
00:52:06,866 --> 00:52:08,056 A:middle
that the sampler is using?

1075
00:52:08,386 --> 00:52:10,616 A:middle
Well, I'm going to have to do
to the sampler and do that.

1076
00:52:11,106 --> 00:52:13,216 A:middle
I'm going to do that by
touching the sampler's icon.

1077
00:52:14,016 --> 00:52:16,816 A:middle
We're now in a separate
application and I'm going

1078
00:52:16,816 --> 00:52:19,826 A:middle
to select a different
sample bank to use so how

1079
00:52:19,826 --> 00:52:21,596 A:middle
about something nice
like a harpsichord?

1080
00:52:22,026 --> 00:52:24,256 A:middle
Let me just do that there.

1081
00:52:24,256 --> 00:52:26,026 A:middle
OK. So now, we're
in harpsichord,

1082
00:52:26,026 --> 00:52:28,546 A:middle
I'm going to touch the
host icon there and go back

1083
00:52:28,546 --> 00:52:29,666 A:middle
to the host application.

1084
00:52:30,356 --> 00:52:36,736 A:middle
Touch the show keyboard
again and listen for it.

1085
00:52:36,736 --> 00:52:36,803 A:middle
[ Music ]

1086
00:52:36,803 --> 00:52:38,576 A:middle
That's a harpsichord.

1087
00:52:39,746 --> 00:52:42,186 A:middle
OK. So, the next thing that
I'm going to show you is how

1088
00:52:42,186 --> 00:52:45,716 A:middle
to use the callbacks that the
host application has published

1089
00:52:45,996 --> 00:52:49,676 A:middle
to get the time code of the
host application when it records

1090
00:52:49,676 --> 00:52:50,616 A:middle
and plays back things.

1091
00:52:50,896 --> 00:52:53,446 A:middle
So one again, I'm going to go
the sampler by touching its icon

1092
00:52:53,446 --> 00:52:57,286 A:middle
and I'm going to touch the
record button and I'm going

1093
00:52:57,286 --> 00:52:59,346 A:middle
to record some audio in the host

1094
00:52:59,346 --> 00:53:01,816 A:middle
so I'm sending a remote
message to the host.

1095
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

1096
00:53:02,876 --> 00:53:06,836 A:middle
[Music] I'm going
to stop recoding

1097
00:53:06,836 --> 00:53:07,966 A:middle
by touching record button again.

1098
00:53:08,216 --> 00:53:09,366 A:middle
Now, what I want
you to pay attention

1099
00:53:09,366 --> 00:53:12,196 A:middle
to is the blue text
over the play button.

1100
00:53:12,806 --> 00:53:15,286 A:middle
This text is going to be
updated with the callbacks

1101
00:53:15,286 --> 00:53:17,396 A:middle
that the host application
has published and were going

1102
00:53:17,396 --> 00:53:20,686 A:middle
to use this to display a
time code indicating how far

1103
00:53:20,686 --> 00:53:21,786 A:middle
into the recording we are.

1104
00:53:21,786 --> 00:53:28,126 A:middle
So, I'm going to touch the play
button and watch that text.

1105
00:53:28,126 --> 00:53:33,766 A:middle
[Music] So, if I do
that again and I go

1106
00:53:33,766 --> 00:53:36,596 A:middle
to the host application, you'll
see the time code is consistent

1107
00:53:36,596 --> 00:53:38,856 A:middle
across both applications
so let me do that.

1108
00:53:38,856 --> 00:53:41,666 A:middle
Let me press play again and go
to-- back to host application.

1109
00:53:42,116 --> 00:53:50,986 A:middle
[Music] Okay, so
for my grand finale,

1110
00:53:51,326 --> 00:53:54,756 A:middle
I'm going to add an effect
and that effect is going

1111
00:53:54,756 --> 00:53:56,426 A:middle
to be the delay effect
that you saw.

1112
00:53:56,466 --> 00:53:59,046 A:middle
So once again, I touched
the add effect button.

1113
00:53:59,316 --> 00:54:00,786 A:middle
It shows you all of the effects

1114
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

1115
00:54:00,786 --> 00:54:02,746 A:middle
that are installed
on this device.

1116
00:54:02,746 --> 00:54:04,876 A:middle
I'm going to select the delay
one, it's going to launch it

1117
00:54:04,876 --> 00:54:07,446 A:middle
and connect to the host.

1118
00:54:07,446 --> 00:54:11,486 A:middle
OK. So in the host, if I touched
the show keyboard button again

1119
00:54:11,486 --> 00:54:13,636 A:middle
and play a note, it's
going to be delayed.

1120
00:54:18,236 --> 00:54:18,836 A:middle
[Music] How about that?

1121
00:54:18,966 --> 00:54:20,776 A:middle
Much cooler than
remote controlled cars.

1122
00:54:21,256 --> 00:54:24,546 A:middle
Okay everyone, that's
me, these demos are all

1123
00:54:24,546 --> 00:54:28,296 A:middle
up on the developer portal and
I'm done with my demo so back

1124
00:54:28,296 --> 00:54:29,746 A:middle
over to you Doug and
thank you very much.

1125
00:54:30,246 --> 00:54:32,806 A:middle
[Applause]

1126
00:54:33,306 --> 00:54:33,896 A:middle
>> Thank you Harry.

1127
00:54:33,896 --> 00:54:37,556 A:middle
Hey, I found the right button.

1128
00:54:38,006 --> 00:54:44,726 A:middle
So, back to some more
mundane matters here.

1129
00:54:44,826 --> 00:54:47,516 A:middle
Dealing with audio session
interruptions, both host

1130
00:54:47,516 --> 00:54:49,346 A:middle
and node applications
need to deal

1131
00:54:49,346 --> 00:54:50,706 A:middle
with audio session
interruptions.

1132
00:54:51,296 --> 00:54:53,996 A:middle
Here, the usual rules
apply namely

1133
00:54:53,996 --> 00:54:56,696 A:middle
that your AURemoteIO gets
stopped underneath you.

1134
00:54:56,696 --> 00:54:59,296 A:middle
But furthermore, in
a host application,

1135
00:54:59,296 --> 00:55:02,096 A:middle
the system will uninitialize
any node AudioUnits

1136
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

1137
00:55:02,096 --> 00:55:02,956 A:middle
that you have open.

1138
00:55:03,366 --> 00:55:06,126 A:middle
This will reclaim the
resources I've been talking

1139
00:55:06,126 --> 00:55:10,506 A:middle
about that you acquire when you
initialize the node AudioUnit.

1140
00:55:12,076 --> 00:55:14,376 A:middle
One other bit of
housekeeping here,

1141
00:55:14,836 --> 00:55:16,866 A:middle
you can make your
application more robust

1142
00:55:16,866 --> 00:55:19,886 A:middle
if you handle a media
services reset correctly.

1143
00:55:19,946 --> 00:55:25,506 A:middle
It's a little bit hard to test
this sometimes-- oops, but--

1144
00:55:25,506 --> 00:55:28,946 A:middle
let me find my way back.

1145
00:55:29,796 --> 00:55:33,146 A:middle
But if you implement this,

1146
00:55:33,146 --> 00:55:37,476 A:middle
your application will
survive calamities.

1147
00:55:38,276 --> 00:55:40,506 A:middle
So, when this happens, you can--

1148
00:55:40,506 --> 00:55:42,256 A:middle
you will find out that all

1149
00:55:42,256 --> 00:55:44,646 A:middle
of your inter-app audio
connections have been broken,

1150
00:55:44,646 --> 00:55:46,826 A:middle
the component instances
have been invalidated.

1151
00:55:47,236 --> 00:55:50,086 A:middle
So, in a host audio--
host application,

1152
00:55:50,086 --> 00:55:53,376 A:middle
you should dispose your node
AudioUnit and your AURemoteIO.

1153
00:55:53,906 --> 00:55:55,486 A:middle
And in a node application,

1154
00:55:55,636 --> 00:55:57,836 A:middle
you should also dispose
your AURemoteIO.

1155
00:55:58,346 --> 00:56:00,016 A:middle
So in general, it's simplest

1156
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

1157
00:56:00,016 --> 00:56:03,026 A:middle
to dispose your entire audio
engine including those Apple

1158
00:56:03,026 --> 00:56:03,936 A:middle
Audio objects.

1159
00:56:04,716 --> 00:56:07,166 A:middle
And then, start over
from scratch

1160
00:56:07,166 --> 00:56:09,246 A:middle
as if your app has
just been launched

1161
00:56:09,476 --> 00:56:10,786 A:middle
and that's the simplest way

1162
00:56:10,786 --> 00:56:14,316 A:middle
to robustly handle the
media services being reset.

1163
00:56:16,856 --> 00:56:19,976 A:middle
Some questions that have come
up in showing this feature

1164
00:56:19,976 --> 00:56:21,626 A:middle
to people, in talking with them,

1165
00:56:21,986 --> 00:56:24,506 A:middle
can you have multiple
host applications?

1166
00:56:24,906 --> 00:56:26,456 A:middle
Yes, if they are all mixable.

1167
00:56:26,996 --> 00:56:29,056 A:middle
If one is unmixable, of course,

1168
00:56:29,056 --> 00:56:32,126 A:middle
it will interrupt everything
else as it takes control.

1169
00:56:32,896 --> 00:56:36,336 A:middle
Also, if you were to have
multiple host that are mixable

1170
00:56:36,336 --> 00:56:41,686 A:middle
and one node application,
only one host can connect

1171
00:56:41,776 --> 00:56:42,936 A:middle
to that node at a time.

1172
00:56:43,686 --> 00:56:45,536 A:middle
Can you have multiple
node applications?

1173
00:56:45,536 --> 00:56:48,646 A:middle
Yes, Harry just showed us that
that's more than possible.

1174
00:56:49,246 --> 00:56:53,016 A:middle
A couple of debugging
tips here you may find

1175
00:56:53,106 --> 00:56:55,076 A:middle
when creating a node application

1176
00:56:55,076 --> 00:56:57,356 A:middle
that you're having
trouble getting it to show

1177
00:56:57,356 --> 00:56:59,236 A:middle
up in host applications.

1178
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

1179
00:57:00,346 --> 00:57:03,116 A:middle
If you see that happening, you
should watch the system log.

1180
00:57:03,246 --> 00:57:05,896 A:middle
We try to leave some
clues for you there

1181
00:57:06,316 --> 00:57:07,696 A:middle
in the form of error messages.

1182
00:57:08,016 --> 00:57:10,376 A:middle
If you see a problem with
your Info.plist entry

1183
00:57:10,376 --> 00:57:13,076 A:middle
which is a little bit
easy to do unfortunately

1184
00:57:13,076 --> 00:57:15,356 A:middle
but if you do see a problem
there, we'll tell you that

1185
00:57:15,846 --> 00:57:18,786 A:middle
and I would recommend going
and comparing your Info.plist

1186
00:57:19,036 --> 00:57:21,656 A:middle
with the one-- in one of
our example applications.

1187
00:57:21,896 --> 00:57:26,556 A:middle
I should also mention here
the infamous error of 12,985

1188
00:57:26,586 --> 00:57:29,666 A:middle
which many people stub
their toes on in a lot

1189
00:57:29,666 --> 00:57:30,786 A:middle
of different contexts.

1190
00:57:31,256 --> 00:57:34,786 A:middle
I can tell you that what it
means is operation denied.

1191
00:57:35,436 --> 00:57:40,256 A:middle
And in the context of inter-app
audio, you're likely to hit it

1192
00:57:40,416 --> 00:57:41,986 A:middle
if you start playing
from the background.

1193
00:57:42,886 --> 00:57:47,136 A:middle
We do hope to in an upcoming
release give that a proper name

1194
00:57:47,136 --> 00:57:50,036 A:middle
and maybe another
value but in any case,

1195
00:57:50,036 --> 00:57:51,546 A:middle
if you do see it,
that's what it means.

1196
00:57:52,636 --> 00:57:55,866 A:middle
So we've looked at how node
applications register themselves

1197
00:57:55,866 --> 00:57:58,156 A:middle
with the system,
hosts discover them.

1198
00:57:58,846 --> 00:58:01,856 A:middle
Hosts create connections
to node applications.

1199
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

1200
00:58:03,166 --> 00:58:06,946 A:middle
Once that connection is up, host
and node apps can stream audio

1201
00:58:06,946 --> 00:58:08,206 A:middle
to and from each other.

1202
00:58:08,526 --> 00:58:11,066 A:middle
Host apps can send MIDI
to node applications.

1203
00:58:11,736 --> 00:58:13,646 A:middle
Hosts can communicate
their transport

1204
00:58:13,646 --> 00:58:14,916 A:middle
and timeline information.

1205
00:58:15,416 --> 00:58:19,176 A:middle
And finally, we have seen how
nodes can remotely control hosts

1206
00:58:20,396 --> 00:58:23,246 A:middle
so I think if you
have an existing music

1207
00:58:23,246 --> 00:58:26,086 A:middle
or audio application,
it's not that much work

1208
00:58:26,086 --> 00:58:27,466 A:middle
to convert it to a node.

1209
00:58:27,996 --> 00:58:30,206 A:middle
It's mostly adding a
little bit of code to deal

1210
00:58:30,206 --> 00:58:32,966 A:middle
with the transitions to and
from the connected state

1211
00:58:33,016 --> 00:58:34,596 A:middle
and you can look how that works

1212
00:58:34,966 --> 00:58:37,286 A:middle
in the example apps
we have posted.

1213
00:58:37,896 --> 00:58:40,156 A:middle
Creating a host application
is a bit more work

1214
00:58:40,156 --> 00:58:43,016 A:middle
but you're using existing
API for audio units

1215
00:58:43,016 --> 00:58:45,726 A:middle
and there's a lot of history
there as well as powerful--

1216
00:58:46,046 --> 00:58:47,706 A:middle
there's a lot of
power and flexibility.

1217
00:58:49,186 --> 00:58:50,276 A:middle
We also like you to--

1218
00:58:50,726 --> 00:58:53,786 A:middle
we encourage you to look
at our sample applications.

1219
00:58:54,336 --> 00:58:56,646 A:middle
They'll help you with a lot
of the little ins and outs

1220
00:58:56,996 --> 00:58:58,156 A:middle
and we're really looking forward

1221
00:58:58,156 --> 00:59:01,296 A:middle
to the great music apps
you're going to make.

1222
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

1223
00:59:01,846 --> 00:59:05,416 A:middle
On to some housekeeping matters
here, if you wish to talk

1224
00:59:05,416 --> 00:59:07,656 A:middle
to an Apple Evangelist,
there's John Geleynse.

1225
00:59:07,956 --> 00:59:11,056 A:middle
Here are some links
to some documentation

1226
00:59:11,056 --> 00:59:12,546 A:middle
and our developer forums.

1227
00:59:13,176 --> 00:59:15,346 A:middle
This is the only Core
Audio Session this year

1228
00:59:15,806 --> 00:59:18,756 A:middle
but here are some other media
sessions later this week

1229
00:59:18,756 --> 00:59:19,926 A:middle
that you might be interested in.

1230
00:59:20,766 --> 00:59:21,806 A:middle
Thank you very much.

1231
00:59:22,306 --> 00:59:29,690 A:middle
[ Silence ]

1232
