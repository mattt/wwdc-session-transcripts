1
00:00:06,210 --> 00:00:10,660
>> My name is Gokhan Avkarogullari. I'm
with the iPhone GP software group at Apple.

1
2
00:00:10,660 --> 00:00:20,180
And my colleague Richard Schreyer and I will be talking
about OpenGL ES and the iPhone implementation of OpenGL,

2
3
00:00:20,180 --> 00:00:24,180
what's new on it and all the new things we added on iOS 4.

3
4
00:00:24,180 --> 00:00:30,450
So last year at the WWDC we introduced
OpenGLOpenGL ES 2.0 and iPhone 3GS.

4
5
00:00:30,450 --> 00:00:34,590
Since then, we introduced third-generation
iPhone, third-generation iPod Touch, iPad,

5
6
00:00:34,590 --> 00:00:40,160
and we're going to very soon release iPhone 4 and iOS 4.

6
7
00:00:40,160 --> 00:00:43,940
With all this new hardware and software, there
are a lot of new features added to the system.

7
8
00:00:43,940 --> 00:00:49,650
Today we're going to give you an overview
of the new extensions that we added.

8
9
00:00:49,650 --> 00:00:56,050
We're going to talk about the retina display, the higher
definition display, the higher resolution, and what it means

9
10
00:00:56,050 --> 00:00:59,310
from OpenGL's perspective, which is a pixel-based API.

10
11
00:00:59,310 --> 00:01:05,880
We're going to talk about the impact of high
resolution displays on OpenGL based applications.

11
12
00:01:05,880 --> 00:01:12,310
And finally, we're going to talk about multitasking,
and how you can make the user experience the greatest

12
13
00:01:12,310 --> 00:01:17,550
with the changes to your OpenGL API calls.

13
14
00:01:17,550 --> 00:01:19,970
We're going to talk about new extensions first.

14
15
00:01:21,010 --> 00:01:29,440
Multisample Framebuffer is an extension being implemented
to help resolve aliasing issues in the rendering operations.

15
16
00:01:29,440 --> 00:01:31,550
So let's look at what aliasing is.

16
17
00:01:31,550 --> 00:01:35,340
Here is an example screenshot from
Touch Fighter application.

17
18
00:01:35,340 --> 00:01:41,550
If you draw your attention to the edges of the wings of the
fighter plane, you can see that there's a staircase pattern,

18
19
00:01:41,550 --> 00:01:46,740
a jaggedness to it, on all edges on the wings of the plane.

19
20
00:01:46,740 --> 00:01:52,670
If you do a close-up, you can see in the
zoomed up version, there's a staircase pattern.

20
21
00:01:52,670 --> 00:01:57,490
This is actually a significant reverse on the
live application, because the pattern changes

21
22
00:01:57,490 --> 00:02:01,760
from frame to frame, and they look like busy lines.

22
23
00:02:01,760 --> 00:02:07,390
So multisampling helps us deal with this problem
by smoothing out the edges by rounding it

23
24
00:02:07,390 --> 00:02:12,400
to a higher resolution buffer and then generating
results from that higher resolution buffer.

24
25
00:02:12,400 --> 00:02:15,200
So how can we do it?

25
26
00:02:15,200 --> 00:02:19,920
This is your regular way of creating
framebuffer objects without multisampling.

26
27
00:02:19,920 --> 00:02:25,330
You would have a framebuffer object that you would use
to display the results of your rendering operation,

27
28
00:02:25,330 --> 00:02:31,050
and it would have, normally, a color attachment, a
depth attachment, sometimes a stencil attachment.

28
29
00:02:31,050 --> 00:02:35,790
Which, if you want to do multisampling in your
application, you will need two of these framebuffer objects.

29
30
00:02:35,790 --> 00:02:40,760
One to display the results of your rendering
operation, and the other one to generate the --

30
31
00:02:40,760 --> 00:02:46,590
to do the rendering into where you
can get higher resolution images.

31
32
00:02:46,590 --> 00:02:51,400
On the right is the one that you will use for
displaying the results of your rendering operation.

32
33
00:02:51,400 --> 00:02:56,890
It only has a single attachment in this case,
a color attachment, and it's not filled in yet.

33
34
00:02:56,890 --> 00:03:02,310
On the left is the multisample framebuffer object
where the rendering operations initially take place.

34
35
00:03:02,310 --> 00:03:06,630
You can see that it has a depth attachment and
that takes place on this frame-buffer object,

35
36
00:03:06,630 --> 00:03:09,740
that's why the other one doesn't have a depth attachment.

36
37
00:03:09,740 --> 00:03:14,250
And you can see that the buffers in the
multisample framebuffer object are fill.

37
38
00:03:14,250 --> 00:03:18,520
Basically it has a result of your rendering commands.

38
39
00:03:18,520 --> 00:03:24,420
So with multisampling, once that is done, and the size
of the buffers are different on the right and left.

39
40
00:03:24,420 --> 00:03:29,720
So we're assuming here in this example, there's 4X
multisampling, and therefore the buffer is attached

40
41
00:03:29,720 --> 00:03:36,850
to the multisample framebuffer object is four times the size
of the buffer attached to the display framebuffer object.

41
42
00:03:36,850 --> 00:03:42,420
So at the end of your rendering operation, you do resolve,
which means that you take the average of the pixel --

42
43
00:03:42,420 --> 00:03:48,210
of the color values of the samples for each pixel
and generate a single color value for the pixel,

43
44
00:03:48,210 --> 00:03:54,050
and write it out to the framebuffer object that you're
going to use to display your images on the screen.

44
45
00:03:54,050 --> 00:03:57,100
So let's look at how you can do it in the code.

45
46
00:03:57,100 --> 00:04:05,090
Here's a single sampled framebuffer creation. You have
a color buffer, you generate it, bind it to your FBO,

46
47
00:04:05,090 --> 00:04:10,770
and then you get a storage packing storage for that from the
CALayer, and now finally you attach it

47
48
00:04:10,770 --> 00:04:13,420
to the color attachment of your framebuffer object.

48
49
00:04:13,420 --> 00:04:16,300
Similar operations takes place for the depth buffer.

49
50
00:04:16,300 --> 00:04:22,560
You generate it, bind it, and get storage for it through the
render buffer storage API call, and now finally attach it

50
51
00:04:22,560 --> 00:04:26,980
to the depth attachment of your framebuffer
object in the single sample case.

51
52
00:04:26,980 --> 00:04:31,320
Now, as I told you before, you have to create two
framebuffer objects for the multisampling operation.

52
53
00:04:31,320 --> 00:04:36,390
So let's look at how the framebuffer objects are
created for the multisample framebuffer object.

53
54
00:04:36,390 --> 00:04:41,480
The difference is mainly in how you
allocate storage for your buffers.

54
55
00:04:41,480 --> 00:04:47,400
In a multisample framebuffer case, the color buffer storage
comes from the render buffer storage multisample Apple API.

55
56
00:04:47,400 --> 00:04:49,760
Just like in the depth render buffer case.

56
57
00:04:49,760 --> 00:04:53,610
The difference between this API
and the previous one is you specify

57
58
00:04:53,610 --> 00:04:57,670
to OpenGL how many samples there will be in your buffer.

58
59
00:04:57,670 --> 00:05:00,410
So in this case, in this example, it could be four.

59
60
00:05:00,410 --> 00:05:02,270
Four samples to use.

60
61
00:05:02,270 --> 00:05:08,090
And the buffers that are created will take into account
that you are planning to use four samples per pixel,

61
62
00:05:08,090 --> 00:05:11,840
and they allocate buffers based on that information.

62
63
00:05:13,180 --> 00:05:17,280
Let's look at how you would normally
do a render in a single sample case.

63
64
00:05:17,280 --> 00:05:23,280
You would bind your framebuffer object, set your report,
issue your draw calls, and finally bind your color buffer

64
65
00:05:23,280 --> 00:05:27,710
that you want to display on the
screen, and then present it on the display.

65
66
00:05:27,710 --> 00:05:32,650
With the multisampling case, you have two framebuffer
objects, and you do the rendering operations

66
67
00:05:32,650 --> 00:05:35,670
to one and display operations on the other one.

67
68
00:05:35,670 --> 00:05:39,040
So to do the rendering operation on
the multisample framebuffer object,

68
69
00:05:39,040 --> 00:05:41,510
you need to bind multisample framebuffer object first,

69
70
00:05:41,510 --> 00:05:45,720
and set your viewport to your regular
draw operations, just like you did before.

70
71
00:05:45,720 --> 00:05:51,550
But you need to get the data from the multisample
framebuffer object to the single sample framebuffer object.

71
72
00:05:51,550 --> 00:05:56,750
Now, for that, we need to set the multisample
framebuffer object as a target of a resolve operation,

72
73
00:05:56,750 --> 00:06:02,990
as the retarget of the resolve operation, and
the display framebuffer object as the draw target

73
74
00:06:02,990 --> 00:06:08,660
of that frame resolve operation, and finally you should
resolve the multisample framebuffer Apple API call

74
75
00:06:08,660 --> 00:06:14,220
to get the contents of the multisample framebuffer object
to the screen size on the display frame buffer object.

75
76
00:06:14,220 --> 00:06:19,830
And just before, you would attach, you would bind
the color buffer so you can display it on the screen.

76
77
00:06:19,830 --> 00:06:26,020
So as you can see, the changes to your application
to get multisampling behavior enabled is very simple.

77
78
00:06:26,020 --> 00:06:30,290
You need to change your initialization code
to generate a multisample framebuffer object,

78
79
00:06:30,290 --> 00:06:32,730
and you need to change your rendering just a little

79
80
00:06:32,730 --> 00:06:37,750
so that you can do a final resolve
operation at the end of the rendering loop.

80
81
00:06:37,750 --> 00:06:42,700
You might be thinking, "What kind of performance
implications that would have on my application?"

81
82
00:06:42,700 --> 00:06:49,380
And the performance implications are different,
depending on what kind of GP you have on the product.

82
83
00:06:49,380 --> 00:06:57,530
All iPhone OS devices starting with
iPhone 3GS have PowerVR SGX GP on them.

83
84
00:06:57,530 --> 00:07:04,650
And PowerVR SGX GP has native hardware restoration support
for multisampling operation It understands the difference

84
85
00:07:04,650 --> 00:07:09,050
between what a sample is and what a pixel
is, and therefore it grounds your shaders

85
86
00:07:09,050 --> 00:07:13,210
to generate color value per pixel, not per sample.

86
87
00:07:13,210 --> 00:07:21,060
But it keeps depth values for each sample, and it also
does depth testing for each sample, not per pixel.

87
88
00:07:21,060 --> 00:07:25,750
So in the four-sample case, it would do four
depth tests and would generate four depth values.

88
89
00:07:25,750 --> 00:07:27,180
A single color value.

89
90
00:07:27,180 --> 00:07:33,170
And depending on if the depth test passes or fails, it
might have the same depth value and same color value

90
91
00:07:33,170 --> 00:07:39,270
for all four pixels, or it might have different
depth and different color values for all samples.

91
92
00:07:39,270 --> 00:07:44,730
It might have different depth and color values
for all samples, depending on if the pixel is

92
93
00:07:44,730 --> 00:07:47,810
on the edge of a polygon or the inside of a polygon.

93
94
00:07:47,810 --> 00:07:52,400
And after all these values are computed for each
sample and your rendering operation is done,

94
95
00:07:52,400 --> 00:07:59,100
it takes the averages of the color values and that one
would be your final color value after the resolve operation.

95
96
00:07:59,100 --> 00:08:04,140
PowerVR MBX Lite is the GP that we use
on all our products before iPhone 3GS.

96
97
00:08:04,140 --> 00:08:10,420
Unfortunately, it doesn't have native support, native
hardware restoration support, for multisampling.

97
98
00:08:10,420 --> 00:08:13,510
Therefore, we implemented multisampling
in that case, a super sampling.

98
99
00:08:13,510 --> 00:08:19,250
Which means that we used high resolution buffers, and since
the MBX Lite doesn't know the difference between the sample

99
100
00:08:19,250 --> 00:08:24,810
and pixel, it generates the same, it does processing
for each pixel in these high resolution buffer.

100
101
00:08:24,810 --> 00:08:31,740
So it generates color values for each sample, it generates
depth samples, does the depth testing, for each sample,

101
102
00:08:31,740 --> 00:08:39,510
and finally that all these values result in a single
value written into your result framebuffer object.

102
103
00:08:39,510 --> 00:08:46,830
That means that there is more performance impact on the
PowerVR MBX Lite GPs than there is on the PowerVR SGX GPs.

103
104
00:08:46,830 --> 00:08:51,740
So you might be thinking to yourself, "I could
already have done that, render the texture.

104
105
00:08:51,740 --> 00:09:00,790
I could have generated an FPL that does have larger color
attachments as textures and read them back and then sample

105
106
00:09:00,790 --> 00:09:04,310
from them and have the Multisample
behavior on my application already.

106
107
00:09:04,310 --> 00:09:05,640
So what is the difference?"

107
108
00:09:05,640 --> 00:09:10,710
The difference is that you're giving us the
information that your intention is to do multisampling.

108
109
00:09:10,710 --> 00:09:16,100
So we can actually award the readback
and then average operation at the end

109
110
00:09:16,100 --> 00:09:19,120
of your rendering textures multisampling.

110
111
00:09:19,120 --> 00:09:26,230
When we generate the values, the high resolution
values, we know that you're going to generate,

111
112
00:09:26,230 --> 00:09:29,420
you're going to use those values
to create a single sampled version,

112
113
00:09:29,420 --> 00:09:34,430
so we write out both the high resolution
and the lower resolution one.

113
114
00:09:34,430 --> 00:09:39,610
And I'm going to talk about another extension that helps
performance with the multisampling next, that's the discard.

114
115
00:09:39,610 --> 00:09:45,290
With the discard, the difference between using
multisampling and render-to-texture becomes even larger.

115
116
00:09:45,290 --> 00:09:53,460
But first, let's give a small demo, what kind of quality
impact does multisampling have on your application.

116
117
00:09:53,460 --> 00:10:01,340
So, in this case, as you can see, there is a plane
in the front, the edges of the wings are really busy,

117
118
00:10:01,340 --> 00:10:05,250
you can see the patterns changing from frame to frame.

118
119
00:10:05,250 --> 00:10:10,480
And even if you look at the ones actually, the
farthest away, it's even more visible over there.

119
120
00:10:10,480 --> 00:10:16,760
And I turn on multisampling, it becomes
significantly better than what it was before.

120
121
00:10:16,760 --> 00:10:20,450
To have no effect whatsoever you
have to have infinite resolution,

121
122
00:10:20,450 --> 00:10:23,350
but this one is significantly better
than what we had before.

122
123
00:10:23,350 --> 00:10:27,090
And if I go back and forth, it will be more visible to you,

123
124
00:10:27,090 --> 00:10:36,080
the change from non-multisampled
case to the multisampled case.

124
125
00:10:36,080 --> 00:10:41,860
As I said, multisampling is very
easy to get into your application.

125
126
00:10:41,860 --> 00:10:46,980
And you should try, experiment, see what kind
of quality difference it makes and what kind

126
127
00:10:46,980 --> 00:10:49,830
of performance impact it has on your application.

127
128
00:10:49,830 --> 00:10:55,880
We're going to talk about a second application,
that is a Discard Framebuffer extension.

128
129
00:10:55,880 --> 00:11:02,250
Discard Framebuffer, that helps the performance, fill rate performance with multisampling.

129
130
00:11:02,250 --> 00:11:08,270
And even for it helps with the fill
rate performance in the non-multisampled case.

130
131
00:11:08,270 --> 00:11:12,110
Usually once a frame is rendered, the depth
and stencil values that are associated

131
132
00:11:12,110 --> 00:11:16,160
with that frame are not needed
for rendering of the next frame.

132
133
00:11:16,160 --> 00:11:19,590
In a multisampled case, not only the
depth and stencil but the color attachment

133
134
00:11:19,590 --> 00:11:24,280
of the multisampled framebuffer object
is not needed to render the next frame.

134
135
00:11:24,280 --> 00:11:28,860
So by using this extension, you can
tell OpenGL that you don't need it,

135
136
00:11:28,860 --> 00:11:32,690
and we can discard those values
without writing out to those buffers.

136
137
00:11:32,690 --> 00:11:38,030
Which means that a significant amount of
memory bandwidth can be preserved and not used

137
138
00:11:38,030 --> 00:11:40,360
for this operation, the writing-out operation.

138
139
00:11:40,360 --> 00:11:45,480
If your application is fillrate-bound, is
bound by the amount of memory activity,

139
140
00:11:45,480 --> 00:11:50,250
then this extension will help significantly to
get better performance from your application.

140
141
00:11:51,310 --> 00:11:53,170
Here's how it conceptually works.

141
142
00:11:53,170 --> 00:11:57,020
Here's our framebuffer object,
it's a single sampled example.

142
143
00:11:57,020 --> 00:11:59,500
Our framebuffer has color and depth attachment.

143
144
00:11:59,500 --> 00:12:01,380
Right now not filled in.

144
145
00:12:01,380 --> 00:12:06,910
And the GP has done, has finished rendering
and has generated the color and depth values.

145
146
00:12:06,910 --> 00:12:11,130
Without using the Discard extension
the next step would be the GP writing

146
147
00:12:11,130 --> 00:12:15,530
out those values to the color and depth attachments.

147
148
00:12:15,530 --> 00:12:19,670
So what we don't really need for
most of the cases is the depth values

148
149
00:12:19,670 --> 00:12:22,430
for the rendering of the next time, the next frame.

149
150
00:12:22,430 --> 00:12:28,060
So if you use Discard, you can avoid writing out
the depth value, and it will never leave the GPU.

150
151
00:12:28,060 --> 00:12:33,850
And this way, the write-out section, the amount of memory
used for writing out those values will be available

151
152
00:12:33,850 --> 00:12:39,070
to the application, to read more
data into your rendering operations.

152
153
00:12:39,070 --> 00:12:41,530
Let's look at the code example.

153
154
00:12:41,530 --> 00:12:46,950
This is how we normally render in a
single sample case when you're not using Discard.

154
155
00:12:46,950 --> 00:12:51,510
You would have your framebuffer bound, and you
would send your report, issue rendering commands,

155
156
00:12:51,510 --> 00:12:54,440
and finally present your color buffer on the screen.

156
157
00:12:54,440 --> 00:13:00,510
But with Discard all you need to do is define
what attachments you're going to discard,

157
158
00:13:00,510 --> 00:13:06,170
and through the DiscardFrameBufferEXT call,
specify which of those attachments to discard.

158
159
00:13:06,170 --> 00:13:11,220
And my memory's possible OpenGL will
avoid writing out to those buffers.

159
160
00:13:11,220 --> 00:13:15,350
You can imagine that in a multisample case,
you've created buffers, they are four times larger

160
161
00:13:15,350 --> 00:13:21,350
than the original single sample case, so there's
significantly more memory activity for writing out color

161
162
00:13:21,350 --> 00:13:23,410
and depth values of the multisample buffer.

162
163
00:13:23,410 --> 00:13:27,040
So Discard makes a significant difference
in terms of fillrate performance

163
164
00:13:27,040 --> 00:13:30,140
of your application, especially in the multisampled case.

164
165
00:13:30,140 --> 00:13:34,930
But even in the non-multisampled case, you will still
be avoiding writing to the depth and stencil buffers

165
166
00:13:34,930 --> 00:13:37,760
and it will help your fillrate in your application.

166
167
00:13:37,760 --> 00:13:41,020
That was the Discard FramebufferFramebuffer.

167
168
00:13:41,020 --> 00:13:45,940
We have another extension that might help
you, that might help on the performance side

168
169
00:13:45,940 --> 00:13:48,870
of your application, and that's Vertex Array Objects.

169
170
00:13:48,870 --> 00:13:56,240
If you have been to the morning session about OpenGL, the
objects in general, GL objects, were discussed in detail,

170
171
00:13:56,240 --> 00:13:58,700
and Vertex Array Objects was one of them.

171
172
00:13:58,700 --> 00:14:04,690
What Vertex Array Objects do is they encapsulate
all the data for vertex arrays into a single object.

172
173
00:14:04,690 --> 00:14:10,670
Things like where the offset for your pointer
is, your vertex array for your position or normal

173
174
00:14:10,670 --> 00:14:16,920
or for text coordinates, what the size of each
element is within those arrays, what the stride is,

174
175
00:14:16,920 --> 00:14:19,980
which arrays are enabled, if there's an index buffer or not.

175
176
00:14:19,980 --> 00:14:22,700
They're all encapsulated into a single object.

176
177
00:14:22,700 --> 00:14:25,560
So when you switch between different Vertex Array Objects,

177
178
00:14:25,560 --> 00:14:31,170
once you bind all the different information it becomes
immediately available to the GL to take advantage of.

178
179
00:14:31,170 --> 00:14:32,930
So how does this help you?

179
180
00:14:32,930 --> 00:14:35,980
First of all, it provides a great convenience.

180
181
00:14:35,980 --> 00:14:40,340
Once you log in your assets for an
object, you can log the vertex away

181
182
00:14:40,340 --> 00:14:45,070
and encapsulate the entire state into
a single object at the load time.

182
183
00:14:45,070 --> 00:14:49,290
And if you're not dynamically updating your
object ever after, it means that you're not going

183
184
00:14:49,290 --> 00:14:54,600
to issue these commands ever again, except for
binding it and drawing from the Vertex Array Object.

184
185
00:14:54,600 --> 00:14:57,170
It also allows us to do optimizations.

185
186
00:14:57,170 --> 00:15:01,410
Since this state is validated only once, at
the load time, we don't have to rebuild it.

186
187
00:15:01,410 --> 00:15:05,940
We know, unless you update it, we know that
nothing has changed, nothing has to be revalidated.

187
188
00:15:05,940 --> 00:15:08,710
So it saves you CP time on state validation.

188
189
00:15:08,710 --> 00:15:13,530
On top of that, the Vertex Array Object gives us
very good information about the vertex layout,

189
190
00:15:13,530 --> 00:15:17,000
the layout of your data for your vertices for the arrays.

190
191
00:15:17,000 --> 00:15:21,840
So we can make use of that to find
out how to reorder them if necessary

191
192
00:15:21,840 --> 00:15:25,640
to get better performance out of the drawing operations.

192
193
00:15:25,640 --> 00:15:27,830
Let's look at the code example.

193
194
00:15:29,370 --> 00:15:33,530
This is when you are using a Vertex
Buffer Object, how you would render it.

194
195
00:15:33,530 --> 00:15:40,290
You would bind your video, and you would set
your pointers for your position, for your normal,

195
196
00:15:40,290 --> 00:15:46,490
for your texture coordinates, and you would basically
enable the non-default enables, such as the normal rate,

196
197
00:15:46,490 --> 00:15:54,890
enable the texture coordinate enables, and you will basically do-- you will do the
draw operation by calling DrawArrays or drawElements calls.

197
198
00:15:54,890 --> 00:16:03,320
And finally, you have to set the state back so it
doesn't negatively impact the next rendering operation.

198
199
00:16:03,320 --> 00:16:05,290
This has to be done at every draw call.

199
200
00:16:05,290 --> 00:16:10,300
You have to specify these things when
you're using VBOs at every draw call.

200
201
00:16:10,300 --> 00:16:15,340
With VAOs, you only need to specify
which VAO you're going to use,

201
202
00:16:15,340 --> 00:16:18,600
because all that information is
already encapsulated in the VAO object.

202
203
00:16:18,600 --> 00:16:21,290
So you know about them, you don't have to re-specify them.

203
204
00:16:21,290 --> 00:16:25,620
And we use that and then draw our arrays based on that VAO.

204
205
00:16:25,620 --> 00:16:32,880
This is possible because you've done the work to
define the state only once, at one-time setup time.

205
206
00:16:32,880 --> 00:16:38,650
You basically bind your VAO, you specify where
the pointers and strides and all these things are,

206
207
00:16:38,650 --> 00:16:46,300
and you specify which states are enabled, and they are
basically written out, captured in one place at one time,

207
208
00:16:46,300 --> 00:16:51,360
and can be re-used over and over again
for the subsequent drawing calls.

208
209
00:16:51,360 --> 00:16:52,570
That was Vertex Array Object.

209
210
00:16:52,570 --> 00:16:59,360
As you can see, it's very easy to take
advantage of, and it gives you performance boost,

210
211
00:16:59,360 --> 00:17:03,230
it will hopefully make your code less error-prone
because there will be fewer lines of code,

211
212
00:17:03,230 --> 00:17:09,250
and it will make your code more readable, because
there are also fewer lines of changes in your code.

212
213
00:17:09,250 --> 00:17:11,780
There are six more extensions I'd like to talk about.

213
214
00:17:11,780 --> 00:17:16,300
I'll give information on each one and how
you can use them in your applications.

214
215
00:17:16,300 --> 00:17:22,270
The first one I'd like to talk about is
the APPLE_texture_max_level extension.

215
216
00:17:22,270 --> 00:17:28,600
This extension allows the application to
specify the maximum (coarsest) mipmap level

216
217
00:17:28,600 --> 00:17:31,830
that may be selected for texturing operation.

217
218
00:17:31,830 --> 00:17:35,560
So it helps us to control the filtering
across atlas boundaries.

218
219
00:17:35,560 --> 00:17:42,270
As texture atlases contain textures for multiple
different objects, as mipmap levels get smaller,

219
220
00:17:42,270 --> 00:17:47,730
there is some atlas boundary filtering operation
that takes place that generates visual artifacts

220
221
00:17:47,730 --> 00:17:52,410
So this extension is implemented to solve that problem.

221
222
00:17:52,410 --> 00:17:58,350
You can enable this extension by doing a text parameter
call with the GL_TEXTURE_MAX_LEVEL_APPLE call,

222
223
00:17:58,350 --> 00:18:03,280
and you can specify up to which mipmap level
you want to use for the texturing operation.

223
224
00:18:03,280 --> 00:18:05,120
Let's visualize this.

224
225
00:18:05,120 --> 00:18:06,910
This is a texture atlas from the Quest game.

225
226
00:18:06,910 --> 00:18:15,640
You can see that there are textures in one, single texture
object, textures for walls, for stairs, for statues,

226
227
00:18:15,640 --> 00:18:19,290
and all of them are here in one single texture atlas.

227
228
00:18:19,290 --> 00:18:26,390
If you look at the mipmap levels and I have
from 256 by 256 to 1 by 1 in this picture,

228
229
00:18:26,390 --> 00:18:30,770
this is the visualization of the
mipmap levels in terms of pixels.

229
230
00:18:30,770 --> 00:18:33,330
But let's look at the mipmap levels
in terms of the coordinates.

230
231
00:18:33,330 --> 00:18:37,410
If you were to use 0 to 1 coordinates
for the entire texture atlas,

231
232
00:18:37,410 --> 00:18:40,710
the 0 to 1 coordinate on this mipmap
levels will look like this.

232
233
00:18:40,710 --> 00:18:44,680
At the very top level, it will
be exactly correct and perfect.

233
234
00:18:44,680 --> 00:18:47,110
It will have all the necessary pixels in it.

234
235
00:18:47,110 --> 00:18:49,460
This is a 256 by 256.

235
236
00:18:49,460 --> 00:18:57,600
But at the lowest level, 1 by 1 pixel, 0 to 1 coordinate
scan is only one pixel, and therefore it has only one color.

236
237
00:18:57,600 --> 00:19:05,030
So if you can imagine that you have the stairs, the walls
and the statues, close up, they will use the first one,

237
238
00:19:05,030 --> 00:19:12,060
the 256 by 256 one, but farther away, they will use this
one, or something small, something closer in mipmap level

238
239
00:19:12,060 --> 00:19:15,430
to this one, and they will all look the same color.

239
240
00:19:15,430 --> 00:19:19,760
So texture_max_level extension
avoids this problem by allowing you

240
241
00:19:19,760 --> 00:19:24,380
to specify the mipmap levels you
care about and only use those.

241
242
00:19:24,380 --> 00:19:30,300
So in this example, you can say the max level is 3,
and the texturing hardware will only use the first,

242
243
00:19:30,300 --> 00:19:35,810
second and third level of mipmap levels for
texturing from the particular texture object.

243
244
00:19:35,810 --> 00:19:39,360
Let's look at another extension that
modifies the texturing behavior,

244
245
00:19:39,360 --> 00:19:43,020
that is Apple_shader_texture level of detail extension.

245
246
00:19:43,020 --> 00:19:47,840
It gives you explicit control over setting the
level of detail for your texturing operations.

246
247
00:19:47,840 --> 00:19:53,970
So if you'd like to have an object look
sharper, then you can specify a mipmap level

247
248
00:19:53,970 --> 00:19:56,340
that is finer than the hardware would choose.

248
249
00:19:56,340 --> 00:19:59,570
You can specify a lower level mipmap with higher precision.

249
250
00:19:59,570 --> 00:20:07,800
Or if you want to have better fluid performance at the
expense of having fuzzy detection applied to your objects,

250
251
00:20:07,800 --> 00:20:14,340
you can choose a lower mipmap level, a
coarser mipmap level, through this extension.

251
252
00:20:14,340 --> 00:20:20,370
So all you need to do is in your
shader, enable the extension

252
253
00:20:20,370 --> 00:20:23,310
and control the mipmap level through the texture_lod.

253
254
00:20:23,310 --> 00:20:27,110
There are further APIs in this extension
for controlling the gradients and such.

254
255
00:20:27,110 --> 00:20:33,070
I'd like you to go through and read the extension
to find out what more you can do with this extension.

255
256
00:20:33,070 --> 00:20:42,040
And this is a shader-based extension,
it's only available on PowerVR SGX based devices that have PowerVR SGX GPUs on them. Okay.

256
257
00:20:42,040 --> 00:20:48,410
So we also added the depth texture extension to our
system, so that you can capture the depth information

257
258
00:20:48,410 --> 00:20:53,740
into the texture and use it for things like
shadow mapping or depth of field effect.

258
259
00:20:53,740 --> 00:20:57,880
In shadow mapping, you would render your
scene from the perspective of light,

259
260
00:20:57,880 --> 00:21:01,960
and then you will capture the depth information
from the perspective of light, into a texture.

260
261
00:21:01,960 --> 00:21:03,640
You will do it for every light.

261
262
00:21:03,640 --> 00:21:09,150
And then once you're rendering from the perspective
of the camera, you can basically calculate

262
263
00:21:09,150 --> 00:21:16,120
if that particular pixel sees any of the lights, and if it
sees all of the light, it will be eliminated by all of them.

263
264
00:21:16,120 --> 00:21:19,000
If it sees some of them, it will
be eliminated by some of them.

264
265
00:21:19,000 --> 00:21:22,880
If it sees none of them, it will be entirely in a shadow.

265
266
00:21:22,880 --> 00:21:26,690
So you can use that texture extension to do shadow mapping.

266
267
00:21:26,690 --> 00:21:31,970
But since you're rendering the scene multiple times, there's
a lot of fillrate this consumes, so you need to be careful

267
268
00:21:31,970 --> 00:21:37,130
about performance implications of using this technique.

268
269
00:21:37,130 --> 00:21:40,880
Another example of using that textures
is depth of field effect.

269
270
00:21:40,880 --> 00:21:45,620
I will show a small demo of that,
so I will talk to that in the demo.

270
271
00:21:45,620 --> 00:21:50,760
One thing I need to remind you that when you are using a
texture as a depth attachment to your framebuffer objects

271
272
00:21:50,760 --> 00:21:55,760
to capture the depth information of the
scene, you can only use the NEAREST method.

272
273
00:21:55,760 --> 00:22:03,380
The reason for that is that doing filtering across different
depth values, just you know, it's incorrect values.

273
274
00:22:03,380 --> 00:22:08,570
Something close to the river and something
far away in the river, when they're filtered,

274
275
00:22:08,570 --> 00:22:16,790
the depth values filter generates something in between,
though there's no object in between these two depth values.

275
276
00:22:16,790 --> 00:22:21,580
Okay. So how can you get that texture
extension into your application.

276
277
00:22:21,580 --> 00:22:27,880
You just generate your texture as usual, and when you
create your framebuffer object, you attach this texture

277
278
00:22:27,880 --> 00:22:30,360
to the depth attachment of your framebuffer object.

278
279
00:22:30,360 --> 00:22:35,850
And in subsequent rendering operations, the depth
information will be captured in this texture,

279
280
00:22:35,850 --> 00:22:42,760
and then you can use it for texturing later to do whatever
post-processing effects or whatever effects you want to do.

280
281
00:22:42,760 --> 00:22:50,950
And let's look at the demo, how we can generate a
depth of field effect with the depth_texture extension.

281
282
00:22:50,950 --> 00:22:55,420
So here again, we are using three
planes, and I'd like to point

282
283
00:22:55,420 --> 00:22:59,840
out that this is how you would normally
render it without the depth of field effect.

283
284
00:22:59,840 --> 00:23:05,930
Everything is sharp, in focus, the plane in
the back, the stars, the plane in the front.

284
285
00:23:05,930 --> 00:23:08,380
So here's the depth, visualization of depth information.

285
286
00:23:08,380 --> 00:23:13,080
This is the depth texture captured by rendering
to texture and then displaying the texture.

286
287
00:23:13,080 --> 00:23:15,480
So things that are black are closer to the screen,

287
288
00:23:15,480 --> 00:23:22,590
things that are white are farther away .And we
re-render the same thing in a blurred version,

288
289
00:23:22,590 --> 00:23:25,430
at a low resolution and the blurring introduced.

289
290
00:23:25,430 --> 00:23:32,870
So human eye, when it focus onto something, there's a
range of objects that are in that range that are sharp,

290
291
00:23:32,870 --> 00:23:39,120
but the things that are closer or things that are farther
away from that focus point on the range are blurrier.

291
292
00:23:39,120 --> 00:23:46,840
So this texture, this will capture that blurrier part,
and the original scene will capture the sharper part.

292
293
00:23:46,840 --> 00:23:48,910
You can see that they are quite different.

293
294
00:23:48,910 --> 00:23:54,040
So the operation between the two is
basically generating a mixture of these two.

294
295
00:23:54,040 --> 00:24:01,110
So here, I'm visualizing which texture I'm going to
use for my finally rendering depth of field effect.

295
296
00:24:01,110 --> 00:24:05,500
If the focus is by the near plane, and
this entire range, it will be black,

296
297
00:24:05,500 --> 00:24:11,140
so it's entirely from the darker, from the sharper image.

297
298
00:24:11,140 --> 00:24:19,750
And as I get the range closer, smaller, it will
basically start using the values from the blurred image.

298
299
00:24:19,750 --> 00:24:24,510
So that when I have the range at 0, it means
that everything is blurred, nothing is in focus,

299
300
00:24:24,510 --> 00:24:29,060
and everything will be used from the blurred image.

300
301
00:24:29,060 --> 00:24:31,280
So let's look at it visually.

301
302
00:24:31,280 --> 00:24:34,860
So if I have full range, I end up with the original scene.

302
303
00:24:34,860 --> 00:24:39,060
Original rendering without the depth of field effect.

303
304
00:24:39,060 --> 00:24:44,410
Then if I set the range to shorter, you can
see that the stars have become blurrier,

304
305
00:24:44,410 --> 00:24:47,050
and the two planes in the back are blurry.

305
306
00:24:47,050 --> 00:24:48,590
Now the entire thing is blurry.

306
307
00:24:48,590 --> 00:24:54,340
I'll move the range out and it gets
things into focus and it becomes sharper.

307
308
00:24:54,340 --> 00:25:02,780
Or I can use a short range and move my focal point away and
the things in the front will become blurrier and the stars

308
309
00:25:02,780 --> 00:25:06,470
and the third plane in the back will become sharper.

309
310
00:25:07,960 --> 00:25:11,510
As you can see, the things -- the
planes in the front are blurrier.

310
311
00:25:11,510 --> 00:25:16,580
But if I move my range to capture the entire --
near and far planes, again everything is sharper,

311
312
00:25:16,580 --> 00:25:23,440
because we have enough range everywhere
to have focal point covering everything.

312
313
00:25:23,440 --> 00:25:30,730
So that's one of the examples of how
you can use depth_texture extension.

313
314
00:25:30,730 --> 00:25:34,400
Another shadowing technique that's
very popular is stencil shadow volume.

314
315
00:25:34,400 --> 00:25:40,210
This extension, the stencil_wrap extension,
helps us to improve performance for that.

315
316
00:25:40,210 --> 00:25:43,880
With this extension, the value of
the stencil buffer will wrap around,

316
317
00:25:43,880 --> 00:25:46,000
then the array goes in and out of the shadow volume.

317
318
00:25:46,000 --> 00:25:52,160
Now, stencil shadow volumes is a large topic, and
it's a very nice way of creating real-time shadows.

318
319
00:25:52,160 --> 00:25:57,170
We're spending a significant amount of time in the
next session over here in the shaders OpenGLS shading

319
320
00:25:57,170 --> 00:26:01,580
and advanced rendering session, on
how to generate them, how to use them.

320
321
00:26:01,580 --> 00:26:07,530
There's really cool demos on visualization
of this technique and implementation of it.

321
322
00:26:08,770 --> 00:26:12,630
We added two new data types for
texturing operations, float textures,

322
323
00:26:12,630 --> 00:26:18,030
you can specify your textures to
contain 16 bit or 32 bit floats.

323
324
00:26:18,030 --> 00:26:26,230
So again, this requires hardware support that is
only available on devices that have PowerVR SGX GPUs.

324
325
00:26:26,230 --> 00:26:32,910
The float values that you can store in your textures
can be used for visualizing high dynamic range images,

325
326
00:26:32,910 --> 00:26:38,080
for you can use it for tone mapping
and display high dynamic range images

326
327
00:26:38,080 --> 00:26:42,120
in all its glory on iOS 4 based devices.

327
328
00:26:42,120 --> 00:26:47,690
It also can be used for general-purpose
GP operations, for the GPU math.

328
329
00:26:47,690 --> 00:26:54,070
You can load your data in the float textures and do
FFTs and other signal processing or other applications,

329
330
00:26:54,070 --> 00:26:57,330
whatever math you want to use with this extension.

330
331
00:26:57,330 --> 00:27:04,120
And the way you specify a texture to be
float values is basically telling to OpenGL

331
332
00:27:04,120 --> 00:27:09,220
that its format is GL half float OES or GL float OES.

332
333
00:27:09,220 --> 00:27:12,020
This is the last extension I'm going to talk about today.

333
334
00:27:12,020 --> 00:27:15,050
It's the APPLE_rgb_422 extension.

334
335
00:27:15,050 --> 00:27:19,500
And this extension enables getting video textures onto GPU.

335
336
00:27:19,500 --> 00:27:25,220
Specifically, interleaved 422 YCbCr type of video.

336
337
00:27:25,220 --> 00:27:30,890
In this extension, we do not specify the color
format of the video, so it could be based on,

337
338
00:27:30,890 --> 00:27:34,980
it could be captured from a 601,
standard definition video format,

338
339
00:27:34,980 --> 00:27:42,300
or it could be coming from HD709 based color
format, or it could be JPEG full-range video.

339
340
00:27:42,300 --> 00:27:47,610
And, so therefore, since we do not specify it,
we give you the freedom and flexibility

340
341
00:27:47,610 --> 00:27:51,400
to implement your color space convergent
to your-- in your shader.

341
342
00:27:51,400 --> 00:28:00,020
With this extension, when you specify YCbYCr, you copy the
values over from Y to the G channel, copy the values from CR

342
343
00:28:00,020 --> 00:28:06,190
to R channel, and Cb values to the B channel,
and once you do your color space conversion,

343
344
00:28:06,190 --> 00:28:11,240
you end up with the RGB values that are
coming from originally a video texture.

344
345
00:28:11,240 --> 00:28:17,790
Then again this extension relies on hardware support,
so it's only available on the devices that have

345
346
00:28:17,790 --> 00:28:18,320
PowerVR SGX.

346
347
00:28:18,320 --> 00:28:22,230
So, let's look at how you can use this extension.

347
348
00:28:22,230 --> 00:28:28,450
This is how you would do texturing operations,
something from a texture in non-RGB 422 case,

348
349
00:28:28,450 --> 00:28:33,840
you will specify your type of image and then you will
create a sample and texture and sample from that.

349
350
00:28:33,840 --> 00:28:42,650
With the 422 extension, you need to specify
the format of your texture as RGB_422_APPLE,

350
351
00:28:42,650 --> 00:28:52,050
and you need to specify the type of it as unsigned short,
either 8 and 8 rev or forward or reverse ordering of Cb and Ys,

351
352
00:28:52,050 --> 00:28:58,950
and finally your shader, you need to
convert from YCbCr values to the RGB values,

352
353
00:28:58,950 --> 00:29:05,270
and then you can do whatever effect you want to do, do black and white or just attach your texture

353
354
00:29:05,270 --> 00:29:09,600
to another object that you want
to do -- you want to use it on.

354
355
00:29:09,600 --> 00:29:11,770
So these are these other six extensions.

355
356
00:29:11,770 --> 00:29:21,130
The texture_max_level, the shaded texture level of detail,
depth texturing, stencil wrap, flow texturing and RGB 422,

356
357
00:29:21,130 --> 00:29:27,850
and with that, I'd like to invite
Richard to talk about retina display,

357
358
00:29:27,850 --> 00:29:32,250
the impact of high-resolution displays
on performance, and multitasking.

358
359
00:29:32,250 --> 00:29:33,530
>> Thank you.

359
360
00:29:37,050 --> 00:29:38,250
So thank you.

360
361
00:29:38,250 --> 00:29:46,370
So, Gokhan has just given us a description of all
the new features you'll find within OpenGL in iOS 4,

361
362
00:29:46,370 --> 00:29:52,040
so I'm going to continue the what's new topic, but really
going to focus on what's new in the rest of the platform

362
363
00:29:52,040 --> 00:29:56,230
around you that impacts you as
an OpenGL application developer.

363
364
00:29:56,230 --> 00:30:01,370
First and foremost among these is the new
Retina Display you'll find on iPhone 4.

364
365
00:30:01,370 --> 00:30:03,880
So, you've undoubtedly seen the demo.

365
366
00:30:03,880 --> 00:30:09,760
The Retina Display gives, is a 640x960 pixel display.

366
367
00:30:09,760 --> 00:30:15,300
That's in effect four times larger
than we've seen on any previous iPhone.

367
368
00:30:15,300 --> 00:30:19,800
One of the really big points I want to drive home about
the Retina display is that we're not cramming a whole bunch

368
369
00:30:19,800 --> 00:30:21,760
of content into the upper left-hand corner.

369
370
00:30:21,760 --> 00:30:28,150
All of the various views and other widgets remain
physically exactly the same size on the display.

370
371
00:30:28,150 --> 00:30:32,630
The status bar, the URL bar, are all exactly the same size.

371
372
00:30:32,630 --> 00:30:36,470
What's changed is the amount of detail
you find within any specific view.

372
373
00:30:36,470 --> 00:30:43,060
This is equivalently true to the UIKit
content as it is to OpenGL content.

373
374
00:30:43,060 --> 00:30:49,470
So how do you actually adopt -- really
make the best use of the Retina display?

374
375
00:30:49,470 --> 00:30:52,120
For OpenGL applications, it requires
a little bit of adoption.

375
376
00:30:52,120 --> 00:30:54,430
It's not something you get out of the box.

376
377
00:30:54,430 --> 00:30:56,330
The steps are pretty simple.

377
378
00:30:56,330 --> 00:30:58,910
Right off the bat, we want to render more pixels.

378
379
00:30:58,910 --> 00:31:01,040
We need to allocate a larger image.

379
380
00:31:01,040 --> 00:31:06,890
The second step is, now that you're rendering to a
different size image, we've found that a large number

380
381
00:31:06,890 --> 00:31:12,450
of applications have, for their own convenience,
hard-coded various pixel dimensions in their applications.

381
382
00:31:12,450 --> 00:31:14,860
That's something that we'll need to flush out.

382
383
00:31:14,860 --> 00:31:17,610
And finally, this is where it really gets interesting,

383
384
00:31:17,610 --> 00:31:21,620
taking advantage of the new display
to load great new artwork.

384
385
00:31:21,620 --> 00:31:24,420
So, step 1.

385
386
00:31:24,420 --> 00:31:28,160
Generating high resolution content is
actually done on a view by view basis,

386
387
00:31:28,160 --> 00:31:33,800
and this is controlled with a new
UIKit API called Content Scale Factor.

387
388
00:31:33,800 --> 00:31:41,960
So, you can figure your view with the same bounds that
you would always have, in a 320 by 480 coordinate space.

388
389
00:31:41,960 --> 00:31:48,140
What changes is that you set the content scale factor
to, say, 1 or 2, and that will in turn affect the number

389
390
00:31:48,140 --> 00:31:53,280
of pixels that are allocated to
back the image behind that view.

390
391
00:31:53,280 --> 00:31:59,320
For UIKit content, this is generally set on your behalf
to whatever is appropriate for the current device.

391
392
00:31:59,320 --> 00:32:05,760
Right out of the box, all of your buttons and your text
fields are going to be as sharp as they possibly can be.

392
393
00:32:05,760 --> 00:32:08,810
But that is not true for OpenGL views.

393
394
00:32:08,810 --> 00:32:13,020
For OpenGL views, the default value
for content scale factor remains at 1,

394
395
00:32:13,020 --> 00:32:18,610
and you have to explicitly opt in by setting that otherwise.

395
396
00:32:18,610 --> 00:32:23,500
Usually, the straightforward thing to do is to query
the scale of the screen that you're running on,

396
397
00:32:23,500 --> 00:32:31,470
and then set that to the content scale factor .On
an iPhone 3GS, this will be 1, and nothing changes.

397
398
00:32:31,470 --> 00:32:37,430
On an iPhone 4, this will be 2, and you're effectively
doubling the width and height of your render buffer.

398
399
00:32:37,430 --> 00:32:42,900
At the time you call render buffer storage, core animation
is going to snapshot both the bounds of your view

399
400
00:32:42,900 --> 00:32:50,890
and the scale factor, and it will do that to arrive at
the actual width of the image you'll be rendering into.

400
401
00:32:50,890 --> 00:32:55,410
Knowing what that width and height is
is usually pretty convenient to have,

401
402
00:32:55,410 --> 00:33:02,620
so you can derive that by doing your own bounds times scale,
or , even easier and more foolproof, is to just go ahead

402
403
00:33:02,620 --> 00:33:05,780
and ask OpenGL what the allocated width and height are.

403
404
00:33:05,780 --> 00:33:10,800
So, I just want to -- this is actually a pretty good
idea to just ask OpenGL and take these two values

404
405
00:33:10,800 --> 00:33:13,060
and stash them away somewhere on the side.

405
406
00:33:13,060 --> 00:33:17,090
They're really useful to have.

406
407
00:33:17,090 --> 00:33:19,210
That brings us to step 2, and that is, fixing any place

407
408
00:33:19,210 --> 00:33:23,260
where you have any hard-coded dimensions
that may no longer be valid.

408
409
00:33:23,260 --> 00:33:27,410
If you've -- if your application is already
universal, it runs on both iPhone and iPad,

409
410
00:33:27,410 --> 00:33:30,280
you've probably already done this, and you can move on.

410
411
00:33:30,280 --> 00:33:34,890
If you haven't done that, you may find that you have
a few of these cases, and I want to point out a couple

411
412
00:33:34,890 --> 00:33:39,270
of the most common cases you'll find in your application.

412
413
00:33:39,270 --> 00:33:46,140
First is that while core animation has chosen the size
of your color buffer, the depth buffer is something

413
414
00:33:46,140 --> 00:33:50,180
that you allocate, and the sizes of
those two resources has to match.

414
415
00:33:50,180 --> 00:33:55,170
And so, and this is a case where we'll want to
use that saved pixel width and pixel height,

415
416
00:33:55,170 --> 00:33:58,400
and pass it right on through to render buffer storage.

416
417
00:33:58,400 --> 00:34:03,800
If you don't do this, you'll find yourself with an
incomplete framebuffer and no drawing will happen.

417
418
00:34:03,800 --> 00:34:07,020
Another common case is GL Viewport.

418
419
00:34:07,020 --> 00:34:12,900
Viewport is a function which chooses which subregion of
the view you're rendering into at any given point in time,

419
420
00:34:12,900 --> 00:34:15,860
and every single application has to set it at least once.

420
421
00:34:15,860 --> 00:34:18,800
You'll find it somewhere in your source code.

421
422
00:34:18,800 --> 00:34:23,750
Most applications really don't ever use anything other
than a full screen viewport, so this is another case

422
423
00:34:23,750 --> 00:34:27,170
where you'll just want to pass pixel
width and pixel height right on through.

423
424
00:34:27,170 --> 00:34:30,260
Step three is actually where it gets really interesting.

424
425
00:34:30,260 --> 00:34:35,270
At this point, your application is a basic
correctly adopter of the Retina display.

425
426
00:34:35,270 --> 00:34:41,150
You've now got much greater detail on your polygon
edges, but there's still more room to improve things

426
427
00:34:41,150 --> 00:34:43,590
and really take advantage of this display.

427
428
00:34:43,590 --> 00:34:49,850
And so, you know, this is the right place to, for example,
load higher resolution textures and other artwork.

428
429
00:34:49,850 --> 00:34:53,660
Again, if your application is universal,
you may already have a library of assets

429
430
00:34:53,660 --> 00:34:57,820
that are perfectly relevant, that
you can use right away on this.

430
431
00:34:57,820 --> 00:35:05,300
Usually, the easiest way to do this is take your
existing bitmap textures and just add a new base level,

431
432
00:35:05,300 --> 00:35:08,400
and leave all the existing artwork in place.

432
433
00:35:08,400 --> 00:35:12,890
This can really significantly improve
the visual quality of your application.

433
434
00:35:12,890 --> 00:35:20,040
Just one word of caution here, is that you can do this
on any iPhone OS device, but it's going to be a waste

434
435
00:35:20,040 --> 00:35:24,820
on the devices that don't have large displays,
and so you actually really want to be selective

435
436
00:35:24,820 --> 00:35:28,240
about which devices you choose to
load the largest level of detail on.

436
437
00:35:28,240 --> 00:35:30,100
Otherwise you're just burning memory.

437
438
00:35:30,100 --> 00:35:35,680
One other word of warning is using UIImage to load textures.

438
439
00:35:35,680 --> 00:35:41,340
UIImage has a size property which refers to dimensions, the
dimensions of that image, but those dimensions are measured

439
440
00:35:41,340 --> 00:35:44,700
in device-independent points, not pixels.

440
441
00:35:44,700 --> 00:35:51,500
So if you have a higher resolution image that's
256 by 256 pixels, the size might only be,

441
442
00:35:51,500 --> 00:35:59,280
the size in points might only be 128 by 128, so you can't
just take those values and pipe them into GL Tech Image 2D.

442
443
00:35:59,280 --> 00:36:05,660
So, this is another one of those cases where you'll have to
do your own size by scale, or you can just drop down a level

443
444
00:36:05,660 --> 00:36:11,450
to CGImageGetWidth, GetHeight, which will give
you the image dimensions straight out in pixels.

444
445
00:36:11,450 --> 00:36:15,910
If you get caught up by this, you'll probably
see some really really strange effects.

445
446
00:36:15,910 --> 00:36:20,480
That's really about all there is to say
about making the most of the Retina display.

446
447
00:36:20,480 --> 00:36:26,280
If -- tomorrow there's going to be a session
that talks about the UIKit changes in detail,

447
448
00:36:26,280 --> 00:36:33,760
which is where you'll hear all about how UIKits
measurements in points, where OpenGL is a pixel-based API.

448
449
00:36:33,760 --> 00:36:37,650
So UIKit can do almost everything
for you with no application changes,

449
450
00:36:37,650 --> 00:36:39,980
where you do need some changes for OpenGL.

450
451
00:36:39,980 --> 00:36:43,370
But really ,when you get right down
to it, the one line of code change

451
452
00:36:43,370 --> 00:36:46,230
that really matters is setting content scale factor.

452
453
00:36:46,230 --> 00:36:54,110
There's one more really interesting topic about the Retina
display, is that you're drawing four times as many pixels.

453
454
00:36:54,110 --> 00:36:57,130
That can have some pretty significant
performance implications.

454
455
00:36:57,130 --> 00:37:03,250
This is equivalently true if your application runs on
iPad, or even if your application uses an external display.

455
456
00:37:03,250 --> 00:37:05,430
TVs can be quite large as well.

456
457
00:37:05,430 --> 00:37:08,200
So I want to talk a little bit about this too.

457
458
00:37:08,200 --> 00:37:14,390
So really, the first thing to do here is to roll up
your sleeves and start working through he standard set

458
459
00:37:14,390 --> 00:37:17,330
of fillrate optimizations and investigations.

459
460
00:37:17,330 --> 00:37:24,120
You have to think about how many pixels is your
application drawing, in this case, X and Y got a lot bigger.

460
461
00:37:24,120 --> 00:37:27,750
You also have a lot of control
over how expensive each pixel is.

461
462
00:37:27,750 --> 00:37:31,190
Properties of mipmaps can significantly
improve GPU efficiency.

462
463
00:37:31,190 --> 00:37:37,630
You are in direct control over the complexity of your
fragment shaders, operations like Alpha Test and Discard,

463
464
00:37:37,630 --> 00:37:41,330
also the costs of those add up pretty
quickly with screen size as well.

464
465
00:37:41,330 --> 00:37:46,170
I'm really going to stop here, and really not
get into the details of performance optimization,

465
466
00:37:46,170 --> 00:37:49,350
because that's a gigantic subject,
and we're going spend a whole session

466
467
00:37:49,350 --> 00:37:53,230
on that this afternoon, in OpenGL
ES Tuning and Optimization.

467
468
00:37:53,230 --> 00:37:57,520
That being said, in our experience, there are a
lot of interesting applications that do have room

468
469
00:37:57,520 --> 00:38:04,130
for performance applications, and do end up being
satisfied with the performance they get on these devices,

469
470
00:38:04,130 --> 00:38:08,840
even when running on higher resolutions,
both iPhone 4 and iPad.

470
471
00:38:08,840 --> 00:38:11,300
But that's not universally true.

471
472
00:38:11,300 --> 00:38:14,100
There are -- some developers are really aggressive.

472
473
00:38:14,100 --> 00:38:17,470
They're already using everything
these devices have to offer.

473
474
00:38:17,470 --> 00:38:23,550
And for these particularly complex applications,
you may find that you've used up all the --

474
475
00:38:23,550 --> 00:38:26,490
you've optimized everything there is to optimize.

475
476
00:38:26,490 --> 00:38:32,160
And so there's one more big tool in our
toolbox, and we're actually going to go back

476
477
00:38:32,160 --> 00:38:35,520
to how many pixels are you actually drawing.

477
478
00:38:35,520 --> 00:38:40,570
So you don't necessarily need to render
at the size that matches the display.

478
479
00:38:40,570 --> 00:38:45,750
For example, on iPhone 4, has a 640 by 960 screen.

479
480
00:38:45,750 --> 00:38:54,050
If you could instead render 720 by 480, that's still a
significant step up in quality when compared to a 3GS,

480
481
00:38:54,050 --> 00:38:56,860
and on the other hand, you're only
filling half as many pixels

481
482
00:38:56,860 --> 00:39:00,690
as you would be had you gone all
the way up to match the display.

482
483
00:39:00,690 --> 00:39:07,340
You're going to find very few other opportunities out there
to find a 2x performance jump in a single line of code.

483
484
00:39:07,340 --> 00:39:12,020
So if this becomes an option that you
want to pursue, how do you do this?

484
485
00:39:12,020 --> 00:39:17,810
You could just throw some black bars
around the sides, but not really.

485
486
00:39:17,810 --> 00:39:21,300
What you really want to do is we want to
scale that, take that lower resolution image

486
487
00:39:21,300 --> 00:39:24,200
and actually scale it to fill the whole display.

487
488
00:39:24,200 --> 00:39:27,160
Okay? How do you do that?

488
489
00:39:27,160 --> 00:39:29,250
Well, you actually don't need to do that at all.

489
490
00:39:29,250 --> 00:39:32,580
This is something that Core Animation will do for you.

490
491
00:39:32,580 --> 00:39:37,120
In fact, this is something that Core Animation
will do for you really well and really efficiently.

491
492
00:39:37,120 --> 00:39:39,460
Much more so than you could do yourself.

492
493
00:39:39,460 --> 00:39:44,780
In this case, you know, actually, move on to the next slide.

493
494
00:39:44,780 --> 00:39:50,210
That is, in the end, a really nice tradeoff
between performance and visual quality.

494
495
00:39:50,210 --> 00:39:53,320
So actually, how do you really make use of this?

495
496
00:39:53,320 --> 00:39:57,150
Well, the answer is that you literally have to do nothing.

496
497
00:39:57,150 --> 00:40:01,020
This is how your applications already
work right out of the box.

497
498
00:40:01,020 --> 00:40:05,770
The API that controls this is, again, Content Scale Factor.

498
499
00:40:05,770 --> 00:40:11,460
As I said, for compatibility reasons, today your
applications render into a view with a Content Scale Factor

499
500
00:40:11,460 --> 00:40:19,470
of 1, which means that Core Animation is already taking your
content and scaling it to fit the display, very efficiently.

500
501
00:40:19,470 --> 00:40:22,690
So right out of the gate, your
application already performs as well

501
502
00:40:22,690 --> 00:40:25,270
as it always has, and looks as good as it always has.

502
503
00:40:25,270 --> 00:40:31,070
That's a pretty decent place to start, if you
can change your application such that you can run

503
504
00:40:31,070 --> 00:40:35,130
at the native resolution of these
devices, well, that's pretty good too.

504
505
00:40:35,130 --> 00:40:40,610
But we see that there's going to be a fairly large class
of applications that do have performance headroom to, say,

505
506
00:40:40,610 --> 00:40:45,220
step it up a little bit, but you can't take
a four times jump in the number of pixels.

506
507
00:40:45,220 --> 00:40:49,180
And so there's some really interesting
middle grounds to think about here.

507
508
00:40:49,180 --> 00:40:54,090
One of these is to stick with a scale
factor of 1, but adopt anti-aliasing.

508
509
00:40:54,090 --> 00:40:59,310
We just saw Gokhan discuss our edition of the
Apple Framebuffer Multisampling Extension.

509
510
00:40:59,310 --> 00:41:02,980
This can also do a really good
job of smoothing polygon edges.

510
511
00:41:02,980 --> 00:41:07,940
In our experience, many applications that adopt
multisampling end up looking almost as good

511
512
00:41:07,940 --> 00:41:12,060
as if you were running at native resolution,
but the performance impact can be much,

512
513
00:41:12,060 --> 00:41:16,630
much less severe than increasing
the number of pixels four times.

513
514
00:41:16,630 --> 00:41:21,080
So this is a very compelling tradeoff to think about.

514
515
00:41:21,080 --> 00:41:27,110
Another interesting option is that you don't
necessarily have to pick integer content scale factor.

515
516
00:41:27,110 --> 00:41:29,330
You can pick something between 1 and 2.

516
517
00:41:29,330 --> 00:41:36,930
In the example I started from, 720 by 480, to get that
effect, you can set a content scale factor of 1.5,

517
518
00:41:36,930 --> 00:41:40,230
and that's actually all you really have to do.

518
519
00:41:40,230 --> 00:41:41,810
I want to half change the subject now.

519
520
00:41:41,810 --> 00:41:45,370
I want to talk about iPad.

520
521
00:41:45,370 --> 00:41:53,000
iPad has an even larger display, and so the motives for
wanting to do application optimization are just as true,

521
522
00:41:53,000 --> 00:41:59,220
and for some applications, the motives for wanting to
render to a smaller render buffer are just as true.

522
523
00:41:59,220 --> 00:42:06,330
There's just one unfortunate catch, and that is
that our very convenient new API is new to iOS 4.

523
524
00:42:06,330 --> 00:42:10,560
It's just not there for you to use in iPhone OS 3.2.

524
525
00:42:10,560 --> 00:42:16,210
Fortunately, there is another way, and that
is using the UIView Transform property,

525
526
00:42:16,210 --> 00:42:20,530
which has been there since iPhone SDK first shipped.

526
527
00:42:20,530 --> 00:42:23,490
So, I'm going to put up a snippet of sample code here.

527
528
00:42:23,490 --> 00:42:27,200
So think back to the beginning of the
presentation, when I said that the size

528
529
00:42:27,200 --> 00:42:30,660
of your render buffer was your bounds times your scale.

529
530
00:42:30,660 --> 00:42:33,050
On iPad, the scale is implicitly 1.

530
531
00:42:33,050 --> 00:42:35,960
There's no API, so we'll call it implicitly 1.

531
532
00:42:35,960 --> 00:42:42,880
In which case, if we want to render an 800 by 600
image on iPad, you can set the bounds to 800 by 600,

532
533
00:42:42,880 --> 00:42:48,950
and then you can set on the Transform property,
you can set a scaling transform that will take

533
534
00:42:48,950 --> 00:42:51,970
that and scale it up to fill the display.

534
535
00:42:51,970 --> 00:42:55,970
Performance-wise, these two methods are --
actually, both performance and quality-wise,

535
536
00:42:55,970 --> 00:42:58,130
these two methods are pretty much equivalent.

536
537
00:42:58,130 --> 00:43:02,670
The advantage of this method is that
you can start using it on iPhone OS 3.2,

537
538
00:43:02,670 --> 00:43:08,400
whereas ContentScaleFactor for iPhone iOS
4 and later is just more convenient.

538
539
00:43:08,400 --> 00:43:10,920
That's what I have to say about large display performance.

539
540
00:43:10,920 --> 00:43:18,950
We're going to talk a lot about this kind of performance
investigation in detail later this afternoon in the Tuning

540
541
00:43:18,950 --> 00:43:24,670
and Optimization session, and if you just run out of steam
there, then you've got some really fine-grained control

541
542
00:43:24,670 --> 00:43:32,370
over what resolution you actually render at, which can
significantly reduce the number of pixels you have to fill.

542
543
00:43:32,370 --> 00:43:36,740
That brings us to the last topic of
the day, and that's multitasking.

543
544
00:43:36,740 --> 00:43:40,060
So, I want to start this by providing an example.

544
545
00:43:40,060 --> 00:43:45,050
Say your product is a game, and the user is
playing, and they receive a text message.

545
546
00:43:45,050 --> 00:43:47,240
They leave your game to go write a response.

546
547
00:43:47,240 --> 00:43:49,760
Sure, dinner sounds great.

547
548
00:43:49,760 --> 00:43:51,140
They come right back to your game.

548
549
00:43:51,140 --> 00:43:54,060
They probably spent 10 seconds outside of it.

549
550
00:43:54,060 --> 00:43:56,290
So what do they see when they return?

550
551
00:43:57,690 --> 00:44:01,530
They see that they're going to get to wait.

551
552
00:44:01,530 --> 00:44:04,020
That's not a very good user experience.

552
553
00:44:04,020 --> 00:44:10,180
In fact, they end up waiting for longer than they
spent outside of your application in the first place,

553
554
00:44:10,180 --> 00:44:13,060
that's really not a good user experience.

554
555
00:44:13,060 --> 00:44:17,860
And so, this is what we're talking about
when we talk about fast app switching.

555
556
00:44:17,860 --> 00:44:20,740
If you went to the session, you heard
that there are a bunch of other scenarios.

556
557
00:44:20,740 --> 00:44:26,990
There's voice over IP, location
tracking, let's see, location tracking,

557
558
00:44:26,990 --> 00:44:29,440
there's finish tasks, there's audio tasks.

558
559
00:44:29,440 --> 00:44:34,180
All of those are about various modes
of doing work while in the background.

559
560
00:44:34,180 --> 00:44:36,160
Fast App Switching is different.

560
561
00:44:36,160 --> 00:44:41,060
The Fast App Switching scenario is an application
that does absolutely nothing in the background.

561
562
00:44:41,060 --> 00:44:43,860
It's completely silent, completely idle.

562
563
00:44:43,860 --> 00:44:49,670
It's there simply to lie in wait, so that it can leap back
into action the instant that the user relaunches that app.

563
564
00:44:49,670 --> 00:44:54,470
And in fact, for OpenGL, that is the only mode.

564
565
00:44:54,470 --> 00:44:59,510
GPU access is exclusively for the foreground
application to ensure responsiveness.

565
566
00:44:59,510 --> 00:45:05,750
So while you can use some of these other scenarios, you can
create a finish task to do CPU processing in the background,

566
567
00:45:05,750 --> 00:45:10,410
you do not have access to the GP one in the background.

567
568
00:45:10,410 --> 00:45:14,710
There's one really important point I want to make,
is that if you think back to that progress bar,

568
569
00:45:14,710 --> 00:45:19,710
a lot of what that application was probably
doing was loading things like OpenGL textures.

569
570
00:45:19,710 --> 00:45:21,910
That tends to be pretty time consuming.

570
571
00:45:21,910 --> 00:45:27,180
So one thing you don't have to do is
de-allocate all of your OpenGL resources.

571
572
00:45:27,180 --> 00:45:31,090
You can leave all of your textures and all of
your buffer objects and everything else in place.

572
573
00:45:31,090 --> 00:45:36,070
You just have to go hands off and not touch
them for awhile, but they can stay there.

573
574
00:45:36,070 --> 00:45:40,630
This means that when a user does bring your application
back to the foreground, all o those really expensive

574
575
00:45:40,630 --> 00:45:45,120
to load resources are already there, ready to go right away.

575
576
00:45:45,120 --> 00:45:48,340
Keeping all that stuff in the background
does have some implications on memory usage,

576
577
00:45:48,340 --> 00:45:52,070
and that leads to a really interesting
tradeoff that you should think about carefully.

577
578
00:45:52,070 --> 00:45:58,680
It is generally a really good idea to reduce your
application's memory usage when you run in the background.

578
579
00:45:58,680 --> 00:46:02,580
For example, the system memory
as a whole is a shared resource.

579
580
00:46:02,580 --> 00:46:06,750
If the application in the foreground needs more
memory, the system will go find applications

580
581
00:46:06,750 --> 00:46:09,610
in the background to terminate to make room for it.

581
582
00:46:09,610 --> 00:46:13,100
That list is ordered by who's using the most memory.

582
583
00:46:13,100 --> 00:46:14,970
Guess who's going to be on top?

583
584
00:46:14,970 --> 00:46:17,780
So, you have a really compelling,
even perfectly selfish reason,

584
585
00:46:17,780 --> 00:46:20,560
to want to reduce your memory use as much as possible.

585
586
00:46:20,560 --> 00:46:26,570
Because that means that your process is probably going to be
more likely to be there to come back to in the first place.

586
587
00:46:26,570 --> 00:46:32,240
On the other hand, if you're making the resume
operation slow by spending a whole bunch

587
588
00:46:32,240 --> 00:46:35,120
of time loading resources, we've
kind of defeated the purpose.

588
589
00:46:35,120 --> 00:46:37,520
There's really a balancing act to be made here.

589
590
00:46:37,520 --> 00:46:44,280
So the way you should think about it is look at
your application on a resource-by-resource basis,

590
591
00:46:44,280 --> 00:46:48,640
and think about what you really need to
pick up right where the user left off.

591
592
00:46:48,640 --> 00:46:52,250
And also think about how expensive is this to re-create.

592
593
00:46:52,250 --> 00:46:56,440
If you've got your standard textures, those
are probably pretty expensive to re-create.

593
594
00:46:56,440 --> 00:46:58,130
You want to keep those around.

594
595
00:46:58,130 --> 00:47:01,270
On the other hand, there are some
that are really cheap to re-create.

595
596
00:47:01,270 --> 00:47:03,390
Think about your color and depth buffers.

596
597
00:47:03,390 --> 00:47:08,280
There's no drawing in the background, so they're
really just sitting there, not doing anything.

597
598
00:47:08,280 --> 00:47:12,360
Re-creating them is not like a texture,
where you have to go to the file system

598
599
00:47:12,360 --> 00:47:15,610
and load data and decompress it and so on.

599
600
00:47:15,610 --> 00:47:19,230
Reallocating your color and depth buffers
is just conjuring up empty memory.

600
601
00:47:19,230 --> 00:47:21,080
It's really fast.

601
602
00:47:21,080 --> 00:47:24,910
Also think about cases where you actually have idle
resources that aren't actually needed for the current scene.

602
603
00:47:24,910 --> 00:47:30,540
You know, if you've got a bunch of GL textures that
are around because you needed them in the past,

603
604
00:47:30,540 --> 00:47:34,950
and you're keeping them around pre-emptively because you
might need them in the future, this is a really good time

604
605
00:47:34,950 --> 00:47:38,200
to clear out all the idle textures in that
cache and leave all the active ones in place.

605
606
00:47:38,200 --> 00:47:42,310
A little bit more about the mechanics of it.

606
607
00:47:42,310 --> 00:47:44,920
How do you actually enter the background and come back?

607
608
00:47:44,920 --> 00:47:50,450
Your application is going to receive
a data enter background notification.

608
609
00:47:50,450 --> 00:47:53,790
When this happens, we have to stop our usage of the GPU.

609
610
00:47:53,790 --> 00:47:58,460
Specifically, your access to the GPU ends
as soon as this notification returns.

610
611
00:47:58,460 --> 00:48:03,030
So you have to be done before you return from this function.

611
612
00:48:03,030 --> 00:48:06,330
The second is that you want to save application state.

612
613
00:48:06,330 --> 00:48:11,330
In this case, if you're writing a painting application
with OpenGL, that might involve using read pixels

613
614
00:48:11,330 --> 00:48:15,660
to pull back what the user has painted
and save it to the file system.

614
615
00:48:15,660 --> 00:48:19,370
And this is really important, because
your application may be terminated

615
616
00:48:19,370 --> 00:48:23,000
in the background to make memory, to free up memory.

616
617
00:48:23,000 --> 00:48:27,750
And then finally, here's our example of
releasing memory for this application.

617
618
00:48:27,750 --> 00:48:30,000
You know, we say we're going to go release our framebuffers,

618
619
00:48:30,000 --> 00:48:34,140
because we can re-create them really
fast, without slowing the user down.

619
620
00:48:34,140 --> 00:48:38,850
On the other side, when your application
wants to enter the foreground,

620
621
00:48:38,850 --> 00:48:42,690
you'll receive an applicationWillEnterForeground notification.

621
622
00:48:42,690 --> 00:48:47,860
We'll spend a tiny fraction of a second allocating
our framebuffer, and then we're ready to go.

622
623
00:48:47,860 --> 00:48:49,780
This is exactly where we want to be.

623
624
00:48:49,780 --> 00:48:50,800
So that's great.

624
625
00:48:50,800 --> 00:48:54,880
Except that there's one other case here that
you really have to think about very carefully.

625
626
00:48:54,880 --> 00:49:00,490
And this is-- and this is, you might receive
applicationDidFinishLaunching instead.

626
627
00:49:00,490 --> 00:49:03,320
If your process was terminated while in the background,

627
628
00:49:03,320 --> 00:49:07,370
then you have lost all of your GL
resources, as well as everything else.

628
629
00:49:07,370 --> 00:49:10,500
And you now have to reload them,
and that's going to take time.

629
630
00:49:10,500 --> 00:49:14,420
The more interesting part here
is restoring that saved state,

630
631
00:49:14,420 --> 00:49:18,460
restoring that state that we saved
when entering the background.

631
632
00:49:18,460 --> 00:49:22,550
Because when the user enters -- the user doesn't know

632
633
00:49:22,550 --> 00:49:26,910
when an application is terminated
in the background to free up memory.

633
634
00:49:26,910 --> 00:49:30,750
It's a completely invisible implementation detail to them.

634
635
00:49:30,750 --> 00:49:37,980
So to you, it's effectively unpredictable which one of these
paths your application will take, to reenter the foreground.

635
636
00:49:37,980 --> 00:49:42,780
And so, if in one of these cases you put them
right back in their game, and everything's golden,

636
637
00:49:42,780 --> 00:49:50,390
whereas in the other case, you make them page through a
parade of logos and select a menu and click Load Game,

637
638
00:49:50,390 --> 00:49:52,670
the user is effectively going to see random behavior here.

638
639
00:49:52,670 --> 00:49:58,700
They won't know which case to expect when they
press on your icon, and that's fairly disconcerting.

639
640
00:49:58,700 --> 00:50:04,270
And so it's really critical here that
regardless of which path your application takes,

640
641
00:50:04,270 --> 00:50:09,830
you put them back in exactly the same
place, say, reloading their game.

641
642
00:50:09,830 --> 00:50:13,380
Ideally, the user just can't tell the
difference between these two cases.

642
643
00:50:13,380 --> 00:50:16,110
Practically speaking, there will
be a performance difference.

643
644
00:50:16,110 --> 00:50:20,000
The whole point of Fast App Switching is to keep
those resources around, and if they're not around,

644
645
00:50:20,000 --> 00:50:22,220
you're going to have to spend time to load them.

645
646
00:50:22,220 --> 00:50:26,760
But for the best-behaved applications, the
application's performance will be the only difference

646
647
00:50:26,760 --> 00:50:28,680
in behavior the user can see.

647
648
00:50:28,680 --> 00:50:30,580
That's Fast App Switching.

648
649
00:50:30,580 --> 00:50:35,290
You want to free up as much memory as possible, but not
to the extent that you're going to slow down Resume.

649
650
00:50:35,290 --> 00:50:39,890
And then you also need to think about doing a really
good job of saving and restoring your GL state.

650
651
00:50:39,890 --> 00:50:48,070
Which could include actually the contents of your
OpenGL resources, if you're modifying those on the fly.

651
652
00:50:48,070 --> 00:50:51,140
So that actually brings us to the
end of today's presentation.

652
653
00:50:51,140 --> 00:50:56,380
To just give you a quick recap of where we've
been, we've talked about some new OpenGL extensions

653
654
00:50:56,380 --> 00:51:01,170
to improve the visual quality of your application:
multisample, float texture and depth texture.

654
655
00:51:01,170 --> 00:51:06,600
We have new features to improve the performance of your
application: Vertex Array Objects and Discard Framebuffer.

655
656
00:51:06,600 --> 00:51:09,250
Discard and Multisample go together particularly well.

656
657
00:51:09,250 --> 00:51:12,960
We talked about how to adopt the Retina display.

657
658
00:51:12,960 --> 00:51:15,390
Ideally, that's one line of code.

658
659
00:51:15,390 --> 00:51:18,690
We talked about resolution selection for large displays.

659
660
00:51:18,690 --> 00:51:24,290
This is really your big hammer to solve
fillrate issues if you have no other option.

660
661
00:51:24,290 --> 00:51:29,710
And finally we talked about multitasking, where the key
is always think about the phrase Fast App Switching.

661
662
00:51:29,710 --> 00:51:31,760
We have a number of related sessions.

662
663
00:51:31,760 --> 00:51:37,160
Coming up later this afternoon is OpenGL ES Tuning and
Optimization, where we'll go through the process of how

663
664
00:51:37,160 --> 00:51:42,050
to actually look at fillrate performance in your
application, as well as introduce a new developer tool

664
665
00:51:42,050 --> 00:51:46,490
that can really help you understand
what your application is really doing.

665
666
00:51:46,490 --> 00:51:49,800
Shading and Advanced Rendering is more of
an applied session, where we're going to go

666
667
00:51:49,800 --> 00:51:52,370
through some really classic graphics
algorithms and then talk

667
668
00:51:52,370 --> 00:51:57,740
about how we practically applied
them to the graphics in Quest.

668
669
00:51:57,740 --> 00:52:05,230
OpenGL Essential Design Practices happened earlier today,
and this is a really great talk which goes into the subject

669
670
00:52:05,230 --> 00:52:12,450
of general OpenGL design practices, where you want to use
this kind of object, learning about modern API changes.

670
671
00:52:12,450 --> 00:52:18,100
This kind of stuff is equally applicable
to both desktop and embedded OpenGL.

671
672
00:52:18,100 --> 00:52:23,730
And then finally, there's a couple sessions that
talk about multitasking and the Retina display,

672
673
00:52:23,730 --> 00:52:27,130
as they apply to the whole platform, not just OpenGL.

673
674
00:52:27,130 --> 00:52:31,610
You can contact Alan Schaffer directly, he's
our Game and Graphics Technologies Evangelist,

674
675
00:52:31,610 --> 00:52:38,160
and we also have a great collection of written
documentation in the OpenGL ES programming guide for iPhone.

675
676
00:52:38,160 --> 00:52:42,630
So, with that, I hope this talk was useful to
you today, and I hope to see you at the labs.

676
677
00:52:42,630 --> 00:52:42,880
Thank you.

677
