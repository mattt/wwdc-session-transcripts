X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

1
00:00:12,056 --> 00:00:12,606 A:middle
>> Good morning, everyone.

2
00:00:12,976 --> 00:00:15,786 A:middle
My name is Kapil Krishnamurthy
and I work in Core Audio.

3
00:00:16,746 --> 00:00:17,956 A:middle
I'm here today to talk to you

4
00:00:17,956 --> 00:00:20,506 A:middle
about a new API called
AVAudioEngine

5
00:00:20,746 --> 00:00:25,106 A:middle
that we are introducing for
Mac OS X Yosemite and iOS 8.

6
00:00:26,986 --> 00:00:29,706 A:middle
As part of today's talk we'll
first look at an overview

7
00:00:29,706 --> 00:00:32,866 A:middle
of Core Audio and then we'll
dive into AVAudioEngine,

8
00:00:33,536 --> 00:00:35,376 A:middle
look at some of the
goals behind the project,

9
00:00:35,796 --> 00:00:36,986 A:middle
features of the new API,

10
00:00:38,006 --> 00:00:39,716 A:middle
the different building
blocks you'll be using

11
00:00:40,616 --> 00:00:43,816 A:middle
and finally we'll do a
section on gaming and 3D audio.

12
00:00:45,276 --> 00:00:46,036 A:middle
So let's get started.

13
00:00:46,036 --> 00:00:49,846 A:middle
For those of you who aren't
familiar with Core Audio,

14
00:00:50,726 --> 00:00:53,896 A:middle
Core Audio provides a
number of C APIs as part

15
00:00:53,896 --> 00:00:57,176 A:middle
of a different frameworks
on both iOS and Mac OS X.

16
00:00:57,176 --> 00:00:59,766 A:middle
And you can use these
different APIs

17
00:00:59,916 --> 00:01:02,286 A:middle
to implement audio features
in your applications.

18

19
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

20
00:00:59,916 --> 00:01:02,286 A:middle
to implement audio features
in your applications.

21
00:01:03,746 --> 00:01:07,026 A:middle
So using these APIs you will be
able to play in the card sounds

22
00:01:07,126 --> 00:01:10,966 A:middle
with low latency, convert
between different file

23
00:01:10,966 --> 00:01:15,716 A:middle
and data formats, read and write
audio files, work with many data

24
00:01:15,856 --> 00:01:18,486 A:middle
and also play sounds
that get spatialized.

25
00:01:20,996 --> 00:01:25,076 A:middle
Several years ago we added
some simple objective C classes

26
00:01:25,146 --> 00:01:28,226 A:middle
to AVFoundation and
they're called AVAudioPlayer

27
00:01:28,226 --> 00:01:29,366 A:middle
and AVAudioRecorder.

28
00:01:29,996 --> 00:01:33,176 A:middle
And using these classes you
can play sounds from files

29
00:01:33,306 --> 00:01:35,526 A:middle
or record directly to a file.

30
00:01:35,986 --> 00:01:37,806 A:middle
Now while these classes
worked really well

31
00:01:37,806 --> 00:01:39,046 A:middle
for simple use cases,

32
00:01:39,386 --> 00:01:41,936 A:middle
a more advanced user might
find themselves a bit limited.

33
00:01:42,586 --> 00:01:45,276 A:middle
So this year we're adding
a whole new set of API

34
00:01:45,676 --> 00:01:48,746 A:middle
to AVFoundation called
AVAudioEngine

35
00:01:49,376 --> 00:01:51,716 A:middle
and my colleague Doug
also spoke about a number

36
00:01:51,716 --> 00:01:54,826 A:middle
of AV audio utility
classes in session 501.

37
00:01:55,806 --> 00:01:58,706 A:middle
So using this new
API you will be able

38
00:01:58,706 --> 00:02:02,206 A:middle
to write powerful features with
just a fraction of the amount

39

40
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

41
00:01:58,706 --> 00:02:02,206 A:middle
to write powerful features with
just a fraction of the amount

42
00:02:02,206 --> 00:02:04,186 A:middle
of code that you may have
had to previously write.

43
00:02:05,296 --> 00:02:06,106 A:middle
So let's get started.

44
00:02:06,926 --> 00:02:08,556 A:middle
What were the goals
behind this project?

45
00:02:09,436 --> 00:02:12,156 A:middle
One of the biggest goals
was to provide a powerful

46
00:02:12,156 --> 00:02:13,476 A:middle
and feature-rich API set.

47
00:02:13,476 --> 00:02:16,646 A:middle
And we're able to do that
because we're building on top

48
00:02:16,646 --> 00:02:18,936 A:middle
of our existing Core Audio APIs.

49
00:02:19,866 --> 00:02:22,246 A:middle
Using this API we want
to developers to be able

50
00:02:22,246 --> 00:02:24,726 A:middle
to achieve simple as
well as complex tasks.

51
00:02:25,316 --> 00:02:27,926 A:middle
And a simple task could be
something like playing a sound

52
00:02:27,926 --> 00:02:29,356 A:middle
and running it through
an effect.

53
00:02:29,776 --> 00:02:31,886 A:middle
A complex task could
be something as big

54
00:02:31,886 --> 00:02:34,026 A:middle
as writing an entire
audio engine for a game.

55
00:02:35,376 --> 00:02:37,666 A:middle
We also wanted to
simplify real-time audio.

56
00:02:38,546 --> 00:02:40,036 A:middle
For those of you
who are not familiar

57
00:02:40,036 --> 00:02:42,906 A:middle
with real-time audio it
can be quite challenging.

58
00:02:43,856 --> 00:02:46,446 A:middle
You have a number of audio
callbacks every second

59
00:02:47,056 --> 00:02:48,366 A:middle
and for each callback you have

60
00:02:48,366 --> 00:02:50,136 A:middle
to provide data in
a timely fashion.

61
00:02:50,916 --> 00:02:53,386 A:middle
You can't do things like
take locks on the I/O thread

62
00:02:53,676 --> 00:02:55,806 A:middle
or call functions that
could block indefinitely.

63
00:02:56,636 --> 00:02:58,896 A:middle
So we make all of this
easier for you to work

64
00:02:58,896 --> 00:03:02,486 A:middle
with by giving you a
real-time audio system but one

65

66
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

67
00:02:58,896 --> 00:03:02,486 A:middle
with by giving you a
real-time audio system but one

68
00:03:02,486 --> 00:03:05,446 A:middle
that you interact with in
a non real-time context.

69
00:03:06,616 --> 00:03:10,686 A:middle
Features of the new API: this
is a full-featured Objective-C

70
00:03:10,736 --> 00:03:11,976 A:middle
API set.

71
00:03:12,446 --> 00:03:15,416 A:middle
You get a real-time audio
system to work with meaning

72
00:03:15,416 --> 00:03:17,376 A:middle
that any changes
that you make on any

73
00:03:17,376 --> 00:03:19,516 A:middle
of the blocks take
effect immediately.

74
00:03:20,336 --> 00:03:24,006 A:middle
Using this API you will be able
to read and write audio files,

75
00:03:24,646 --> 00:03:28,846 A:middle
play and record audio, connect
different audio processing

76
00:03:28,846 --> 00:03:31,856 A:middle
blocks together and then
while the engine is running

77
00:03:31,936 --> 00:03:35,096 A:middle
and audio is flowing through
this system you can tap the

78
00:03:35,096 --> 00:03:37,276 A:middle
output of each of these
processing blocks.

79
00:03:37,966 --> 00:03:40,426 A:middle
You'll also be able to
implement 3D audio for games.

80
00:03:41,166 --> 00:03:44,436 A:middle
Now before we actually jump

81
00:03:44,556 --> 00:03:46,676 A:middle
into the engine's building
blocks I thought I would give

82
00:03:46,676 --> 00:03:49,656 A:middle
you two sample use cases
to give you a little flavor

83
00:03:49,656 --> 00:03:51,636 A:middle
of what you'll be able
to do using this API.

84
00:03:52,606 --> 00:03:55,986 A:middle
So the first sample use case
is a karaoke application.

85
00:03:56,696 --> 00:03:58,316 A:middle
You have a backing
track that's playing

86
00:03:58,316 --> 00:04:00,966 A:middle
and the user is singing
along with it in real-time.

87

88
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

89
00:03:58,316 --> 00:04:00,966 A:middle
and the user is singing
along with it in real-time.

90
00:04:01,806 --> 00:04:04,916 A:middle
The output of the microphone
is passed through a delay,

91
00:04:04,976 --> 00:04:07,636 A:middle
which is just a musical
effect and both

92
00:04:07,636 --> 00:04:10,856 A:middle
of these audio chains are mixed
and sent to the output hardware.

93
00:04:11,116 --> 00:04:12,916 A:middle
This could be a speaker
or headphones.

94
00:04:14,206 --> 00:04:18,026 A:middle
Let's also say that you tap
the output of the microphone

95
00:04:18,646 --> 00:04:23,066 A:middle
and analyze that raw data
to see the user's on pitch,

96
00:04:23,066 --> 00:04:24,016 A:middle
he's doing a great job.

97
00:04:24,666 --> 00:04:26,916 A:middle
And if he is, play
some sound effects,

98
00:04:27,156 --> 00:04:29,626 A:middle
so this stream also
gets mixed in and played

99
00:04:29,626 --> 00:04:30,746 A:middle
out to the output hardware.

100
00:04:32,276 --> 00:04:33,556 A:middle
Here's another use case.

101
00:04:33,976 --> 00:04:36,766 A:middle
You have a streaming
application and you receive data

102
00:04:36,766 --> 00:04:37,866 A:middle
from the remote location.

103
00:04:38,556 --> 00:04:41,606 A:middle
You can now stuff this
data into different buffers

104
00:04:41,606 --> 00:04:43,066 A:middle
and schedule them on a player.

105
00:04:44,286 --> 00:04:45,836 A:middle
You can run the output
of the player

106
00:04:46,196 --> 00:04:49,406 A:middle
through an EQ whose UI
you present to the user

107
00:04:49,546 --> 00:04:52,326 A:middle
so that they can tweak the
EQ based on that preference.

108
00:04:53,116 --> 00:04:55,966 A:middle
The output of the EQ then
goes to the output hardware.

109
00:04:56,876 --> 00:04:59,276 A:middle
So these are just
two sample use cases.

110
00:04:59,276 --> 00:05:01,736 A:middle
You'll be able to do a
whole lot more once we talk

111

112
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

113
00:04:59,276 --> 00:05:01,736 A:middle
You'll be able to do a
whole lot more once we talk

114
00:05:01,736 --> 00:05:02,866 A:middle
about AVAudioEngine.

115
00:05:03,276 --> 00:05:04,086 A:middle
So let's get started.

116
00:05:04,716 --> 00:05:08,946 A:middle
The two main objects
that we're going to start

117
00:05:08,946 --> 00:05:11,616 A:middle
with are the engine
object and the node object.

118
00:05:11,616 --> 00:05:15,176 A:middle
And there are three specific
types of nodes: the output node,

119
00:05:15,346 --> 00:05:16,736 A:middle
mixer node and the player node.

120
00:05:17,826 --> 00:05:19,986 A:middle
We have other nodes as
well that we will get to

121
00:05:20,136 --> 00:05:22,126 A:middle
but these are the initial
building block nodes.

122
00:05:22,426 --> 00:05:24,726 A:middle
So the engine is an object

123
00:05:25,006 --> 00:05:27,256 A:middle
that maintains a
graph of audio nodes.

124
00:05:28,396 --> 00:05:31,026 A:middle
You create nodes and you
attach them to the engine

125
00:05:31,026 --> 00:05:33,306 A:middle
and then you use the
engine to make connections

126
00:05:33,346 --> 00:05:34,776 A:middle
between these different
audio nodes.

127
00:05:35,616 --> 00:05:38,986 A:middle
The engine will analyze these
connections and determine

128
00:05:38,986 --> 00:05:41,036 A:middle
which ones add up
to an active chain.

129
00:05:41,926 --> 00:05:43,496 A:middle
When you then start the engine,

130
00:05:44,226 --> 00:05:46,636 A:middle
audio flows through all
of the active chains.

131
00:05:47,746 --> 00:05:51,816 A:middle
A powerful feature that the
engine has is that it allows you

132
00:05:51,816 --> 00:05:54,056 A:middle
to dynamically reconfigure
these nodes.

133
00:05:54,476 --> 00:05:58,476 A:middle
This means that while the engine
is rendering you can add new

134
00:05:58,476 --> 00:06:00,796 A:middle
nodes and then wire them up.

135

136
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

137
00:05:58,476 --> 00:06:00,796 A:middle
nodes and then wire them up.

138
00:06:00,956 --> 00:06:04,066 A:middle
And so essentially you're adding
or removing chains dynamically.

139
00:06:04,726 --> 00:06:07,936 A:middle
So the typical workflow
of the engine is

140
00:06:07,936 --> 00:06:10,876 A:middle
that you create an instance of
the engine, create instances

141
00:06:10,876 --> 00:06:12,636 A:middle
of all the nodes you
want to work with,

142
00:06:13,076 --> 00:06:15,946 A:middle
attach them to the engine so
the engine is now aware of them

143
00:06:15,946 --> 00:06:19,756 A:middle
and then connect them
together, start the engine.

144
00:06:20,436 --> 00:06:22,166 A:middle
This will create an
active render thread

145
00:06:22,166 --> 00:06:25,666 A:middle
and audio will flow through
all of the active chains.

146
00:06:26,856 --> 00:06:29,236 A:middle
So let's now talk about a node.

147
00:06:30,106 --> 00:06:33,486 A:middle
A node is a basic audio
block and we have three types

148
00:06:33,486 --> 00:06:36,376 A:middle
of nodes: there are source
nodes, which are nodes

149
00:06:36,376 --> 00:06:37,406 A:middle
that generate an audio.

150
00:06:37,806 --> 00:06:40,726 A:middle
And examples of this are the
player or the input node.

151
00:06:41,646 --> 00:06:43,596 A:middle
You have nodes that
process audio.

152
00:06:44,066 --> 00:06:47,326 A:middle
So they take some audio and do
something to it and push it up.

153
00:06:47,736 --> 00:06:50,626 A:middle
And examples are a
mixer or an effect.

154
00:06:51,046 --> 00:06:53,886 A:middle
You also have destination
nodes that receive audio

155
00:06:53,976 --> 00:06:55,966 A:middle
and do something with it.

156
00:06:56,396 --> 00:06:58,796 A:middle
Every one of these nodes
has a certain number

157
00:06:58,796 --> 00:07:01,766 A:middle
of input and output buses.

158

159
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

160
00:06:58,796 --> 00:07:01,766 A:middle
of input and output buses.

161
00:07:01,766 --> 00:07:05,886 A:middle
And typically you see that
most nodes have a single input

162
00:07:05,886 --> 00:07:06,976 A:middle
and output bus.

163
00:07:07,366 --> 00:07:09,516 A:middle
But an exception to
this is a mixer node

164
00:07:09,726 --> 00:07:12,796 A:middle
that has multiple input busses
and a single output bus.

165
00:07:13,816 --> 00:07:17,946 A:middle
Every bus now has an audio
data format associated with it.

166
00:07:18,756 --> 00:07:20,486 A:middle
So let's talk about connections.

167
00:07:21,806 --> 00:07:24,276 A:middle
If you have a connection
between a source node

168
00:07:24,516 --> 00:07:27,416 A:middle
and a destination node,
that forms an active chain.

169
00:07:28,326 --> 00:07:30,636 A:middle
You can insert any
number of processing nodes

170
00:07:30,636 --> 00:07:32,956 A:middle
between the source node
and the destination node.

171
00:07:33,456 --> 00:07:35,576 A:middle
But as long as you
wire every bit

172
00:07:35,576 --> 00:07:37,806 A:middle
of this chain up,
it's an active chain.

173
00:07:38,546 --> 00:07:41,986 A:middle
As soon as you break one of the
connections, all of the nodes

174
00:07:42,016 --> 00:07:44,766 A:middle
that are upstream of the
point of disconnection go

175
00:07:44,766 --> 00:07:45,876 A:middle
into an inactive state.

176
00:07:46,716 --> 00:07:48,636 A:middle
In this case, I've
broken the connection

177
00:07:48,796 --> 00:07:51,236 A:middle
between the processing node
and the destination node,

178
00:07:51,736 --> 00:07:52,806 A:middle
so my processing node

179
00:07:52,806 --> 00:07:55,226 A:middle
and my source node are
now in an inactive state.

180
00:07:56,316 --> 00:07:57,976 A:middle
The same holds true
in this example.

181
00:07:57,976 --> 00:08:03,016 A:middle
So let's now look at
the specific node types.

182

183
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

184
00:07:57,976 --> 00:08:03,016 A:middle
So let's now look at
the specific node types.

185
00:08:03,796 --> 00:08:05,256 A:middle
The first node that
we're going to talk

186
00:08:05,256 --> 00:08:06,236 A:middle
about is the output node.

187
00:08:06,916 --> 00:08:09,486 A:middle
The engine has an
implicit destination node

188
00:08:09,666 --> 00:08:10,876 A:middle
and it's called the output node.

189
00:08:10,876 --> 00:08:14,156 A:middle
And the role of the output
node is to take the data

190
00:08:14,156 --> 00:08:16,756 A:middle
that it receives and hand
it to the output hardware,

191
00:08:17,076 --> 00:08:18,136 A:middle
so this could be the speaker.

192
00:08:19,406 --> 00:08:22,236 A:middle
You cannot create a standalone
instance of the output node.

193
00:08:22,526 --> 00:08:24,196 A:middle
You have to get it
from the instance

194
00:08:24,196 --> 00:08:27,466 A:middle
of the engine that
you've created.

195
00:08:27,566 --> 00:08:29,386 A:middle
Let's move on to the mixer node.

196
00:08:30,186 --> 00:08:33,726 A:middle
Mixer nodes are processing
nodes and they receive data

197
00:08:34,116 --> 00:08:36,666 A:middle
on different input
busses which they then mix

198
00:08:37,176 --> 00:08:40,096 A:middle
to a single output, which
goes out on the output bus.

199
00:08:41,145 --> 00:08:43,346 A:middle
When you use a mixer,
you get control

200
00:08:43,346 --> 00:08:45,236 A:middle
of the volume of each input bus.

201
00:08:45,696 --> 00:08:47,946 A:middle
And if you add an application
that was playing a number

202
00:08:47,946 --> 00:08:50,196 A:middle
of sounds and you put
each of these sounds

203
00:08:50,256 --> 00:08:51,746 A:middle
in on a separate input bus,

204
00:08:52,406 --> 00:08:55,376 A:middle
using this volume control
you can essentially blend

205
00:08:55,376 --> 00:08:57,926 A:middle
in the amount of each sound
that you want to hear.

206
00:08:58,236 --> 00:08:59,556 A:middle
So you create a mix.

207

208
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

209
00:09:00,656 --> 00:09:03,806 A:middle
You now have control
over the output volume

210
00:09:03,946 --> 00:09:05,086 A:middle
as well using a mixer.

211
00:09:05,486 --> 00:09:06,956 A:middle
So you are controlling
the volume

212
00:09:06,956 --> 00:09:08,576 A:middle
of the mix that you've created.

213
00:09:09,456 --> 00:09:13,206 A:middle
If your application has
several categories of sound,

214
00:09:13,496 --> 00:09:16,026 A:middle
you can make use of a
concept called submixing

215
00:09:16,346 --> 00:09:17,546 A:middle
to create submixers.

216
00:09:18,276 --> 00:09:20,516 A:middle
So let's say that you
have some UI sounds

217
00:09:20,576 --> 00:09:21,586 A:middle
and you have some music.

218
00:09:22,076 --> 00:09:24,166 A:middle
And you put all of the UI
sounds through one mixer,

219
00:09:24,346 --> 00:09:25,996 A:middle
all of the music
through another mixer.

220
00:09:26,726 --> 00:09:29,246 A:middle
Using the output volumes
of each of these mixers,

221
00:09:29,246 --> 00:09:30,886 A:middle
you can essentially
control the volume

222
00:09:30,886 --> 00:09:32,366 A:middle
of each of these submixers.

223
00:09:33,196 --> 00:09:35,976 A:middle
Let's take that concept a
step further and put all

224
00:09:35,976 --> 00:09:38,456 A:middle
of the submixers
through a master mixer.

225
00:09:39,646 --> 00:09:43,136 A:middle
The output volume of the master
mixer will essentially control

226
00:09:43,136 --> 00:09:45,676 A:middle
the volume of the entire
mix in your application.

227
00:09:46,926 --> 00:09:49,776 A:middle
Now the engine has an
implicit mixer node.

228
00:09:50,306 --> 00:09:52,666 A:middle
And when you ask the
engine for its mixer node,

229
00:09:52,986 --> 00:09:54,576 A:middle
it creates an instance
of a mixer.

230
00:09:54,576 --> 00:09:56,696 A:middle
It creates an instance
of the output node

231
00:09:56,996 --> 00:09:59,196 A:middle
and connects it together
for you by default.

232

233
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

234
00:10:00,326 --> 00:10:02,346 A:middle
The difference here
between the mixer node

235
00:10:02,346 --> 00:10:05,396 A:middle
and the output node is that you
can create additional instances

236
00:10:05,856 --> 00:10:07,496 A:middle
and then attach them
to the engine

237
00:10:07,496 --> 00:10:08,846 A:middle
and use them how you please.

238
00:10:10,396 --> 00:10:14,346 A:middle
Mixers can also have
different audio data formats

239
00:10:14,456 --> 00:10:15,826 A:middle
for each input bus.

240
00:10:16,166 --> 00:10:17,696 A:middle
And the mixer will do the work

241
00:10:17,866 --> 00:10:21,456 A:middle
of efficiently converting
the input data formats

242
00:10:21,896 --> 00:10:23,846 A:middle
to the output data format.

243
00:10:24,836 --> 00:10:28,426 A:middle
So now that we have looked
at these initial nodes,

244
00:10:28,806 --> 00:10:32,156 A:middle
let's talk about how this works
in the context of the engine.

245
00:10:32,696 --> 00:10:33,676 A:middle
So let's say that I have an app

246
00:10:33,996 --> 00:10:35,666 A:middle
that creates an instance
of the engine.

247
00:10:35,666 --> 00:10:39,376 A:middle
We can now ask the engine
for its main mixer node

248
00:10:39,376 --> 00:10:41,676 A:middle
so it's going to create
the instance of a mixer,

249
00:10:41,886 --> 00:10:43,566 A:middle
create a mixer of
the output node

250
00:10:43,886 --> 00:10:45,556 A:middle
and connect the two together.

251
00:10:46,196 --> 00:10:49,666 A:middle
I can now create a clear note
and attach it to the engine

252
00:10:50,166 --> 00:10:51,386 A:middle
and connect it to the mixer.

253
00:10:52,166 --> 00:10:54,746 A:middle
So at this point I have a
connection chain going all the

254
00:10:54,746 --> 00:10:56,636 A:middle
way from a source
to a destination,

255
00:10:57,126 --> 00:10:58,686 A:middle
so I have an active chain.

256
00:10:59,746 --> 00:11:01,486 A:middle
When I then start up the engine,

257

258
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

259
00:10:59,746 --> 00:11:01,486 A:middle
When I then start up the engine,

260
00:11:01,846 --> 00:11:03,776 A:middle
an active render
thread is created

261
00:11:03,776 --> 00:11:05,916 A:middle
and data is pulled
by the destination.

262
00:11:06,676 --> 00:11:08,606 A:middle
So I have an active
flow of data here.

263
00:11:09,626 --> 00:11:13,316 A:middle
The app can now interact
with each one of these blocks

264
00:11:13,806 --> 00:11:16,106 A:middle
and any change that
it makes on any

265
00:11:16,106 --> 00:11:18,246 A:middle
of the nodes will take
effect immediately.

266
00:11:19,886 --> 00:11:21,006 A:middle
So now that we've talked

267
00:11:21,156 --> 00:11:26,056 A:middle
about an active render thread
how do you push your audio data

268
00:11:26,266 --> 00:11:27,186 A:middle
on the render thread.

269
00:11:27,736 --> 00:11:29,486 A:middle
You use a player to do that.

270
00:11:30,636 --> 00:11:31,716 A:middle
Let's look at player nodes.

271
00:11:32,656 --> 00:11:34,556 A:middle
Player nodes are nodes
that can play data

272
00:11:34,556 --> 00:11:36,876 A:middle
from files and from buffers.

273
00:11:37,116 --> 00:11:40,346 A:middle
And the way that it happens
or the way that it's done is

274
00:11:40,346 --> 00:11:41,506 A:middle
by scheduling events,

275
00:11:42,086 --> 00:11:45,046 A:middle
which simply means play
data at a specified time.

276
00:11:45,866 --> 00:11:49,466 A:middle
That data, that time could be
now or sometime in the future.

277
00:11:51,266 --> 00:11:54,666 A:middle
When you're scheduling buffers
you can schedule either multiple

278
00:11:54,666 --> 00:11:57,956 A:middle
buffers and as each
buffer is consumed

279
00:11:57,956 --> 00:12:00,076 A:middle
by the player you get
an individual callback,

280

281
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

282
00:11:57,956 --> 00:12:00,076 A:middle
by the player you get
an individual callback,

283
00:12:00,626 --> 00:12:04,216 A:middle
which you can then use a cue to
go ahead and schedule more data.

284
00:12:05,516 --> 00:12:07,806 A:middle
You can also schedule
a single buffer

285
00:12:07,806 --> 00:12:09,256 A:middle
that plays in a loop fashion.

286
00:12:10,056 --> 00:12:12,976 A:middle
And this is useful in the case
when you may have a musical loop

287
00:12:13,016 --> 00:12:15,836 A:middle
or a sound effect that you want
to play over and over again.

288
00:12:16,476 --> 00:12:19,436 A:middle
So, you load the data and
then you play the buffer

289
00:12:19,436 --> 00:12:22,886 A:middle
and it'll continue to play
until you stop the player

290
00:12:23,276 --> 00:12:25,776 A:middle
or you interrupt it
with another buffer.

291
00:12:25,776 --> 00:12:27,076 A:middle
We'll get into that.

292
00:12:27,916 --> 00:12:30,146 A:middle
You can also schedule
a file or a portion

293
00:12:30,146 --> 00:12:31,486 A:middle
of a file called a segment.

294
00:12:32,986 --> 00:12:35,966 A:middle
So going back to our previous
diagram, we had an engine

295
00:12:35,966 --> 00:12:37,026 A:middle
that was in a running state.

296
00:12:37,676 --> 00:12:41,356 A:middle
So now I can create an instance
of a buffer and load my data

297
00:12:41,426 --> 00:12:43,856 A:middle
into it, shown by the red arrow.

298
00:12:44,156 --> 00:12:48,216 A:middle
Once I do that, I can schedule
this buffer on the player.

299
00:12:48,666 --> 00:12:52,326 A:middle
And when the player is playing,
the player will consume the data

300
00:12:52,566 --> 00:12:55,516 A:middle
in the buffer and push
it on the render thread.

301
00:12:55,516 --> 00:12:59,526 A:middle
In a similar manner, I can
work with multiple buffers.

302
00:12:59,926 --> 00:13:02,086 A:middle
So over here I have
multiple buffers.

303

304
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

305
00:12:59,926 --> 00:13:02,086 A:middle
So over here I have
multiple buffers.

306
00:13:02,176 --> 00:13:05,796 A:middle
I load data into each one of
them and I schedule each one

307
00:13:05,796 --> 00:13:08,126 A:middle
of them to play in
sequence on the player.

308
00:13:09,456 --> 00:13:12,026 A:middle
As each buffer is
consumed by the player,

309
00:13:12,496 --> 00:13:14,916 A:middle
I get individual
callbacks letting me know

310
00:13:14,916 --> 00:13:16,006 A:middle
that that buffer is done.

311
00:13:16,006 --> 00:13:18,996 A:middle
I can use that as a cue
and schedule more data.

312
00:13:19,656 --> 00:13:24,316 A:middle
In a similar manner you
can work with a file.

313
00:13:24,316 --> 00:13:27,606 A:middle
And the difference here is you
don't have to actually deal

314
00:13:27,606 --> 00:13:29,046 A:middle
with the audio data yourself.

315
00:13:29,786 --> 00:13:32,996 A:middle
All you need is a URL
to a physical audio file

316
00:13:33,316 --> 00:13:35,996 A:middle
with which you can create
an AVAudioFile object

317
00:13:36,566 --> 00:13:38,516 A:middle
and then schedule that
directly on the player.

318
00:13:39,146 --> 00:13:41,966 A:middle
The player will do the work of
reading the data from the file

319
00:13:42,086 --> 00:13:45,716 A:middle
and pushing it on
the render thread.

320
00:13:45,896 --> 00:13:48,426 A:middle
So, let's now look
at a good example

321
00:13:48,456 --> 00:13:49,806 A:middle
of how we can achieve this.

322
00:13:50,366 --> 00:13:54,966 A:middle
I first create an instance of
the engine, create an instance

323
00:13:54,966 --> 00:13:57,816 A:middle
of a player and attach
the player to the engine,

324
00:13:57,986 --> 00:13:59,516 A:middle
so the engine is now
aware of the player.

325
00:13:59,816 --> 00:14:02,986 A:middle
I'm now going to
split my example up

326

327
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

328
00:13:59,816 --> 00:14:02,986 A:middle
I'm now going to
split my example up

329
00:14:02,986 --> 00:14:05,066 A:middle
and show how you can
first work with a file.

330
00:14:05,986 --> 00:14:08,636 A:middle
So, given a URL to
an audio file,

331
00:14:08,726 --> 00:14:10,936 A:middle
I can create the
AudioFile object.

332
00:14:12,366 --> 00:14:15,866 A:middle
The next thing that I do is ask
the engine for its mainMixer.

333
00:14:16,676 --> 00:14:19,066 A:middle
So the engine will create
an instance of a mixer node,

334
00:14:19,716 --> 00:14:22,226 A:middle
create an output node and
connect the two together.

335
00:14:22,226 --> 00:14:26,726 A:middle
I can now go ahead and
connect the player to the mixer

336
00:14:27,386 --> 00:14:28,936 A:middle
with the files processing
format.

337
00:14:28,936 --> 00:14:31,526 A:middle
So I have so I have a connection
chain going all the way

338
00:14:31,526 --> 00:14:33,026 A:middle
from a player that's a source

339
00:14:33,586 --> 00:14:35,516 A:middle
to the output node
that's a destination.

340
00:14:37,176 --> 00:14:40,776 A:middle
Now I can schedule my
file to play atTime:nil,

341
00:14:41,056 --> 00:14:42,666 A:middle
which is as soon as possible.

342
00:14:43,206 --> 00:14:45,776 A:middle
And in this case I pass a nil
for the completion handler.

343
00:14:46,486 --> 00:14:48,326 A:middle
If I had some work that
I needed to be done

344
00:14:48,426 --> 00:14:50,276 A:middle
after the file is
consumed by the player,

345
00:14:50,486 --> 00:14:51,826 A:middle
I can pass in a block over here.

346
00:14:53,146 --> 00:14:56,986 A:middle
So in a similar manner I can
work with a buffer as well.

347
00:14:57,636 --> 00:15:01,066 A:middle
Let's say that I create
an AVAudioPCMBuffer object

348

349
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

350
00:14:57,636 --> 00:15:01,066 A:middle
Let's say that I create
an AVAudioPCMBuffer object

351
00:15:01,156 --> 00:15:02,996 A:middle
and load some data into it.

352
00:15:03,546 --> 00:15:06,876 A:middle
The specifics of that part
are covered in session 501.

353
00:15:07,016 --> 00:15:09,496 A:middle
So if you missed that,
please refer to that session.

354
00:15:10,346 --> 00:15:13,966 A:middle
Once I have my buffer object I
can go ahead and ask the engine

355
00:15:13,966 --> 00:15:17,116 A:middle
for its mixer and make the
connection between the player

356
00:15:17,526 --> 00:15:20,216 A:middle
to the mixer with
the buffer's format.

357
00:15:21,196 --> 00:15:25,226 A:middle
Now I can go ahead and
schedule this buffer atTime:nil,

358
00:15:25,226 --> 00:15:27,036 A:middle
which is as soon as possible.

359
00:15:27,726 --> 00:15:29,776 A:middle
But note that we have
an additional argument

360
00:15:29,966 --> 00:15:32,856 A:middle
when we are working with
buffers, the options argument.

361
00:15:33,536 --> 00:15:35,656 A:middle
We're going to talk about
that right after this.

362
00:15:35,766 --> 00:15:37,246 A:middle
But for now I'm going
to pass nil,

363
00:15:38,386 --> 00:15:40,296 A:middle
and nil for the completion
handler as well.

364
00:15:41,256 --> 00:15:42,816 A:middle
So now that I've
scheduled my data

365
00:15:43,206 --> 00:15:46,046 A:middle
on the player I can go
ahead and start the engine.

366
00:15:46,506 --> 00:15:48,196 A:middle
This creates an active
render thread

367
00:15:48,196 --> 00:15:50,376 A:middle
and then call play
on the player.

368
00:15:50,376 --> 00:15:53,336 A:middle
And the player will do the
work of creating the data

369
00:15:53,706 --> 00:15:56,666 A:middle
from the file in the buffer and
pushing it on the render thread.

370
00:15:57,566 --> 00:15:59,946 A:middle
So let's now talk about
the different buffer

371
00:15:59,946 --> 00:16:01,186 A:middle
scheduling options.

372

373
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

374
00:15:59,946 --> 00:16:01,186 A:middle
scheduling options.

375
00:16:03,436 --> 00:16:06,596 A:middle
In all of the examples that
I'm going to talk about now,

376
00:16:06,596 --> 00:16:10,126 A:middle
I'm going to specify nil
for the atTime argument

377
00:16:10,126 --> 00:16:12,976 A:middle
and that just means that
in all of these examples,

378
00:16:13,156 --> 00:16:15,976 A:middle
I'm going to schedule something
to play as soon as possible.

379
00:16:16,606 --> 00:16:20,076 A:middle
So let's talk about the first
option and that's when you want

380
00:16:20,076 --> 00:16:22,746 A:middle
to schedule a buffer to
play as soon as possible.

381
00:16:23,506 --> 00:16:27,826 A:middle
In that case all you need
to do is schedule a buffer

382
00:16:27,826 --> 00:16:28,946 A:middle
with the option set to nil.

383
00:16:29,566 --> 00:16:32,066 A:middle
You call play on the player
and that buffer gets played.

384
00:16:33,676 --> 00:16:36,146 A:middle
If you have a buffer that's
playing now and you want

385
00:16:36,146 --> 00:16:39,816 A:middle
to append a new buffer,
it's the exact same call.

386
00:16:40,416 --> 00:16:43,506 A:middle
You schedule the new buffer
with the option set to nil

387
00:16:44,036 --> 00:16:47,216 A:middle
and so the new buffer
gets appended to the cue

388
00:16:47,216 --> 00:16:48,676 A:middle
of currently playing buffers.

389
00:16:48,676 --> 00:16:51,536 A:middle
On the other hand if I want

390
00:16:51,536 --> 00:16:54,686 A:middle
to interrupt my currently
playing buffer

391
00:16:54,856 --> 00:16:58,156 A:middle
with a new buffer I can
schedule the new buffer

392
00:16:58,346 --> 00:17:01,486 A:middle
with the AVAudioPlayerNode
BufferInterrupts option.

393

394
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

395
00:16:58,346 --> 00:17:01,486 A:middle
with the AVAudioPlayerNode
BufferInterrupts option.

396
00:17:02,056 --> 00:17:04,476 A:middle
So that will interrupt the
currently playing buffer

397
00:17:04,596 --> 00:17:06,836 A:middle
and start playing my
new buffer right away.

398
00:17:08,445 --> 00:17:11,616 A:middle
Let's now look at the different
variants with a looping buffer.

399
00:17:12,346 --> 00:17:14,695 A:middle
So like I said earlier,
if I have a buffer that's

400
00:17:14,695 --> 00:17:17,016 A:middle
to be played in a looped
fashion, like a sound effect,

401
00:17:17,016 --> 00:17:21,066 A:middle
for instance, I can load the
data in that buffer and schedule

402
00:17:21,066 --> 00:17:24,046 A:middle
that buffer with the
AVAudioPlayerNodeBufferLoops

403
00:17:24,046 --> 00:17:24,435 A:middle
option.

404
00:17:25,346 --> 00:17:27,976 A:middle
When I call play on the
player, that buffer starts

405
00:17:27,976 --> 00:17:29,306 A:middle
to play in a looped fashion.

406
00:17:30,946 --> 00:17:35,826 A:middle
If I want to interrupt a looping
buffer it's the same option

407
00:17:35,876 --> 00:17:37,206 A:middle
as what we've seen before.

408
00:17:37,206 --> 00:17:39,346 A:middle
I have to schedule a new buffer

409
00:17:39,566 --> 00:17:42,646 A:middle
with the AVAudioPlayerNode
BufferInterrupts option.

410
00:17:43,296 --> 00:17:46,106 A:middle
So essentially it's the same
option for when you want

411
00:17:46,106 --> 00:17:49,706 A:middle
to interrupt a regular
buffer or a looping buffer.

412
00:17:51,256 --> 00:17:53,786 A:middle
The last case is
an interesting one.

413
00:17:54,626 --> 00:17:56,936 A:middle
So if you have a looping
buffer but you want

414
00:17:56,936 --> 00:18:00,636 A:middle
to let the current loop finish
before you start playing your

415

416
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

417
00:17:56,936 --> 00:18:00,636 A:middle
to let the current loop finish
before you start playing your

418
00:18:00,636 --> 00:18:04,516 A:middle
new data you can
schedule your new buffer

419
00:18:04,646 --> 00:18:08,576 A:middle
with the AVAudioPlayerNodeBuffer
InterruptsAtLoop option.

420
00:18:09,576 --> 00:18:13,276 A:middle
So this will let the current
loop finish and as soon

421
00:18:13,276 --> 00:18:16,266 A:middle
as that loop is done the
new buffer starts playing.

422
00:18:17,006 --> 00:18:20,666 A:middle
Now that was a whole bunch
of options, so let's look

423
00:18:20,666 --> 00:18:23,736 A:middle
at one practical example of
how we can use these options.

424
00:18:24,476 --> 00:18:26,616 A:middle
So let's say that I have
a sound that's broken

425
00:18:26,616 --> 00:18:27,696 A:middle
up into three parts.

426
00:18:27,786 --> 00:18:30,106 A:middle
And the example that I'm
going to use here is a siren.

427
00:18:31,066 --> 00:18:33,996 A:middle
So you have the initial buildup
of the sound which is the

428
00:18:33,996 --> 00:18:35,346 A:middle
"attack" portion of the siren.

429
00:18:36,316 --> 00:18:38,706 A:middle
You have the droning
portion of the siren,

430
00:18:38,776 --> 00:18:41,196 A:middle
which can be modeled using
just a looping buffer.

431
00:18:41,896 --> 00:18:43,976 A:middle
And this is the "sustain"
portion of the sound.

432
00:18:44,746 --> 00:18:46,976 A:middle
And then you have the
dying down of the siren,

433
00:18:47,246 --> 00:18:48,966 A:middle
which is the "release"
portion of the sound.

434
00:18:49,376 --> 00:18:52,776 A:middle
So let's say that I load
up each of these sounds

435
00:18:52,826 --> 00:18:53,886 A:middle
into different buffers.

436
00:18:54,186 --> 00:18:57,346 A:middle
The way that I can
implement this in code is

437
00:18:57,346 --> 00:19:00,786 A:middle
to first schedule the attack
buffer with my options set

438

439
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

440
00:18:57,346 --> 00:19:00,786 A:middle
to first schedule the attack
buffer with my options set

441
00:19:00,786 --> 00:19:04,576 A:middle
to nil and then schedule
the sustain buffer

442
00:19:04,996 --> 00:19:07,746 A:middle
with the AVAudioPlayerNodeBuffer
loops option.

443
00:19:08,876 --> 00:19:10,586 A:middle
So when I call play
on the player,

444
00:19:10,966 --> 00:19:13,996 A:middle
what this will do is play the
attack portion of the sound

445
00:19:14,476 --> 00:19:17,036 A:middle
and then immediately start
playing the sustain portion

446
00:19:17,036 --> 00:19:20,866 A:middle
of the sound and continue
to loop that sustain buffer

447
00:19:21,356 --> 00:19:23,856 A:middle
and that goes on until
I'm ready to interrupt it.

448
00:19:24,716 --> 00:19:27,746 A:middle
So after some time has gone
by when I'm ready to interrupt

449
00:19:27,786 --> 00:19:30,866 A:middle
that I can schedule
my release buffer

450
00:19:31,056 --> 00:19:34,486 A:middle
with the AVAudioPlayerNodeBuffer
InterruptsAtLoop option.

451
00:19:34,966 --> 00:19:38,596 A:middle
So this will let the last loop
of the sustained buffer finish

452
00:19:38,596 --> 00:19:42,486 A:middle
up and then play the
release portion of my sound.

453
00:19:44,756 --> 00:19:47,526 A:middle
Now remember that I said
in the beginning that all

454
00:19:47,526 --> 00:19:49,856 A:middle
of my examples involve
scheduling events

455
00:19:49,856 --> 00:19:51,206 A:middle
that play as soon as possible.

456
00:19:51,826 --> 00:19:54,346 A:middle
I can also schedule events
to play in the future.

457
00:19:54,796 --> 00:19:56,336 A:middle
So here's an example of that.

458
00:19:56,796 --> 00:19:59,276 A:middle
In this case I'm just
going to schedule a buffer

459
00:19:59,276 --> 00:20:01,936 A:middle
to play 10 seconds
in the future.

460

461
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

462
00:19:59,276 --> 00:20:01,936 A:middle
to play 10 seconds
in the future.

463
00:20:02,096 --> 00:20:04,186 A:middle
So I create an AVAudioTime
object

464
00:20:04,836 --> 00:20:08,566 A:middle
that has a relative sample
time 10 seconds in the future,

465
00:20:08,986 --> 00:20:11,716 A:middle
and I use the buffer sampleRate
as my reference point.

466
00:20:12,846 --> 00:20:16,196 A:middle
I can now schedule the buffer
with this AVAudioTime object

467
00:20:16,196 --> 00:20:18,536 A:middle
and call play on the player

468
00:20:18,536 --> 00:20:24,706 A:middle
and my buffer gets played
10 seconds in the future.

469
00:20:24,796 --> 00:20:27,856 A:middle
So we've talked about
player nodes

470
00:20:28,076 --> 00:20:29,796 A:middle
and how you can use a player

471
00:20:30,346 --> 00:20:33,016 A:middle
to push your audio data
on the render thread.

472
00:20:33,966 --> 00:20:35,386 A:middle
Well if you wanted to pull data

473
00:20:35,386 --> 00:20:38,016 A:middle
from the render thread
how do you do that?

474
00:20:38,486 --> 00:20:40,546 A:middle
You use a node tap.

475
00:20:40,546 --> 00:20:43,336 A:middle
And here's some reasons for
why you may want to do that.

476
00:20:43,526 --> 00:20:45,796 A:middle
Let's say you want to capture
the output of the microphone

477
00:20:46,626 --> 00:20:51,796 A:middle
and save that data to disk, or
if you have a music application

478
00:20:51,946 --> 00:20:53,796 A:middle
and you want to record
a live performance

479
00:20:54,576 --> 00:20:56,216 A:middle
or if you have a
game and you want

480
00:20:56,216 --> 00:20:57,836 A:middle
to capture the output
mix of the game.

481
00:20:58,456 --> 00:21:00,976 A:middle
You can do all of
that using a node tap.

482

483
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

484
00:20:58,456 --> 00:21:00,976 A:middle
You can do all of
that using a node tap.

485
00:21:01,286 --> 00:21:05,096 A:middle
And what that is, is essentially
a tap that you install

486
00:21:05,096 --> 00:21:06,736 A:middle
on the output bus of a node.

487
00:21:07,726 --> 00:21:11,176 A:middle
So the data that's captured
by the tap is returned back

488
00:21:11,176 --> 00:21:14,146 A:middle
to your application
via the callback log.

489
00:21:14,716 --> 00:21:16,686 A:middle
So going back to a
familiar diagram,

490
00:21:17,086 --> 00:21:18,706 A:middle
I have two players
that's connected

491
00:21:18,706 --> 00:21:19,856 A:middle
to the engines main mixer.

492
00:21:19,856 --> 00:21:24,946 A:middle
And I want to tap the output of
the mixer so I can install a tap

493
00:21:24,946 --> 00:21:27,886 A:middle
on the mixer and the tap
will start pulling data

494
00:21:27,886 --> 00:21:28,826 A:middle
from the render thread.

495
00:21:29,726 --> 00:21:32,636 A:middle
I can then go ahead, the
tap will then go ahead

496
00:21:32,806 --> 00:21:36,696 A:middle
and create a buffer object,
stuff that data into the buffer

497
00:21:37,456 --> 00:21:38,846 A:middle
and return that back

498
00:21:38,846 --> 00:21:41,016 A:middle
to the application
via a callback block.

499
00:21:41,966 --> 00:21:45,186 A:middle
In code it's just
one function call.

500
00:21:45,796 --> 00:21:53,426 A:middle
You install a tap on the mixer's
output bus 0 with a buffer size

501
00:21:53,426 --> 00:21:59,766 A:middle
of 4096 frames and the mixer's
output format for that bus.

502

503
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

504
00:22:00,106 --> 00:22:03,716 A:middle
Within the block I have an
AVAudioPCMBuffer that contains

505
00:22:03,946 --> 00:22:05,036 A:middle
that much amount of data.

506
00:22:05,636 --> 00:22:07,606 A:middle
And I can do whatever I
need to do with that data.

507
00:22:08,546 --> 00:22:10,336 A:middle
Alright, so to quickly
summarize,

508
00:22:10,726 --> 00:22:12,426 A:middle
you have an active
render thread.

509
00:22:12,686 --> 00:22:14,986 A:middle
You use player nodes
to push your audio data

510
00:22:15,246 --> 00:22:18,616 A:middle
on the render thread and use
node taps to pull audio data

511
00:22:18,826 --> 00:22:20,916 A:middle
from the render thread.

512
00:22:21,246 --> 00:22:23,216 A:middle
Let's now switch gears and talk

513
00:22:23,216 --> 00:22:25,436 A:middle
about a new node
called the input node.

514
00:22:26,496 --> 00:22:29,776 A:middle
The input node receives
data from the input hardware

515
00:22:29,886 --> 00:22:31,896 A:middle
and it's parallel
to the output node.

516
00:22:32,776 --> 00:22:35,836 A:middle
With the input node you cannot
create a standalone instance.

517
00:22:36,056 --> 00:22:40,776 A:middle
You have to get the
instance from the engine.

518
00:22:40,776 --> 00:22:44,396 A:middle
When you've connected the
input node in an active chain

519
00:22:44,396 --> 00:22:49,206 A:middle
and the engine is running, data
is pulled from the input node.

520
00:22:50,346 --> 00:22:52,076 A:middle
So let's go back to
a familiar diagram.

521
00:22:52,586 --> 00:22:55,496 A:middle
I've connected the input
node to the mixer nodes

522
00:22:55,946 --> 00:22:57,616 A:middle
and that's connected
to the output node.

523
00:22:58,226 --> 00:23:01,126 A:middle
So when I start the engine,
this is an active chain

524

525
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

526
00:22:58,226 --> 00:23:01,126 A:middle
So when I start the engine,
this is an active chain

527
00:23:01,516 --> 00:23:03,956 A:middle
and data is pulled
from the input node.

528
00:23:04,806 --> 00:23:07,676 A:middle
So, if I'm receiving
data from the input node

529
00:23:07,746 --> 00:23:11,736 A:middle
and the engine is running and
I want to stop receiving data

530
00:23:11,736 --> 00:23:14,056 A:middle
at a certain point,
how do I do that?

531
00:23:14,486 --> 00:23:15,186 A:middle
It's very simple.

532
00:23:15,586 --> 00:23:18,236 A:middle
All you have to do-oh I'm sorry.

533
00:23:18,236 --> 00:23:18,936 A:middle
I raced ahead.

534
00:23:18,936 --> 00:23:20,786 A:middle
Let's look at a code example

535
00:23:20,946 --> 00:23:22,446 A:middle
of how you can connect
the input node.

536
00:23:23,416 --> 00:23:25,416 A:middle
So I get the input
node from the engine.

537
00:23:26,126 --> 00:23:28,016 A:middle
Just make a connection
to any other node

538
00:23:28,526 --> 00:23:30,076 A:middle
with the input node's
hardware format

539
00:23:31,596 --> 00:23:32,846 A:middle
and then start the engine.

540
00:23:33,396 --> 00:23:34,736 A:middle
This creates an active
render thread

541
00:23:34,736 --> 00:23:36,456 A:middle
and the input nodes
pull for data.

542
00:23:37,426 --> 00:23:38,836 A:middle
So like I was saying earlier,

543
00:23:38,906 --> 00:23:42,646 A:middle
if you have an input node that's
being pulled and you don't want

544
00:23:42,646 --> 00:23:45,856 A:middle
to receive data anymore from
the input node, what do you do?

545
00:23:46,266 --> 00:23:47,856 A:middle
Just disconnect the input node.

546
00:23:48,656 --> 00:23:51,496 A:middle
So the input node will no
longer be in an active chain

547
00:23:52,416 --> 00:23:53,756 A:middle
and it won't be pulled for data.

548
00:23:54,786 --> 00:23:57,276 A:middle
In order to do that, it's
just one line of code.

549
00:23:57,896 --> 00:24:00,776 A:middle
Using the engine, you
disconnect the node output

550

551
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

552
00:23:57,896 --> 00:24:00,776 A:middle
Using the engine, you
disconnect the node output

553
00:24:00,776 --> 00:24:02,396 A:middle
of the input node.

554
00:24:02,396 --> 00:24:08,456 A:middle
Now if you want to capture
data from the input node,

555
00:24:08,756 --> 00:24:11,296 A:middle
you can install a node tap
and we've talked about that.

556
00:24:12,086 --> 00:24:15,216 A:middle
But what's interesting about
this particular example is,

557
00:24:15,216 --> 00:24:18,186 A:middle
if I wanted to work with
just the input node,

558
00:24:18,416 --> 00:24:22,346 A:middle
say just capture data from the
microphone and maybe examine it,

559
00:24:22,606 --> 00:24:25,096 A:middle
analyze it in real time or
maybe write it out to file,

560
00:24:25,136 --> 00:24:29,136 A:middle
I can directly install
a tap on the input node.

561
00:24:29,786 --> 00:24:32,976 A:middle
And the tap will do the work of
pulling the input node for data,

562
00:24:33,526 --> 00:24:35,866 A:middle
stuffing it in buffers
and then returning

563
00:24:35,866 --> 00:24:37,076 A:middle
that back to the application.

564
00:24:37,696 --> 00:24:39,796 A:middle
Once you have that data you
can do whatever you need

565
00:24:39,796 --> 00:24:41,006 A:middle
to do with it.

566
00:24:42,626 --> 00:24:46,696 A:middle
And let's now talk about
the last type of nodes

567
00:24:46,816 --> 00:24:49,046 A:middle
in this section, effect nodes.

568
00:24:50,156 --> 00:24:52,666 A:middle
Effect nodes are nodes
that process data.

569
00:24:53,226 --> 00:24:56,326 A:middle
So, depending on the type of
effect, they take some amount

570
00:24:56,326 --> 00:24:59,356 A:middle
of data in, they process
it and push that data out.

571

572
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

573
00:25:00,736 --> 00:25:03,066 A:middle
We have two main
categories of effects.

574
00:25:03,226 --> 00:25:07,666 A:middle
You have AVAudioUnitEffects
and AVAudioUnitTimeEffects.

575
00:25:08,676 --> 00:25:10,586 A:middle
So what's the difference
between the two?

576
00:25:11,456 --> 00:25:16,286 A:middle
AVAudioUnitEffects require the
same amount of data on input

577
00:25:16,286 --> 00:25:19,396 A:middle
as the amount of data they're
being asked to provide.

578
00:25:20,136 --> 00:25:23,296 A:middle
So let's take the example
of a distortion effect.

579
00:25:24,016 --> 00:25:28,636 A:middle
If a distortion node has
to provide 24ms of output,

580
00:25:29,206 --> 00:25:31,646 A:middle
all it needs is 24ms of input

581
00:25:31,876 --> 00:25:33,776 A:middle
that it then processes
and pushes out.

582
00:25:33,776 --> 00:25:38,696 A:middle
As opposed to that, TimeEffects
don't have that constraint.

583
00:25:39,666 --> 00:25:42,146 A:middle
So let's say that you have a
TimeEffect that's doing some

584
00:25:42,146 --> 00:25:43,246 A:middle
amount of time stretching.

585
00:25:43,986 --> 00:25:47,366 A:middle
If it is being asked to
provide 24ms of output,

586
00:25:47,646 --> 00:25:50,196 A:middle
it may require 48ms of input.

587
00:25:51,346 --> 00:25:53,206 A:middle
So that brings me
to my second point.

588
00:25:53,756 --> 00:25:57,386 A:middle
It is for that reason why you
cannot connect a TimeEffect

589
00:25:57,716 --> 00:25:59,326 A:middle
directly with the input node.

590

591
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

592
00:26:00,226 --> 00:26:02,556 A:middle
Because, when you have
the input node running

593
00:26:02,556 --> 00:26:07,536 A:middle
in real-time it cannot provide
data that it doesn't have.

594
00:26:07,756 --> 00:26:09,066 A:middle
As opposed to that,

595
00:26:09,146 --> 00:26:12,186 A:middle
with AVAudioUnitEffects you
can connect them anywhere

596
00:26:12,186 --> 00:26:12,666 A:middle
in the chain.

597
00:26:13,006 --> 00:26:14,516 A:middle
So you can use them with players

598
00:26:14,826 --> 00:26:17,526 A:middle
or you can use them
with the input node.

599
00:26:19,256 --> 00:26:21,556 A:middle
These are the list of effects

600
00:26:21,616 --> 00:26:22,976 A:middle
that we currently
have available.

601
00:26:29,076 --> 00:26:32,276 A:middle
So on the effects side
we have the Delay,

602
00:26:32,276 --> 00:26:34,026 A:middle
Distortion, EQ and Reverb.

603
00:26:34,326 --> 00:26:36,736 A:middle
If you're a musician you're
probably already familiar

604
00:26:36,736 --> 00:26:39,156 A:middle
with these effects so you
can use them in real time

605
00:26:39,186 --> 00:26:40,076 A:middle
or use them in the player.

606
00:26:40,816 --> 00:26:43,346 A:middle
And on the TimeEffect side,

607
00:26:43,636 --> 00:26:45,676 A:middle
we have the Varispeed
and the TimePitch.

608
00:26:46,146 --> 00:26:49,136 A:middle
And these effects are useful
in cases where you want

609
00:26:49,136 --> 00:26:52,116 A:middle
to manipulate the amount of time
stretching or maybe the pitch

610
00:26:52,116 --> 00:26:53,096 A:middle
of the source content.

611
00:26:53,796 --> 00:26:56,476 A:middle
So let's say that you have a
speech file that you're playing

612
00:26:56,606 --> 00:27:00,296 A:middle
and you want to pitch the voice
up to sound like a chipmunk.

613

614
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

615
00:26:56,606 --> 00:27:00,296 A:middle
and you want to pitch the voice
up to sound like a chipmunk.

616
00:27:01,086 --> 00:27:05,386 A:middle
Well you can do that using
one of the TimeEffects.

617
00:27:05,666 --> 00:27:07,416 A:middle
So let's now look at an example

618
00:27:07,596 --> 00:27:09,226 A:middle
of how you can use
one of these effects.

619
00:27:10,086 --> 00:27:13,316 A:middle
In this example I'm going
to make use of the EQ.

620
00:27:13,726 --> 00:27:17,556 A:middle
But note that over here I've
connected the EQ directly

621
00:27:18,046 --> 00:27:19,076 A:middle
to the output node.

622
00:27:20,126 --> 00:27:24,226 A:middle
In all of my prior examples
I was connecting nodes

623
00:27:24,226 --> 00:27:26,936 A:middle
to the mixers, to the
engine's mixer node.

624
00:27:27,246 --> 00:27:28,936 A:middle
But I don't always
have to do that.

625
00:27:29,546 --> 00:27:32,386 A:middle
If I just have one chain of data

626
00:27:33,066 --> 00:27:35,536 A:middle
in my application then I
can just directly connect it

627
00:27:35,536 --> 00:27:37,926 A:middle
to the output node, which
is what I've done here.

628
00:27:40,356 --> 00:27:45,136 A:middle
So this is a multiband EQ
and I specify the number

629
00:27:45,356 --> 00:27:48,106 A:middle
of bands I'm going to use when
I create an instance of the EQ.

630
00:27:48,876 --> 00:27:51,126 A:middle
So over here, I'm
going to use two bands

631
00:27:51,546 --> 00:27:53,046 A:middle
so I create an EQ
with two bands.

632
00:27:53,916 --> 00:27:56,896 A:middle
I can then go ahead and get
access to each of the bands

633
00:27:56,896 --> 00:27:59,136 A:middle
and set up the different
filter parameters.

634
00:27:59,586 --> 00:28:04,466 A:middle
Connecting the EQ
is no different

635

636
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

637
00:27:59,586 --> 00:28:04,466 A:middle
Connecting the EQ
is no different

638
00:28:04,566 --> 00:28:06,376 A:middle
than what we've already seen.

639
00:28:06,836 --> 00:28:09,106 A:middle
I can connect the
player to the EQ

640
00:28:09,166 --> 00:28:12,296 A:middle
with the file's processing
format and connect the EQ

641
00:28:12,296 --> 00:28:17,766 A:middle
to the engine's output node with
the same format and that's it.

642
00:28:18,136 --> 00:28:20,696 A:middle
So with all of this information,
let's look at a demo

643
00:28:20,786 --> 00:28:23,596 A:middle
that makes use of some of the
nodes that we've talked about.

644
00:28:24,306 --> 00:28:26,806 A:middle
Okay, so I'm going to
explain what I have here.

645
00:28:27,666 --> 00:28:30,396 A:middle
Over here I have two
player nodes and each

646
00:28:30,396 --> 00:28:33,236 A:middle
of these players is going
to be fed by a separate,

647
00:28:33,336 --> 00:28:34,816 A:middle
by separate looping buffers.

648
00:28:36,496 --> 00:28:39,576 A:middle
Each of the players are
connected to separate effects

649
00:28:40,396 --> 00:28:43,826 A:middle
and each of these effects are
connected to separate inputs

650
00:28:43,886 --> 00:28:45,196 A:middle
of the engine's main mixer.

651
00:28:45,816 --> 00:28:50,506 A:middle
I have control over the output
volume of the main mixer

652
00:28:51,526 --> 00:28:54,826 A:middle
and down here I have
a transport control,

653
00:28:55,316 --> 00:28:57,686 A:middle
which essentially
controls a node tap

654
00:28:58,056 --> 00:28:59,906 A:middle
that I've installed
on the main mixer.

655

656
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

657
00:29:00,846 --> 00:29:04,526 A:middle
So when I hit Record, a tap
gets installed and I capture

658
00:29:04,526 --> 00:29:06,486 A:middle
that data and save it to a file.

659
00:29:07,116 --> 00:29:08,896 A:middle
And then when I hit
Play I'm just going

660
00:29:08,896 --> 00:29:09,856 A:middle
to play that file back.

661
00:29:10,726 --> 00:29:12,286 A:middle
So let's listen to what
this sounds like [music].

662
00:29:14,596 --> 00:29:18,336 A:middle
So here I'm playing the drums.

663
00:29:18,806 --> 00:29:23,286 A:middle
I can change the volume
and the pan of each player,

664
00:29:26,756 --> 00:29:27,816 A:middle
so you can hear that effect.

665
00:29:28,776 --> 00:29:31,076 A:middle
I'm now going to go ahead and
play the reverb a little bit.

666
00:29:31,276 --> 00:29:36,556 A:middle
It sounds a little too wet, so
I'm going to keep it about here.

667
00:29:37,566 --> 00:29:38,726 A:middle
Let me start the other player.

668
00:29:39,516 --> 00:29:44,546 A:middle
[ Music ]

669
00:29:45,046 --> 00:29:48,686 A:middle
Okay, so what I thought I'd
do now is use the node tap

670
00:29:49,126 --> 00:29:53,506 A:middle
to maybe capture a little live
performance, and any changes

671
00:29:53,556 --> 00:29:56,126 A:middle
that I make to any of the
nodes here should be captured

672
00:29:56,126 --> 00:29:57,146 A:middle
in that performance.

673
00:29:57,146 --> 00:29:59,566 A:middle
So when we go back and listen
to that we should hear that.

674

675
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

676
00:30:00,386 --> 00:30:01,846 A:middle
So let me do that.

677
00:30:02,516 --> 00:30:21,636 A:middle
[ Music ]

678
00:30:22,136 --> 00:30:26,296 A:middle
Okay, so I'm going to stop my
recording, stop my delay player.

679
00:30:28,006 --> 00:30:31,276 A:middle
And let's go back and
listen to the recording.

680
00:30:32,516 --> 00:30:48,516 A:middle
[ Music ]

681
00:30:49,016 --> 00:30:52,000 A:middle
[ Silence ]

682
00:30:52,046 --> 00:30:54,576 A:middle
And that's a preview of
AVAudioEngine in action.

683
00:30:55,966 --> 00:30:57,116 A:middle
Let's go back to slides.

684
00:30:57,996 --> 00:31:01,216 A:middle
Alright, so two of the
settings that I was changing

685

686
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

687
00:30:57,996 --> 00:31:01,216 A:middle
Alright, so two of the
settings that I was changing

688
00:31:02,966 --> 00:31:06,036 A:middle
with the players were the volume
and the pan for each player.

689
00:31:06,616 --> 00:31:11,786 A:middle
But these are actually
settings of the input mixer bus

690
00:31:12,116 --> 00:31:13,606 A:middle
that the player is connected to.

691
00:31:14,636 --> 00:31:18,066 A:middle
So the way we've exposed
mixer input bus settings

692
00:31:18,656 --> 00:31:21,926 A:middle
in the audio engine is
through a protocol called the

693
00:31:21,926 --> 00:31:23,366 A:middle
AVAudioMixing protocol.

694
00:31:24,286 --> 00:31:27,936 A:middle
Source nodes conform to this
protocol so the player node

695
00:31:27,936 --> 00:31:29,816 A:middle
and the input node do that.

696
00:31:30,196 --> 00:31:33,046 A:middle
And settings, like
volume, you can change

697
00:31:33,046 --> 00:31:38,076 A:middle
by just doing player.volume=.5
or player.band=minus 1.

698
00:31:39,776 --> 00:31:43,616 A:middle
When a source node is in an
active connection with the mixer

699
00:31:43,876 --> 00:31:46,416 A:middle
and you make changes
to the protocols,

700
00:31:46,416 --> 00:31:49,476 A:middle
different properties they
take effect immediately.

701
00:31:50,316 --> 00:31:53,266 A:middle
However, if a source node
is not connected to a mixer

702
00:31:53,666 --> 00:31:55,806 A:middle
and you make changes to
the protocol's properties,

703
00:31:56,596 --> 00:32:00,546 A:middle
those changes are cached in the
source node and then applied

704

705
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

706
00:31:56,596 --> 00:32:00,546 A:middle
those changes are cached in the
source node and then applied

707
00:32:00,616 --> 00:32:03,016 A:middle
when you make a physical
connection to a mixer.

708
00:32:03,666 --> 00:32:06,516 A:middle
So these are the
mixing properties

709
00:32:06,516 --> 00:32:07,546 A:middle
that we have available.

710
00:32:08,216 --> 00:32:10,776 A:middle
Under the common mixing
properties we just have volume

711
00:32:10,776 --> 00:32:11,156 A:middle
right now.

712
00:32:11,926 --> 00:32:14,256 A:middle
Under the stereo mixing
properties, we have pan

713
00:32:15,216 --> 00:32:17,556 A:middle
and we have a number
of 3D mixing properties

714
00:32:17,746 --> 00:32:19,436 A:middle
that we're going to look
at in the next section.

715
00:32:19,846 --> 00:32:22,626 A:middle
So in the form of a diagram,

716
00:32:23,106 --> 00:32:26,326 A:middle
let's say that I have Player
1 connected to Mixer 1

717
00:32:27,076 --> 00:32:30,686 A:middle
and I go ahead and set player
to start pan to minus 1,

718
00:32:31,016 --> 00:32:34,366 A:middle
hard pan it to the left and
player 1's volume to .5.

719
00:32:35,546 --> 00:32:39,676 A:middle
So these mixing settings are
now associated with Player 1.

720
00:32:39,886 --> 00:32:41,576 A:middle
And because Player
1 is connected

721
00:32:41,576 --> 00:32:44,966 A:middle
to Mixer 1 they also get
applied on the mixer.

722
00:32:45,326 --> 00:32:49,236 A:middle
If I were to disconnect Player
1 and connect it to Mixer 2,

723
00:32:49,706 --> 00:32:52,656 A:middle
these mixing settings
travel along with Player 1

724
00:32:53,056 --> 00:32:54,896 A:middle
and get applied to Mixer 2.

725
00:32:55,516 --> 00:32:59,576 A:middle
So in this sense, we've
been able to carry settings

726
00:32:59,576 --> 00:33:02,906 A:middle
that belong to the input
bus of a mixer along

727

728
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

729
00:32:59,576 --> 00:33:02,906 A:middle
that belong to the input
bus of a mixer along

730
00:33:02,906 --> 00:33:04,346 A:middle
with the source node itself.

731
00:33:04,926 --> 00:33:10,236 A:middle
Alright, so let's now
move onto the next section

732
00:33:10,436 --> 00:33:11,836 A:middle
on gaming and 3D audio.

733
00:33:14,606 --> 00:33:17,226 A:middle
So in games, typically
you have several types

734
00:33:17,226 --> 00:33:18,156 A:middle
of sounds that you play.

735
00:33:18,156 --> 00:33:22,956 A:middle
You have short sounds, and we've
seen AudioServices, which is one

736
00:33:22,956 --> 00:33:25,086 A:middle
of our C-APIs get used for that.

737
00:33:25,836 --> 00:33:29,476 A:middle
For playing music we see
AVAudioPlayer getting used

738
00:33:29,476 --> 00:33:30,036 A:middle
a lot.

739
00:33:30,266 --> 00:33:32,196 A:middle
And for sounds that
need to be spatialized,

740
00:33:32,516 --> 00:33:34,396 A:middle
OpenAL is the API of choice.

741
00:33:35,796 --> 00:33:38,836 A:middle
Now while each of these
APIs work really well

742
00:33:38,836 --> 00:33:42,086 A:middle
for what they were designed
for, if your application has

743
00:33:42,116 --> 00:33:46,356 A:middle
to make use of all of them, then
one of the biggest tradeoffs is

744
00:33:46,356 --> 00:33:48,156 A:middle
that you have to
familiarize yourself

745
00:33:48,156 --> 00:33:51,156 A:middle
with the nomenclature
associated with each API.

746
00:33:52,266 --> 00:33:55,406 A:middle
In addition, with AudioServices
you don't have a latency

747
00:33:55,406 --> 00:33:57,156 A:middle
guarantee of when
your sound will play.

748
00:33:57,866 --> 00:34:00,316 A:middle
With AVAudioPlayer
you can't play sounds

749

750
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

751
00:33:57,866 --> 00:34:00,316 A:middle
With AVAudioPlayer
you can't play sounds

752
00:34:00,316 --> 00:34:01,496 A:middle
that you have in buffers.

753
00:34:01,976 --> 00:34:05,486 A:middle
And with OpenAL, you can't play
sounds directly from a file

754
00:34:05,706 --> 00:34:07,536 A:middle
or play compressed data.

755
00:34:08,016 --> 00:34:10,556 A:middle
With our knowledge
of AVAudioEngine,

756
00:34:10,616 --> 00:34:12,716 A:middle
we have to go back
and cover cases one

757
00:34:12,716 --> 00:34:15,146 A:middle
and two; we can easily do so.

758
00:34:15,906 --> 00:34:17,775 A:middle
For short sounds we
can just load them

759
00:34:17,775 --> 00:34:20,946 A:middle
into AVAudioBuffer objects
and schedule them on a player.

760
00:34:22,005 --> 00:34:25,706 A:middle
For music you can just create
an AVAudioFile log object

761
00:34:25,795 --> 00:34:27,876 A:middle
and schedule that
directly on a player.

762
00:34:28,815 --> 00:34:31,386 A:middle
So how do you play sounds
that need to be spatialized?

763
00:34:31,746 --> 00:34:32,626 A:middle
We'll look at that now.

764
00:34:33,536 --> 00:34:36,946 A:middle
I'd like to introduce a new
node called the environment node

765
00:34:37,406 --> 00:34:39,726 A:middle
and this is essentially
a 3D mixer.

766
00:34:40,946 --> 00:34:44,775 A:middle
So when you create an instance
of the environment node.

767
00:34:44,775 --> 00:34:48,706 A:middle
You have a 3D space and you
get a listener that's implicit

768
00:34:48,746 --> 00:34:49,926 A:middle
to that 3D space.

769
00:34:50,976 --> 00:34:53,025 A:middle
All of the source
nodes that connect

770
00:34:53,656 --> 00:34:58,026 A:middle
to the environment node act
as sources in this 3D space.

771
00:34:59,366 --> 00:35:01,416 A:middle
So the environment
has some attributes

772

773
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

774
00:34:59,366 --> 00:35:01,416 A:middle
So the environment
has some attributes

775
00:35:01,506 --> 00:35:03,856 A:middle
that you can set directly
on the environment node.

776
00:35:04,496 --> 00:35:07,136 A:middle
And then each of these
sources have some attributes

777
00:35:07,226 --> 00:35:10,536 A:middle
and you can set that using
the AVAudioMixing protocol's

778
00:35:10,596 --> 00:35:11,636 A:middle
3D properties.

779
00:35:13,086 --> 00:35:16,596 A:middle
Now in terms of data formats,
I just wanted to point out that

780
00:35:16,596 --> 00:35:18,186 A:middle
when you're working with
the environment node,

781
00:35:18,666 --> 00:35:23,116 A:middle
all of the sources need to have
a mono data format in order

782
00:35:23,116 --> 00:35:24,726 A:middle
for that audio to
be spatialized.

783
00:35:25,636 --> 00:35:27,846 A:middle
If the sources have
a stereo data format,

784
00:35:28,236 --> 00:35:30,176 A:middle
then that data is passed through

785
00:35:30,626 --> 00:35:33,526 A:middle
and currently the environment
node doesn't support a data

786
00:35:33,526 --> 00:35:36,166 A:middle
format greater than
two channels on input.

787
00:35:36,326 --> 00:35:40,216 A:middle
So as a diagram, this
is what it looks like.

788
00:35:40,586 --> 00:35:43,086 A:middle
I've created an instance
of an environment node

789
00:35:43,236 --> 00:35:45,226 A:middle
which means I now
have a 3D space

790
00:35:45,996 --> 00:35:47,446 A:middle
and I have an implicit listener.

791
00:35:48,866 --> 00:35:50,496 A:middle
I now create two player nodes.

792
00:35:50,496 --> 00:35:54,106 A:middle
Who are going to act as
sources in my 3D space

793
00:35:54,676 --> 00:35:57,846 A:middle
and using the AVAudioMixing
protocol I can set all

794
00:35:57,846 --> 00:35:59,196 A:middle
of the source attributes.

795
00:35:59,766 --> 00:36:04,326 A:middle
So what makes things
sound 3D or virtual 3D?

796

797
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

798
00:35:59,766 --> 00:36:04,326 A:middle
So what makes things
sound 3D or virtual 3D?

799
00:36:04,786 --> 00:36:07,436 A:middle
Well, we have a number of
attributes and some belong

800
00:36:07,436 --> 00:36:09,966 A:middle
to the sources, others
belong to the environment.

801
00:36:10,486 --> 00:36:12,736 A:middle
Let's walk through each of
the source attributes first.

802
00:36:13,826 --> 00:36:17,476 A:middle
So every source has a
position in this 3D space.

803
00:36:17,896 --> 00:36:20,746 A:middle
And right now it's specified
using the right-handed cartesian

804
00:36:20,746 --> 00:36:25,126 A:middle
coordinate system that
right positive Y is up

805
00:36:25,126 --> 00:36:26,846 A:middle
and positive Z is
towards the listener.

806
00:36:28,076 --> 00:36:30,006 A:middle
Now with respect
to the listener,

807
00:36:30,706 --> 00:36:33,486 A:middle
the listener uses
some spatial cues

808
00:36:33,596 --> 00:36:35,886 A:middle
to localize the position
of the source.

809
00:36:36,646 --> 00:36:38,836 A:middle
There's an inter-aural
time difference,

810
00:36:39,086 --> 00:36:43,396 A:middle
just a slight time difference
for the sound made by the source

811
00:36:43,486 --> 00:36:45,466 A:middle
to get to each one of the
listeners in those years.

812
00:36:45,826 --> 00:36:48,326 A:middle
There's also an inter-aural
level difference.

813
00:36:49,066 --> 00:36:52,576 A:middle
In addition, your head has the
effect of doing some filtering

814
00:36:53,096 --> 00:36:55,766 A:middle
and you also have some
filtering here with the ears,

815
00:36:55,816 --> 00:36:56,786 A:middle
depending on the ears.

816
00:36:58,156 --> 00:37:01,926 A:middle
So we have several rendering
algorithms and each one

817

818
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

819
00:36:58,156 --> 00:37:01,926 A:middle
So we have several rendering
algorithms and each one

820
00:37:01,926 --> 00:37:04,216 A:middle
of them model these
spatial cues differently.

821
00:37:05,396 --> 00:37:08,456 A:middle
The thing is that we've exposed
this as a source property.

822
00:37:08,956 --> 00:37:11,806 A:middle
So you can pick a rendering
algorithm per source

823
00:37:12,286 --> 00:37:15,636 A:middle
and some algorithms may sound
better depending on the type

824
00:37:15,636 --> 00:37:17,736 A:middle
of content your source
is playing

825
00:37:18,476 --> 00:37:21,686 A:middle
and also they differ
in terms of CPU cost.

826
00:37:22,146 --> 00:37:25,816 A:middle
So you may want to pick a
more expensive algorithm

827
00:37:26,036 --> 00:37:27,466 A:middle
for an important source

828
00:37:27,866 --> 00:37:30,336 A:middle
and a cheaper algorithm
for a regular source.

829
00:37:30,856 --> 00:37:36,686 A:middle
The next two properties,
obstruction and occlusion,

830
00:37:36,936 --> 00:37:40,356 A:middle
deal with the filtering of
sound if there are obstacles

831
00:37:40,356 --> 00:37:42,066 A:middle
between the source and listener.

832
00:37:43,406 --> 00:37:47,216 A:middle
So in this case, I have the
source, that's the monster,

833
00:37:48,086 --> 00:37:50,126 A:middle
and the listener, that's
the handsome prince,

834
00:37:50,586 --> 00:37:53,236 A:middle
and there is a column between
the source and the listener.

835
00:37:53,326 --> 00:37:58,116 A:middle
So the direct path of sound is
muffled whereas the reflected

836
00:37:58,176 --> 00:38:00,746 A:middle
paths across the walls are clear

837

838
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

839
00:37:58,176 --> 00:38:00,746 A:middle
paths across the walls are clear

840
00:38:01,006 --> 00:38:02,626 A:middle
and this is modeled
by obstruction.

841
00:38:03,516 --> 00:38:08,096 A:middle
On the other hand, if the
source and the listener are

842
00:38:08,096 --> 00:38:11,256 A:middle
on different spaces, so
right now that's a wall

843
00:38:11,256 --> 00:38:12,556 A:middle
between the source
and the listener.

844
00:38:13,276 --> 00:38:14,866 A:middle
Both the direct part of sound

845
00:38:15,296 --> 00:38:17,576 A:middle
and the reflective parts
of sound are muffled.

846
00:38:18,126 --> 00:38:22,226 A:middle
Let's now move on
to the listener,

847
00:38:22,516 --> 00:38:23,866 A:middle
the environment attributes.

848
00:38:25,276 --> 00:38:27,706 A:middle
So every environment
has an implicit listener

849
00:38:27,706 --> 00:38:30,936 A:middle
and the listener has a
position and an orientation.

850
00:38:31,526 --> 00:38:34,266 A:middle
The position is specified using
the same coordinate system.

851
00:38:34,816 --> 00:38:38,226 A:middle
And for the orientation, you
can specify using either two

852
00:38:38,226 --> 00:38:40,366 A:middle
vectors, a front
and an up vector,

853
00:38:41,406 --> 00:38:45,876 A:middle
or three angles yaw,
pitch and draw.

854
00:38:46,646 --> 00:38:49,446 A:middle
You also have distance
attenuation in the environment,

855
00:38:49,916 --> 00:38:51,736 A:middle
which is just the
attenuation of sound

856
00:38:52,276 --> 00:38:54,656 A:middle
as a source moves away
from the listener.

857
00:38:55,446 --> 00:38:58,326 A:middle
So in this graph there are
two points of interest.

858
00:38:58,796 --> 00:39:01,626 A:middle
There's the reference
distance, which is the distance

859

860
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

861
00:38:58,796 --> 00:39:01,626 A:middle
There's the reference
distance, which is the distance

862
00:39:01,626 --> 00:39:05,236 A:middle
above which we start applying
some amount of attenuation.

863
00:39:05,886 --> 00:39:09,216 A:middle
There's also the maximum
distance, which is the point

864
00:39:09,216 --> 00:39:10,836 A:middle
above which the amount

865
00:39:10,836 --> 00:39:12,996 A:middle
of attenuation being
applied is capped.

866
00:39:13,936 --> 00:39:15,706 A:middle
So all of the exciting
stuff happens

867
00:39:15,706 --> 00:39:18,366 A:middle
between the reference distance
and the maximum distance.

868
00:39:19,156 --> 00:39:22,126 A:middle
And in that region we have three
curves that you can pick from.

869
00:39:22,746 --> 00:39:25,796 A:middle
So, in the form of code,
this is what it looks like.

870
00:39:26,126 --> 00:39:28,786 A:middle
All you need to do is get the
distance attenuation parameters

871
00:39:28,786 --> 00:39:32,426 A:middle
object from the environment
and then you can go ahead

872
00:39:32,476 --> 00:39:33,826 A:middle
and tweak all the settings.

873
00:39:34,386 --> 00:39:39,766 A:middle
Now every environment
also has reverberation

874
00:39:39,986 --> 00:39:41,596 A:middle
which is just a simulation

875
00:39:41,596 --> 00:39:43,856 A:middle
of the sound reflections
within that space.

876
00:39:44,846 --> 00:39:49,666 A:middle
The environment node has a
built-in reverb and you can pick

877
00:39:50,286 --> 00:39:52,246 A:middle
from a selection
of factory presets.

878
00:39:53,166 --> 00:39:56,536 A:middle
Now once you pick the type
of reverb you want to use,

879
00:39:57,296 --> 00:40:00,306 A:middle
you can set a blend
amount for each source

880

881
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

882
00:39:57,296 --> 00:40:00,306 A:middle
you can set a blend
amount for each source

883
00:40:01,186 --> 00:40:04,266 A:middle
and that just affects
the amount of each source

884
00:40:04,266 --> 00:40:05,966 A:middle
that you'll here
in the reverb mix.

885
00:40:06,496 --> 00:40:08,256 A:middle
So for some sources,
you may want them

886
00:40:08,256 --> 00:40:11,226 A:middle
to sound completely dry, so you
set the blend amount to zero.

887
00:40:11,636 --> 00:40:14,286 A:middle
And other sources you may
want to sound more ambient

888
00:40:14,426 --> 00:40:16,026 A:middle
so you can turn up
the blend amount.

889
00:40:17,396 --> 00:40:20,076 A:middle
We also have a single
filter that applies

890
00:40:20,136 --> 00:40:21,346 A:middle
to the output of the reverb.

891
00:40:21,786 --> 00:40:24,566 A:middle
So let's say that you pick
one of the factory presets

892
00:40:24,906 --> 00:40:27,116 A:middle
and you want it to sound
maybe a little brighter.

893
00:40:27,726 --> 00:40:29,466 A:middle
You can do that using
the filter.

894
00:40:30,726 --> 00:40:32,316 A:middle
In code, this is
what it looks like.

895
00:40:32,646 --> 00:40:35,376 A:middle
I get the ReverbParameters
object from the environment.

896
00:40:35,986 --> 00:40:37,576 A:middle
In this case, I'm enabling it

897
00:40:37,746 --> 00:40:41,796 A:middle
and then I load a factory
preset, LargeHall preset.

898
00:40:42,276 --> 00:40:44,826 A:middle
And using the AVAudioMixing
protocol,

899
00:40:44,826 --> 00:40:47,896 A:middle
I set the source's
reverbBlend to 0.2.

900
00:40:48,816 --> 00:40:51,276 A:middle
So now we've talked about
two types of mixers.

901
00:40:51,516 --> 00:40:54,816 A:middle
You have the 2D mixer and
you have the 3D mixer.

902
00:40:55,636 --> 00:40:58,506 A:middle
And source nodes, that is
the player or the input node,

903
00:40:59,096 --> 00:41:02,836 A:middle
talk to these mixers using
the AVAudioMixing protocol.

904

905
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

906
00:40:59,096 --> 00:41:02,836 A:middle
talk to these mixers using
the AVAudioMixing protocol.

907
00:41:03,896 --> 00:41:05,826 A:middle
So I just wanted
to point out that

908
00:41:05,826 --> 00:41:08,566 A:middle
when a source node is
connected to a 2D mixer,

909
00:41:08,996 --> 00:41:12,106 A:middle
then all of the common and
the 2D mixing properties

910
00:41:12,166 --> 00:41:12,766 A:middle
take effect.

911
00:41:13,716 --> 00:41:16,656 A:middle
When a source node is
connected to a 3D mixer,

912
00:41:17,156 --> 00:41:20,696 A:middle
then all of the common and
the 3D mixing properties

913
00:41:20,696 --> 00:41:21,256 A:middle
take effect.

914
00:41:22,336 --> 00:41:24,646 A:middle
Let's look at what
that looks like here.

915
00:41:26,126 --> 00:41:27,866 A:middle
So let's say that
I have Player 1

916
00:41:28,316 --> 00:41:30,906 A:middle
who is connected
to the 2D mixer.

917
00:41:31,776 --> 00:41:36,246 A:middle
I set the pan to be -1
and volume to be .5.

918
00:41:36,996 --> 00:41:40,206 A:middle
Note that pan is a
2D mixing property

919
00:41:40,466 --> 00:41:42,496 A:middle
but volume is a common
mixing property.

920
00:41:43,136 --> 00:41:45,646 A:middle
But in this case both of
them will take effect,

921
00:41:46,036 --> 00:41:47,846 A:middle
because the mixer
node implements both

922
00:41:47,846 --> 00:41:48,716 A:middle
of these properties.

923
00:41:49,556 --> 00:41:53,026 A:middle
If I disconnect Player 1
from the mixer and connect it

924
00:41:53,026 --> 00:41:57,836 A:middle
to the environment node, the
pan property will now be cached.

925
00:41:57,946 --> 00:42:00,956 A:middle
It doesn't take effect because
it's a 2D mixing property.

926

927
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

928
00:41:57,946 --> 00:42:00,956 A:middle
It doesn't take effect because
it's a 2D mixing property.

929
00:42:01,136 --> 00:42:03,046 A:middle
It doesn't apply to
the environment node.

930
00:42:04,036 --> 00:42:07,106 A:middle
Volume, on the other hand,
will continue to take effect,

931
00:42:07,486 --> 00:42:10,276 A:middle
because it's a common mixing
property and it's implemented

932
00:42:10,276 --> 00:42:11,216 A:middle
by the environment node.

933
00:42:11,706 --> 00:42:13,806 A:middle
So with all of that
information let's look

934
00:42:13,886 --> 00:42:15,986 A:middle
at a sample gaming setup.

935
00:42:16,786 --> 00:42:20,016 A:middle
This is just one of many
ways that you can do this

936
00:42:20,016 --> 00:42:21,336 A:middle
and this is just a suggestion.

937
00:42:21,676 --> 00:42:23,476 A:middle
It really all depends
on your application.

938
00:42:24,436 --> 00:42:28,316 A:middle
But in this case, I
have two 3D sources.

939
00:42:28,646 --> 00:42:31,366 A:middle
So, I'm going to use a
player to play some sounds

940
00:42:31,366 --> 00:42:34,336 A:middle
that will be spatialized
and also live input.

941
00:42:34,946 --> 00:42:37,466 A:middle
So let's say that the user
is chatting and then you want

942
00:42:37,466 --> 00:42:39,576 A:middle
to spatialize that
in a 3D environment.

943
00:42:40,116 --> 00:42:42,186 A:middle
I can connect the player
node and the input node

944
00:42:42,496 --> 00:42:43,456 A:middle
to the environment node.

945
00:42:43,876 --> 00:42:46,136 A:middle
And that's connected out
to the engine's main mixer.

946
00:42:47,066 --> 00:42:50,526 A:middle
I can now have a second
player that I'm going

947
00:42:50,526 --> 00:42:52,456 A:middle
to dedicate to playing music.

948
00:42:53,216 --> 00:42:56,776 A:middle
So this player is going to play
music and I'm going to run it

949
00:42:56,816 --> 00:42:59,086 A:middle
through an EQ and connect
that to the main mixer.

950
00:42:59,746 --> 00:43:02,346 A:middle
Let's say that I present
some UI for the users

951

952
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

953
00:42:59,746 --> 00:43:02,346 A:middle
Let's say that I present
some UI for the users

954
00:43:02,346 --> 00:43:04,166 A:middle
so that he can tweak
the EQ settings,

955
00:43:04,826 --> 00:43:06,456 A:middle
maybe to make the
music sound better.

956
00:43:06,456 --> 00:43:10,926 A:middle
I have a third player
now that I'm going

957
00:43:10,926 --> 00:43:13,626 A:middle
to dedicate only to
UI sound effects.

958
00:43:14,016 --> 00:43:17,336 A:middle
So maybe the sounds that are
made as I navigate through menus

959
00:43:17,846 --> 00:43:20,726 A:middle
or if my game avatar has
picked up a bonus item,

960
00:43:20,926 --> 00:43:25,406 A:middle
etc. So the UI player
is connected directly

961
00:43:25,526 --> 00:43:26,666 A:middle
to the engine's main mixer.

962
00:43:27,586 --> 00:43:30,746 A:middle
This is what the overall
picture looks like.

963
00:43:33,256 --> 00:43:36,576 A:middle
So given all of this information
let's now look at a demo

964
00:43:36,576 --> 00:43:38,016 A:middle
of the environment node.

965
00:43:39,516 --> 00:43:50,586 A:middle
[ Balls popping ]

966
00:43:51,086 --> 00:43:53,366 A:middle
So I want to explain
what's happening over here.

967
00:43:53,366 --> 00:43:56,916 A:middle
In this demo, I am using
SceneKit for the graphics

968
00:43:56,986 --> 00:43:59,146 A:middle
and SceneKit also comes
with a physics engine.

969
00:43:59,276 --> 00:44:01,306 A:middle
So this works nicely
with AVAudioEngine.

970

971
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

972
00:43:59,276 --> 00:44:01,306 A:middle
So this works nicely
with AVAudioEngine.

973
00:44:01,686 --> 00:44:05,226 A:middle
So I basically have two types
of sounds that I'm playing;

974
00:44:05,656 --> 00:44:07,956 A:middle
that's the "fffuh"
sound that plays

975
00:44:08,136 --> 00:44:10,916 A:middle
and that's before
any ball is launched.

976
00:44:11,596 --> 00:44:13,906 A:middle
So to do that I use
a player node

977
00:44:14,646 --> 00:44:19,196 A:middle
and I have the long sound effect
in a buffer and I schedule

978
00:44:19,196 --> 00:44:20,576 A:middle
that buffer on the player node.

979
00:44:21,076 --> 00:44:24,146 A:middle
But I make use of the
completion handler to know

980
00:44:24,146 --> 00:44:26,066 A:middle
when the player has
consumed the buffer.

981
00:44:26,786 --> 00:44:29,506 A:middle
So, when the player lets me know
that it's done with the buffer,

982
00:44:29,506 --> 00:44:32,946 A:middle
I go ahead and now
create a SceneKit node.

983
00:44:33,246 --> 00:44:37,496 A:middle
That's a ball and I also
create an AVAudioPlayer node,

984
00:44:38,266 --> 00:44:41,756 A:middle
attach it to the
engine and connect

985
00:44:41,876 --> 00:44:43,416 A:middle
that to the environment node.

986
00:44:43,936 --> 00:44:48,176 A:middle
So I'm tying a player, a
dedicated player to each ball.

987
00:44:49,456 --> 00:44:53,046 A:middle
Now the ball is launched into
the world and as it goes about

988
00:44:53,046 --> 00:44:56,166 A:middle
and collides with other
surfaces, for every collision

989
00:44:56,166 --> 00:44:59,946 A:middle
that happens SceneKit's
physics engine lets me know

990
00:44:59,946 --> 00:45:02,326 A:middle
that a collision has happened
with some other surface.

991

992
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

993
00:44:59,946 --> 00:45:02,326 A:middle
that a collision has happened
with some other surface.

994
00:45:02,666 --> 00:45:07,406 A:middle
And I get the point of
collision and also the impulse.

995
00:45:08,656 --> 00:45:11,836 A:middle
So using that, I can go and dig

996
00:45:11,836 --> 00:45:16,036 A:middle
up the player node that's
tied to the SceneKit node.

997
00:45:16,036 --> 00:45:19,816 A:middle
I can set the position
on the player based

998
00:45:19,816 --> 00:45:23,486 A:middle
on where the collision
happened, calculate a volume

999
00:45:23,706 --> 00:45:26,126 A:middle
for the collision sound
based on the impulse

1000
00:45:26,996 --> 00:45:28,246 A:middle
and then just play the sound.

1001
00:45:29,076 --> 00:45:33,166 A:middle
But you can see now
how, in this setup,

1002
00:45:33,346 --> 00:45:36,926 A:middle
for every ball that's
born into this world,

1003
00:45:37,016 --> 00:45:39,106 A:middle
a new player node
is also created.

1004
00:45:39,516 --> 00:45:41,296 A:middle
So the number of
players is growing

1005
00:45:41,296 --> 00:45:44,866 A:middle
and I'm dynamically attaching it
to the engine and connecting it

1006
00:45:44,866 --> 00:45:45,836 A:middle
to the environment node.

1007
00:45:46,366 --> 00:45:47,976 A:middle
So this setup is very flexible.

1008
00:45:48,516 --> 00:45:52,546 A:middle
[ Balls popping ]

1009
00:45:53,046 --> 00:45:54,296 A:middle
Alright, so let's
get back to slides.

1010
00:45:55,126 --> 00:45:56,666 A:middle
That brings us to
the end of our talk.

1011
00:45:56,936 --> 00:45:59,376 A:middle
So let's quickly summarize all
the things we've seen today.

1012

1013
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

1014
00:46:00,586 --> 00:46:02,516 A:middle
We started off with
talking about an engine

1015
00:46:03,076 --> 00:46:06,016 A:middle
and how you can create different
nodes, attach them to the engine

1016
00:46:06,116 --> 00:46:08,016 A:middle
and then use the engine
to make connections

1017
00:46:08,556 --> 00:46:09,806 A:middle
between each of these nodes.

1018
00:46:10,126 --> 00:46:13,396 A:middle
We then looked at the
different types of nodes:

1019
00:46:13,656 --> 00:46:16,536 A:middle
the destination node,
which is the output node.

1020
00:46:17,536 --> 00:46:19,656 A:middle
And we talked about
two source nodes,

1021
00:46:20,126 --> 00:46:21,976 A:middle
the player node and
the input node.

1022
00:46:22,936 --> 00:46:25,576 A:middle
The player is the node you use

1023
00:46:25,576 --> 00:46:27,796 A:middle
to push audio data
on the render thread.

1024
00:46:28,546 --> 00:46:32,316 A:middle
We looked at two types of
mixer nodes, the 2D Mixer

1025
00:46:32,676 --> 00:46:37,596 A:middle
and the 3D Mixer and
how source nodes talk

1026
00:46:37,596 --> 00:46:40,806 A:middle
to these mixers using the
AVAudioMixing protocol.

1027
00:46:41,886 --> 00:46:45,506 A:middle
We then looked at effect nodes
and two types of effect nodes:

1028
00:46:45,586 --> 00:46:48,486 A:middle
the AVAudioEffects and
AVAudioUnitTime effects.

1029
00:46:49,416 --> 00:46:53,596 A:middle
Finally we talked
about node taps

1030
00:46:53,596 --> 00:46:55,906 A:middle
and that's how you pull
data from the render thread.

1031
00:46:56,486 --> 00:46:58,206 A:middle
So I just wanted to point

1032
00:46:58,206 --> 00:47:01,026 A:middle
out that node taps are also
a useful debugging tool.

1033

1034
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00,000

1035
00:46:58,206 --> 00:47:01,026 A:middle
out that node taps are also
a useful debugging tool.

1036
00:47:01,666 --> 00:47:03,776 A:middle
Let's say that you have
a number of connections

1037
00:47:03,896 --> 00:47:06,966 A:middle
in your application and things
don't sound the way you expect

1038
00:47:06,966 --> 00:47:07,566 A:middle
them to sound.

1039
00:47:08,326 --> 00:47:11,876 A:middle
What you can do is install
node taps at different points

1040
00:47:11,876 --> 00:47:15,016 A:middle
in your chain on different nodes
and just examine the output

1041
00:47:15,016 --> 00:47:16,106 A:middle
of each of these nodes.

1042
00:47:16,596 --> 00:47:19,336 A:middle
And using that you can drill
down and where the problem is.

1043
00:47:19,476 --> 00:47:22,246 A:middle
So in that sense node taps
are a useful debugging tool.

1044
00:47:22,716 --> 00:47:25,856 A:middle
So that brings us to
the end of our session.

1045
00:47:26,506 --> 00:47:29,126 A:middle
I just want to say that
this is the first version

1046
00:47:29,126 --> 00:47:31,836 A:middle
of AVAudioEngine and we
are very excited about it.

1047
00:47:32,536 --> 00:47:34,016 A:middle
So, we'd love to
hear what you think.

1048
00:47:34,506 --> 00:47:36,626 A:middle
Please try it out and
give us your feedback.

1049
00:47:37,736 --> 00:47:40,256 A:middle
If you have any further
questions at a later point,

1050
00:47:40,476 --> 00:47:42,916 A:middle
you can contact Filip,
who's our Graphics

1051
00:47:42,916 --> 00:47:45,666 A:middle
and Game Technologies
Evangelist.

1052
